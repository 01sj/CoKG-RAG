#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ„å»ºä¸­æ–‡æ³•å¾‹çŸ¥è¯†å›¾è°±è„šæœ¬ï¼ˆä½¿ç”¨æœ¬åœ° vLLM æ¨¡å‹ï¼‰

æ¥æºå‚è€ƒï¼šbuild_graph.py
è¾“å…¥ï¼š/newdataf/SJ/LeanRAG/output/social_law_7B_processed/ ä¸‹çš„ entity.jsonl ä¸ relation.jsonl
è¾“å‡ºï¼š
  - ç”Ÿæˆçš„ community.json ä¸ generate_relations.json å†™å›åˆ° working_dir
  - æ„å»ºå‘é‡æ£€ç´¢ï¼ˆå¦‚å¯ç”¨ï¼‰
  - å†™å…¥ MySQLï¼ˆå¦‚å¯ç”¨ä¸”é…ç½®æ­£ç¡®ï¼‰

ç‰¹ç‚¹ï¼š
  - ä½¿ç”¨æœ¬åœ° vLLM æ¨¡å‹ï¼Œæ— éœ€åœ¨çº¿ API
  - é’ˆå¯¹ä¸­æ–‡æ³•å¾‹çŸ¥è¯†å›¾è°±ä¼˜åŒ–
  - æ”¯æŒå±‚æ¬¡èšç±»æ„å»º
"""

import argparse
import multiprocessing
import os
import threading
from dataclasses import field

# âš ï¸ é‡è¦ï¼šå¿…é¡»åœ¨å¯¼å…¥ä»»ä½• CUDA ç›¸å…³åº“ï¼ˆå¦‚ vLLMã€Rayã€torchï¼‰ä¹‹å‰è®¾ç½® CUDA_VISIBLE_DEVICES
# å¦åˆ™ç¯å¢ƒå˜é‡è®¾ç½®æ— æ•ˆï¼Œä¼šä½¿ç”¨é»˜è®¤ GPUï¼ˆé€šå¸¸æ˜¯ GPU 0ï¼‰
# é»˜è®¤æ”¹ä¸ºå•å¡ GPU 3ï¼ˆç¬¬å››å¼ å¡ï¼‰ï¼Œå¹¶è¡Œæ•°ä¸º 1ï¼›å¦‚éœ€æ”¹å›å¤šå¡ï¼Œå¯å†è°ƒæ•´ç¯å¢ƒå˜é‡
# æ³¨æ„ï¼šæœåŠ¡å™¨ä¸Šå»ºè®®é€šè¿‡ nvidia-smi/nvitop ç¡®è®¤ GPU 3 ç©ºé—²
default_gpu_ids = os.environ.get('VLLM_GPU_IDS', '3')

# å¼ºåˆ¶è®¾ç½® CUDA_VISIBLE_DEVICESï¼ˆè¦†ç›–ä»»ä½•å·²æœ‰è®¾ç½®ï¼‰
if 'CUDA_VISIBLE_DEVICES' in os.environ:
    old_value = os.environ['CUDA_VISIBLE_DEVICES']
    os.environ['CUDA_VISIBLE_DEVICES'] = default_gpu_ids
    print(f"ğŸ”§ è¦†ç›– CUDA_VISIBLE_DEVICES: {old_value} -> {default_gpu_ids}")
else:
    os.environ['CUDA_VISIBLE_DEVICES'] = default_gpu_ids
    print(f"ğŸ”§ åœ¨å¯¼å…¥ vLLM ä¹‹å‰è®¾ç½® CUDA_VISIBLE_DEVICES={default_gpu_ids}")

print(f"âœ… å½“å‰ CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}ï¼Œå°†ä½¿ç”¨ GPU 3ï¼ˆç¬¬å››å¼ å¡ï¼Œå•å¡ï¼‰")

# å¼ºåˆ¶å•å¡å¼ é‡å¹¶è¡Œé…ç½®ï¼Œç¡®ä¿ vLLM/Ray ä¸ä¼šå°è¯•å¤šå¡
os.environ.setdefault("VLLM_TENSOR_PARALLEL_SIZE", "1")

# å¯¼å…¥vLLMï¼ˆç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦APIæœåŠ¡ï¼‰
try:
    from vllm import LLM, SamplingParams
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: vLLM æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æœ¬åœ°æ¨¡å‹")

import build_graph as bg


def setup_vllm_sync(config):
    """
    ç›´æ¥åŠ è½½vLLMæ¨¡å‹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œç”¨äºå±‚æ¬¡å›¾è°±æ„å»ºï¼‰
    
    å‚æ•°:
        config: LLMé…ç½®å­—å…¸
            - model: æ¨¡å‹è·¯å¾„æˆ–åç§°
            - tensor_parallel_size: å¼ é‡å¹¶è¡ŒGPUæ•°é‡ï¼ˆé»˜è®¤2ï¼‰
            - gpu_memory_utilization: æ¯å¼ GPUçš„å†…å­˜åˆ©ç”¨ç‡ï¼ˆé»˜è®¤0.80ï¼‰
            - max_model_len: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤8192ï¼Œå±‚æ¬¡å›¾è°±éœ€è¦è¾ƒé•¿ä¸Šä¸‹æ–‡ï¼‰
            - temperature: é‡‡æ ·æ¸©åº¦ï¼ˆé»˜è®¤0.2ï¼‰
            - top_p: top_pé‡‡æ ·é˜ˆå€¼ï¼ˆé»˜è®¤0.9ï¼‰
            - max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆé»˜è®¤2048ï¼Œç¤¾åŒºæŠ¥å‘Šå¯èƒ½è¾ƒé•¿ï¼‰
    
    è¿”å›:
        åŒæ­¥LLMç”Ÿæˆå‡½æ•°
    """
    if not VLLM_AVAILABLE:
        raise ImportError("vLLM æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install vllm")
    
    print(f"\n{'='*60}")
    print("åŠ è½½ vLLM æ¨¡å‹ï¼ˆåŒæ­¥æ¨¡å¼ï¼Œç”¨äºå±‚æ¬¡å›¾è°±æ„å»ºï¼‰")
    print(f"{'='*60}")
    print(f"æ¨¡å‹è·¯å¾„: {config['model']}")
    print(f"å¼ é‡å¹¶è¡ŒGPUæ•°é‡: {config.get('tensor_parallel_size', 2)}")
    print(f"æ¯å¼ GPUå†…å­˜åˆ©ç”¨ç‡: {config.get('gpu_memory_utilization', 0.80)}")
    print(f"æœ€å¤§åºåˆ—é•¿åº¦: {config.get('max_model_len', 8192)}")
    
    print("=" * 60)
    print("\næ­£åœ¨åŠ è½½æ¨¡å‹...ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰")
    
    # è®¾ç½®PyTorchå†…å­˜åˆ†é…é…ç½®ï¼ˆé¿å…å†…å­˜ç¢ç‰‡å’ŒOOMï¼‰
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb:512"
    
    # è·å– tensor_parallel_size
    tensor_parallel_size = config.get('tensor_parallel_size', 2)
    
    # åœ¨ä½¿ç”¨å¤š GPU æ—¶ï¼ŒvLLM ä¼šä½¿ç”¨ Ray æ¥åè°ƒï¼Œéœ€è¦æ­£ç¡®åˆå§‹åŒ– Ray
    if tensor_parallel_size > 1:
        try:
            import ray
            # å…ˆå°è¯•åœæ­¢å¯èƒ½å­˜åœ¨çš„æ—§ Ray å®ä¾‹
            try:
                if ray.is_initialized():
                    print("ğŸ”§ æ£€æµ‹åˆ°å·²å­˜åœ¨çš„ Ray å®ä¾‹ï¼Œæ­£åœ¨å…³é—­...")
                    ray.shutdown()
                    import time
                    time.sleep(2)  # ç­‰å¾…æ¸…ç†å®Œæˆ
            except:
                pass
            
            # æ£€æŸ¥ Ray æ˜¯å¦å·²ç»åˆå§‹åŒ–
            if not ray.is_initialized():
                print("ğŸ”§ åˆå§‹åŒ– Ray é›†ç¾¤ï¼ˆç”¨äºå¤š GPU å¼ é‡å¹¶è¡Œï¼‰...")
                # æ˜¾å¼åˆå§‹åŒ– Rayï¼ŒæŒ‡å®š GPU æ•°é‡
                # æ³¨æ„ï¼šRay ä¼šä½¿ç”¨ CUDA_VISIBLE_DEVICES ä¸­æŒ‡å®šçš„ GPU
                # å‡å°‘ object_store_memory ä»¥é¿å…å†…å­˜é—®é¢˜
                ray.init(
                    ignore_reinit_error=True,
                    num_gpus=tensor_parallel_size,
                    num_cpus=tensor_parallel_size,  # æ¯ä¸ª GPU åˆ†é… 1 ä¸ª CPU
                    object_store_memory=1 * 10**9,  # é™ä½åˆ°1GBå¯¹è±¡å­˜å‚¨ï¼Œå‡å°‘å†…å­˜å‹åŠ›
                    _temp_dir="/tmp/ray",  # æŒ‡å®šä¸´æ—¶ç›®å½•
                    _system_config={
                        "object_timeout_milliseconds": 30000,  # 30ç§’è¶…æ—¶
                    }
                )
                print("âœ… Ray é›†ç¾¤åˆå§‹åŒ–å®Œæˆ")
            else:
                print("âœ… Ray é›†ç¾¤å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âš ï¸  Ray åˆå§‹åŒ–è­¦å‘Š: {e}")
            print("   å°†å°è¯•ç»§ç»­è¿è¡Œï¼ŒvLLM å¯èƒ½ä¼šè‡ªåŠ¨åˆå§‹åŒ– Ray")
    
    # åŠ è½½vLLMæ¨¡å‹ï¼ˆæ·»åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•ï¼‰
    llm_kwargs = {
        'model': config['model'],
        'tensor_parallel_size': tensor_parallel_size,
        'gpu_memory_utilization': config.get('gpu_memory_utilization', 0.80),
        'max_model_len': config.get('max_model_len', 8192),
        'trust_remote_code': True,
        'dtype': config.get('dtype', 'auto'),
        'enforce_eager': True,  # å¼ºåˆ¶ä½¿ç”¨ eager æ¨¡å¼ï¼Œé¿å… CUDA graph å¯¼è‡´çš„æ˜¾å­˜å³°å€¼
        'disable_log_stats': True,  # ç¦ç”¨ç»Ÿè®¡æ—¥å¿—ï¼Œå‡å°‘å¼€é”€
        'max_num_seqs': 16,  # é™åˆ¶å¹¶å‘åºåˆ—æ•°ï¼Œé™ä½æ˜¾å­˜å ç”¨
    }
    
    print("æ­£åœ¨åŠ è½½ vLLM æ¨¡å‹...")
    print(f"é…ç½®: gpu_memory_utilization={llm_kwargs['gpu_memory_utilization']}, max_model_len={llm_kwargs['max_model_len']}")
    
    try:
        llm = LLM(**llm_kwargs)
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ vLLM æ¨¡å‹åŠ è½½å¤±è´¥: {error_msg}")
        
        # å¦‚æœæ˜¯ OOM é”™è¯¯ï¼Œæä¾›å»ºè®®
        if "out of memory" in error_msg.lower() or "OOM" in error_msg:
            print("\nâš ï¸  æ˜¾å­˜ä¸è¶³ï¼å»ºè®®:")
            print("  1. é™ä½ gpu_memory_utilization (å½“å‰: {})".format(llm_kwargs['gpu_memory_utilization']))
            print("  2. é™ä½ max_model_len (å½“å‰: {})".format(llm_kwargs['max_model_len']))
            print("  3. ä½¿ç”¨å• GPU æ¨¡å¼: --tensor-parallel-size 1")
            print("  4. è®¾ç½®ç¯å¢ƒå˜é‡: export VLLM_GPU_MEM_UTIL=0.50")
            print("     è®¾ç½®ç¯å¢ƒå˜é‡: export VLLM_MAX_MODEL_LEN=4096")
        
        # å¦‚æœæ˜¯ Ray ç›¸å…³é”™è¯¯
        if "ray" in error_msg.lower() or "EngineCore" in error_msg:
            print("\nâš ï¸  Ray/EngineCore é”™è¯¯ï¼å»ºè®®:")
            print("  1. å…ˆè¿è¡Œ: ray stop")
            print("  2. ä½¿ç”¨å• GPU æ¨¡å¼: --tensor-parallel-size 1")
        
        raise
    
    # è®¾ç½®é‡‡æ ·å‚æ•°ï¼ˆå±‚æ¬¡å›¾è°±æ„å»ºéœ€è¦æ›´ç¨³å®šçš„è¾“å‡ºï¼‰
    sampling_params = SamplingParams(
        temperature=config.get('temperature', 0.2),
        top_p=config.get('top_p', 0.9),
        max_tokens=config.get('max_tokens', 2048),  # ç¤¾åŒºæŠ¥å‘Šå¯èƒ½è¾ƒé•¿
    )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("=" * 60)
    print("")
    
    # åˆ›å»ºçº¿ç¨‹é”ï¼Œç¡®ä¿ vLLM è°ƒç”¨çš„çº¿ç¨‹å®‰å…¨
    vllm_lock = threading.Lock()
    
    # åˆ›å»ºåŒæ­¥ç”Ÿæˆå‡½æ•°ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ï¼‰
    def generate_text_sync(prompt, system_prompt="", **kwargs):
        """
        åŒæ­¥ç”Ÿæˆæ–‡æœ¬å‡½æ•°ï¼ˆåŒ…è£…vLLMçš„åŒæ­¥è°ƒç”¨ï¼Œå¸¦é”™è¯¯å¤„ç†å’Œé‡è¯•ï¼‰
        
        å‚æ•°:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚ response_format ç­‰ï¼‰
        
        è¿”å›:
            ç”Ÿæˆçš„æ–‡æœ¬å­—ç¬¦ä¸²
        """
        import time
        
        # æ„å»ºå®Œæ•´çš„æç¤º
        full_prompt = ""
        if system_prompt:
            full_prompt += f"System: {system_prompt}\n\n"
        full_prompt += f"User: {prompt}\n\nAssistant: "
        
        # å¤„ç† JSON æ ¼å¼è¦æ±‚ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        current_sampling_params = sampling_params
        if kwargs.get('response_format', {}).get('type') == 'json_object':
            # å¯¹äºéœ€è¦ JSON è¾“å‡ºçš„æƒ…å†µï¼Œåœ¨æç¤ºä¸­æ·»åŠ è¦æ±‚
            full_prompt = full_prompt.rstrip("Assistant: ")
            full_prompt += "è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºã€‚\n\nAssistant: "
        
        # é‡è¯•é…ç½®
        max_retries = 3
        retry_delay = 2.0  # åˆå§‹å»¶è¿Ÿ2ç§’
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(max_retries):
            try:
                # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤ vLLM è°ƒç”¨
                with vllm_lock:
                    outputs = llm.generate([full_prompt], current_sampling_params)
                
                if not outputs or not outputs[0].outputs:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸  vLLM è¿”å›ç©ºç»“æœ (å°è¯• {attempt + 1}/{max_retries})ï¼Œç­‰å¾…åé‡è¯•...")
                        time.sleep(retry_delay * (attempt + 1))
                        continue
                    return ""
                
                return outputs[0].outputs[0].text.strip()
                
            except Exception as e:
                error_msg = str(e)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¼•æ“é”™è¯¯æˆ–è¶…æ—¶é”™è¯¯
                is_engine_error = "EngineCore" in error_msg or "EngineDeadError" in str(type(e))
                is_timeout_error = "timeout" in error_msg.lower() or "TimeoutError" in str(type(e))
                
                if (is_engine_error or is_timeout_error) and attempt < max_retries - 1:
                    wait_time = retry_delay * (attempt + 1)  # æŒ‡æ•°é€€é¿
                    print(f"âš ï¸  vLLM è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_msg[:150]}")
                    print(f"   ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    # æœ€åä¸€æ¬¡å°è¯•æˆ–éå¯é‡è¯•é”™è¯¯ï¼Œæ‰“å°é”™è¯¯å¹¶è¿”å›ç©ºå­—ç¬¦ä¸²
                    print(f"âŒ vLLM è°ƒç”¨æœ€ç»ˆå¤±è´¥: {error_msg[:200]}")
                    if attempt == max_retries - 1:
                        print(f"   å·²é‡è¯• {max_retries} æ¬¡ï¼Œæ”¾å¼ƒæœ¬æ¬¡è¯·æ±‚")
                    return ""
        
        return ""
    
    return generate_text_sync


def main():
    # å…ˆæ¸…ç†å¯èƒ½å­˜åœ¨çš„ Ray è¿›ç¨‹ï¼ˆé¿å… EngineCore é”™è¯¯ï¼‰
    print("ğŸ”§ æ¸…ç†å¯èƒ½å­˜åœ¨çš„ Ray è¿›ç¨‹...")
    try:
        import ray
        if ray.is_initialized():
            print("   æ£€æµ‹åˆ°å·²å­˜åœ¨çš„ Ray å®ä¾‹ï¼Œæ­£åœ¨å…³é—­...")
            ray.shutdown()
            import time
            time.sleep(3)
            print("   âœ… Ray å®ä¾‹å·²å…³é—­")
    except Exception as e:
        print(f"   Ray æ¸…ç†è·³è¿‡: {e}")
    
    # å°è¯•é€šè¿‡å‘½ä»¤è¡Œæ¸…ç† Ray
    try:
        import subprocess
        result = subprocess.run(["ray", "stop"], capture_output=True, timeout=10, text=True)
        if result.returncode == 0:
            print("   âœ… Ray è¿›ç¨‹å·²é€šè¿‡å‘½ä»¤è¡Œæ¸…ç†")
    except Exception as e:
        print(f"   å‘½ä»¤è¡Œæ¸…ç† Ray è·³è¿‡: {e}")
    
    # ç¡®ä¿åœ¨ä½¿ç”¨ CUDA ä¸å¤šè¿›ç¨‹å‰è®¾ç½®ä¸º spawn æ¨¡å¼
    try:
        multiprocessing.set_start_method("spawn", force=True)
    except RuntimeError:
        pass  # å·²è®¾ç½®è¿‡ï¼Œå¿½ç•¥

    # å¼ºåˆ¶ embedding ä½¿ç”¨ CPUï¼Œé¿å…ä¸ vLLM æŠ¢å æ˜¾å­˜
    print("ğŸ”§ é…ç½® Embedding ä½¿ç”¨ CPUï¼ˆé¿å…æ˜¾å­˜å†²çªï¼‰")
    os.environ["FORCE_CPU"] = "1"
    os.environ["EMB_MAX_WORKERS"] = "0"  # 0=ä¸²è¡Œï¼Œæ— å¤šè¿›ç¨‹
    os.environ["EMB_BATCH"] = "4"  # æ›´å°çš„batchï¼Œé™ä½å†…å­˜å ç”¨
    parser = argparse.ArgumentParser(description="æ„å»ºä¸­æ–‡æ³•å¾‹çŸ¥è¯†å›¾è°±ï¼ˆä½¿ç”¨æœ¬åœ° vLLM æ¨¡å‹ï¼‰")
    parser.add_argument(
        "-p",
        "--path",
        type=str,
        default="/newdataf/SJ/LeanRAG/output/social_law_7B_processed/",
        help="å»é‡åçš„å®ä½“ä¸å…³ç³»è¾“å‡ºç›®å½•ï¼Œéœ€åŒ…å« entity.jsonl ä¸ relation.jsonl",
    )
    # è¿™é‡Œå‚æ•°ä»ä¿ç•™ï¼Œä½†é»˜è®¤å’Œå®é™…è¿è¡Œéƒ½å¼ºåˆ¶ä¸ºå•å¡ï¼Œæ–¹ä¾¿ä½ åœ¨æœåŠ¡å™¨ä¸Šç”¨ç¬¬ 2 å¼ å¡ç¨³å®šè¿è¡Œ
    parser.add_argument(
        "--tensor-parallel-size",
        type=int,
        default=int(os.environ.get("VLLM_TENSOR_PARALLEL_SIZE", "1")),
        help="å¼ é‡å¹¶è¡ŒGPUæ•°é‡ï¼ˆå½“å‰è„šæœ¬å¼ºåˆ¶ä¸º 1ï¼Œå•å¡è¿è¡Œï¼‰",
    )
    args = parser.parse_args()

    working_dir = args.path.rstrip("/")
    # å¼ºåˆ¶å•å¡è¿è¡Œï¼Œå¿½ç•¥ä¼ å…¥çš„å¤§äº 1 çš„å€¼ï¼Œé¿å… Ray å¤šå¡å¯¼è‡´ EngineCore é”™è¯¯
    tensor_parallel_size = 1

    # è®¾ç½® build_graph æ¨¡å—çš„å…¨å±€ WORKING_DIRï¼ˆå…¶å±‚çº§èšç±»å‡½æ•°å†…éƒ¨ä¾èµ–ï¼‰
    bg.WORKING_DIR = working_dir

    # ============================================
    # é…ç½®æœ¬åœ° vLLM æ¨¡å‹
    # ============================================
    print("="*70)
    print(" æ„å»ºä¸­æ–‡æ³•å¾‹çŸ¥è¯†å›¾è°± - ä½¿ç”¨æœ¬åœ° vLLM æ¨¡å‹")
    print("="*70)
    
    # ä½¿ç”¨æœ¬åœ° Qwen2-7B-Instruct æ¨¡å‹ï¼ˆä¸æå–è„šæœ¬ law_extract_graphrag_parllar2_QWen7B.py ä¸€è‡´ï¼‰
    # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹è·¯å¾„
    if os.environ.get('VLLM_MODEL_PATH'):
        model_path = os.environ.get('VLLM_MODEL_PATH')
        print(f"ğŸ“ ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹è·¯å¾„: {model_path}")
    elif os.environ.get('VLLM_MODEL_NAME'):
        model_path = os.environ.get('VLLM_MODEL_NAME')
        print(f"ğŸ“ ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹åç§°: {model_path}")
    else:
        # é»˜è®¤ä½¿ç”¨æœ¬åœ° Qwen2-7B-Instruct æ¨¡å‹ï¼ˆä¸æå–è„šæœ¬ä¸€è‡´ï¼‰
        # æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼š/newdatad/WHH/MyEmoHH/models/Qwen2-7B-Instruct
        model_path = '/newdatad/WHH/MyEmoHH/models/Qwen2-7B-Instruct'
        print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_path}")
        print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ä¸æå–è„šæœ¬ç›¸åŒçš„æ¨¡å‹ï¼Œä¿è¯ä¸€è‡´æ€§")
        print(f"   é…ç½®: å½“å‰è„šæœ¬å›ºå®šä½¿ç”¨ tensor_parallel_size=1ï¼Œåœ¨ç¬¬ 2 å¼ å¡å•å¡è¿è¡Œ")
    
    # LLM é…ç½®ï¼ˆé’ˆå¯¹ 24GB 4090 ä¼˜åŒ–ï¼Œé¿å… EngineCore å´©æºƒï¼‰
    llm_config = {
        'model': model_path,
        'tensor_parallel_size': tensor_parallel_size,
        'gpu_memory_utilization': float(os.environ.get('VLLM_GPU_MEM_UTIL', '0.75')),  # è®¾ç½®ä¸º0.75ï¼ˆçº¦18GBï¼‰ï¼Œä¸ºç³»ç»Ÿå’Œç¼“å­˜é¢„ç•™ç©ºé—´
        'max_model_len': int(os.environ.get('VLLM_MAX_MODEL_LEN', '4096')),  # é™ä½åˆ°4096ï¼Œå‡å°‘KV cacheæ˜¾å­˜å ç”¨
        'temperature': float(os.environ.get('VLLM_TEMPERATURE', '0.2')),  # è¾ƒä½æ¸©åº¦ï¼Œæ›´ç¨³å®šçš„è¾“å‡º
        'top_p': float(os.environ.get('VLLM_TOP_P', '0.9')),
        'max_tokens': int(os.environ.get('VLLM_MAX_TOKENS', '1024')),  # é™ä½åˆ°1024ï¼Œå‡å°‘ç”Ÿæˆæ—¶æ˜¾å­˜å ç”¨
        'dtype': os.environ.get('VLLM_DTYPE', 'auto'),
    }
    
    print(f"\nğŸ’¡ æ˜¾å­˜é…ç½®:")
    print(f"   GPU å†…å­˜åˆ©ç”¨ç‡: {llm_config['gpu_memory_utilization']} (çº¦ {24 * llm_config['gpu_memory_utilization']:.1f}GB)")
    print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {llm_config['max_model_len']}")
    print(f"   æœ€å¤§ç”Ÿæˆé•¿åº¦: {llm_config['max_tokens']}")
    print(f"   Embedding: å¼ºåˆ¶ä½¿ç”¨ CPUï¼ˆé¿å…æ˜¾å­˜å†²çªï¼‰")
    
    # è®¾ç½® LLM
    print(f"\n{'='*60}")
    print("é…ç½® LLM ç”¨äºå±‚æ¬¡å›¾è°±æ„å»º")
    print(f"{'='*60}")
    
    try:
        use_llm_func = setup_vllm_sync(llm_config)
        print("âœ… LLM é…ç½®å®Œæˆ")
    except Exception as e:
        print(f"âŒ LLM é…ç½®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # ç»„è£…å…¨å±€é…ç½®
    global_config = {}
    global_config["max_workers"] = min(2, tensor_parallel_size * 2)  # æ ¹æ®GPUæ•°é‡è°ƒæ•´
    global_config["working_dir"] = working_dir
    global_config["use_llm_func"] = use_llm_func
    global_config["embeddings_func"] = bg.embedding
    global_config["special_community_report_llm_kwargs"] = field(
        default_factory=lambda: {"response_format": {"type": "json_object"}}
    )

    # åŸºç¡€å­˜åœ¨æ€§æ£€æŸ¥
    entity_path = os.path.join(working_dir, "entity.jsonl")
    relation_path = os.path.join(working_dir, "relation.jsonl")
    if not os.path.exists(entity_path) or not os.path.exists(relation_path):
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ï¼Œè¯·ç¡®è®¤å­˜åœ¨: {entity_path} å’Œ {relation_path}"
        )

    # è°ƒç”¨åŸæœ‰çš„å±‚çº§èšç±»æ„å›¾æµç¨‹
    print(f"\n{'='*70}")
    print("ğŸš€ å¼€å§‹æ„å»ºå±‚çº§çŸ¥è¯†å›¾è°±...")
    print(f"{'='*70}")
    print(f"ğŸ“‚ å·¥ä½œç›®å½•: {working_dir}")
    print(f"ğŸ“Š è¾“å…¥æ–‡ä»¶:")
    print(f"   - {entity_path}")
    print(f"   - {relation_path}")
    print(f"{'='*70}\n")
    
    try:
        bg.hierarchical_clustering(global_config)
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
        community_file = os.path.join(working_dir, "community.json")
        relations_file = os.path.join(working_dir, "generate_relations.json")
        
        print(f"\n{'='*70}")
        print("ğŸ‰ çŸ¥è¯†å›¾è°±æ„å»ºæˆåŠŸï¼")
        print(f"{'='*70}")
        
        if os.path.exists(community_file):
            file_size = os.path.getsize(community_file) / (1024 * 1024)  # MB
            print(f"âœ… ç¤¾åŒºæ–‡ä»¶å·²ç”Ÿæˆ: {community_file}")
            print(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        if os.path.exists(relations_file):
            file_size = os.path.getsize(relations_file) / (1024 * 1024)  # MB
            print(f"âœ… å…³ç³»æ–‡ä»¶å·²ç”Ÿæˆ: {relations_file}")
            print(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        print(f"\nğŸ“Š è¾“å‡ºç›®å½•: {working_dir}")
        print(f"{'='*70}")
        print("âœ¨ ç°åœ¨å¯ä»¥ä½¿ç”¨ query_law_graph_apikey.py è¿›è¡ŒæŸ¥è¯¢äº†ï¼")
        print(f"{'='*70}\n")
        
    except Exception as e:
        print(f"\n{'='*70}")
        print("âŒ çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥ï¼")
        print(f"{'='*70}")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        import traceback
        traceback.print_exc()
        print(f"{'='*70}\n")
        raise


if __name__ == "__main__":
    main()


