#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ··åˆRAGæ£€ç´¢ç³»ç»Ÿï¼šè¯­ä¹‰æ£€ç´¢ + æ–‡æœ¬ç›¸ä¼¼åº¦é‡æ’åº + çŸ¥è¯†å›¾è°±æ£€ç´¢

æµç¨‹ï¼š
1. è¯­ä¹‰å‘é‡æ£€ç´¢ Top10
2. BM25æ–‡æœ¬ç›¸ä¼¼åº¦é‡æ’åºï¼Œä½¿ç”¨æ··åˆæ£€ç´¢
3. æ¯”è¾ƒä¸¤æ¬¡æ’åºçš„ç›¸å…³æ€§ï¼ˆSpearmanç›¸å…³ç³»æ•°ï¼‰
4. å¦‚æœç›¸å…³æ€§é«˜ï¼ˆ>=é˜ˆå€¼ï¼‰ï¼Œç›´æ¥ä½¿ç”¨å‘é‡æ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
5. å¦‚æœç›¸å…³æ€§ä½ï¼ˆ<é˜ˆå€¼ï¼‰ï¼Œå¯ç”¨çŸ¥è¯†å›¾è°±æ£€ç´¢ï¼Œèåˆç»“æœåç”Ÿæˆç­”æ¡ˆ

ä¸ä»…ä»…æ˜¯ç®€å•ã€å¤æ‚ä¸¤ç§ã€‚ç­–ç•¥åˆ†ä¸ºå››ç§
"""

import json
import os
import sys
import time
import logging
import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import argparse

# âš ï¸ é‡è¦ï¼šå¿…é¡»åœ¨å¯¼å…¥ torch å’Œå…¶ä»–åº“ä¹‹å‰è®¾ç½® CUDA_VISIBLE_DEVICES
# å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES æŒ‡å®šGPUï¼Œé»˜è®¤ä½¿ç”¨GPU 1ï¼ˆç¬¬2å¼ å¡ï¼‰
if 'CUDA_VISIBLE_DEVICES' not in os.environ:
    os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # é»˜è®¤ä½¿ç”¨GPU 1ï¼ˆç¬¬2å¼ å¡ï¼‰
print(f"ğŸ”§ è®¾ç½®ä½¿ç”¨GPUå¡: CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}")

import numpy as np
from scipy.stats import spearmanr
from pymilvus import MilvusClient
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
import jieba
import torch

# å¯¼å…¥çŸ¥è¯†å›¾è°±æŸ¥è¯¢æ¨¡å—
from database_utils import (
    search_vector_search,
    find_tree_root,
    search_nodes_link,
    search_community,
    get_text_units,
)
from prompt import PROMPTS

# å¯¼å…¥æŸ¥è¯¢ç®€å•åº¦è¯„ä¼°æ¨¡å—
from query_simplicity_module import (
    measure_query_simplicity,
    calculate_combined_score_with_simplicity
)

# ==================== é…ç½®å‚æ•° ====================
# å‘é‡æ•°æ®åº“é…ç½®
VECTOR_DB_PATH = "/newdataf/SJ/LeanRAG/vectorDB/social_law_milvus.db"
COLLECTION_NAME = "social_law_chunks"

# çŸ¥è¯†å›¾è°±é…ç½®
KG_WORKING_DIR = "/newdataf/SJ/LeanRAG/KG_output/social_law_7B_processed/"

# Embeddingæ¨¡å‹é…ç½®
# SentenceTransformer ä¼šè‡ªåŠ¨ä» ~/.cache/huggingface/hub/ åŠ è½½ç¼“å­˜çš„æ¨¡å‹
# å·²è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œä¸ä¼šè¿æ¥ Hugging Face
EMBEDDING_MODEL = "BAAI/bge-m3"
EMBEDDING_DIM = 1024  # bge-m3çš„ç»´åº¦ä¹Ÿæ˜¯1024

# âš ï¸ é‡è¦ï¼šå¿…é¡»ä¸åˆ›å»ºå‘é‡åº“æ—¶ä½¿ç”¨çš„æ¨¡å‹ä¸€è‡´ï¼
# å¦‚æœå‘é‡åº“ä½¿ç”¨ bge-large-zh-v1.5 åˆ›å»ºï¼Œéœ€è¦é‡å»ºå‘é‡åº“æˆ–ä¿®æ”¹æ­¤å¤„ä¸º bge-large-zh-v1.5

# æ£€ç´¢å‚æ•°
TOP_K = 10  # æ£€ç´¢Top-K
CORRELATION_THRESHOLD = 0.35  # å¤æ‚åº¦é˜ˆå€¼ï¼ˆä¼˜åŒ–åï¼‰

# è®¾å¤‡é…ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


class HybridLegalRAG:
    """æ··åˆæ³•å¾‹RAGæ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(
        self,
        vector_db_path: str,
        collection_name: str,
        kg_working_dir: str,
        embedding_model_name: str,
        device: str = "cpu",
        llm_model_path: str = None,
        llm_params: dict = None,
        # æ¶ˆèå®éªŒå‚æ•°
        use_retrieval_only: bool = False,
        use_intrinsic_only: bool = False,
        fixed_topk: bool = False,
        flat_kg: bool = False
    ):
        """
        åˆå§‹åŒ–æ··åˆRAGç³»ç»Ÿ
        
        Args:
            vector_db_path: Milvuså‘é‡æ•°æ®åº“è·¯å¾„
            collection_name: Collectionåç§°
            kg_working_dir: çŸ¥è¯†å›¾è°±å·¥ä½œç›®å½•
            embedding_model_name: Embeddingæ¨¡å‹åç§°
            device: è®¾å¤‡ï¼ˆcpu/cudaï¼‰
            llm_model_path: LLMæ¨¡å‹è·¯å¾„
            llm_params: LLMå‚æ•°
            use_retrieval_only: æ¶ˆè1-åªç”¨æ£€ç´¢ä¸€è‡´æ€§è¯„ä¼°
            use_intrinsic_only: æ¶ˆè2-åªç”¨é—®é¢˜æœ¬è´¨è¯„ä¼°
            fixed_topk: æ¶ˆè3-å›ºå®šTop-Kæ–‡æ¡£æ•°é‡
            flat_kg: æ¶ˆè4-æ‰å¹³KGç»“æ„
        """
        self.logger = logging.getLogger(__name__)
        
        # ä¿å­˜æ¶ˆèå®éªŒæ ‡å¿—
        self.use_retrieval_only = use_retrieval_only
        self.use_intrinsic_only = use_intrinsic_only
        self.fixed_topk = fixed_topk
        self.flat_kg = flat_kg
        
        # è®°å½•æ¶ˆèå®éªŒæ¨¡å¼
        if any([use_retrieval_only, use_intrinsic_only, fixed_topk, flat_kg]):
            self.logger.info("ğŸ”¬ æ¶ˆèå®éªŒæ¨¡å¼:")
            if use_retrieval_only:
                self.logger.info("   - åªç”¨æ£€ç´¢ä¸€è‡´æ€§è¯„ä¼°")
            if use_intrinsic_only:
                self.logger.info("   - åªç”¨é—®é¢˜æœ¬è´¨è¯„ä¼°")
            if fixed_topk:
                self.logger.info("   - å›ºå®šTop-Kæ–‡æ¡£æ•°é‡")
            if flat_kg:
                self.logger.info("   - æ‰å¹³KGç»“æ„")
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.logger.info(f"æ­£åœ¨è¿æ¥å‘é‡æ•°æ®åº“: {vector_db_path}")
        
        # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
        import os
        vector_db_path = os.path.abspath(vector_db_path)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        db_dir = os.path.dirname(vector_db_path)
        if db_dir and not os.path.exists(db_dir):
            os.makedirs(db_dir, exist_ok=True)
            self.logger.info(f"   åˆ›å»ºç›®å½•: {db_dir}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(vector_db_path):
            self.logger.info(f"   ä½¿ç”¨å·²å­˜åœ¨çš„æ•°æ®åº“: {vector_db_path}")
        else:
            self.logger.warning(f"   âš ï¸ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {vector_db_path}")
            self.logger.warning(f"   å°†åˆ›å»ºæ–°çš„æ•°æ®åº“æ–‡ä»¶")
        
        # è¿æ¥ Milvusï¼ˆå‚è€ƒ database_utils.py çš„æ–¹å¼ï¼‰
        try:
            self.milvus_client = MilvusClient(uri=vector_db_path)
            self.logger.info(f"   âœ… æˆåŠŸè¿æ¥åˆ°å‘é‡æ•°æ®åº“")
        except Exception as e:
            self.logger.error(f"   âŒ è¿æ¥å¤±è´¥: {e}")
            self.logger.error(f"   è¯·ç¡®ä¿:")
            self.logger.error(f"   1. æ•°æ®åº“æ–‡ä»¶å­˜åœ¨: {vector_db_path}")
            self.logger.error(f"   2. pymilvus ç‰ˆæœ¬å…¼å®¹ (æ¨è 2.2.x)")
            self.logger.error(f"   3. å·²å®‰è£… milvus-lite: pip install milvus-lite")
            raise
        
        self.collection_name = collection_name
        
        # åˆå§‹åŒ–Embeddingæ¨¡å‹
        # SentenceTransformerä¼šè‡ªåŠ¨ä½¿ç”¨Hugging Faceçš„ç¼“å­˜æœºåˆ¶
        # é¦–æ¬¡è¿è¡Œä¼šä»Hugging Faceä¸‹è½½æ¨¡å‹åˆ° ~/.cache/huggingface/hub/
        # åç»­è¿è¡Œä¼šç›´æ¥ä»ç¼“å­˜åŠ è½½ï¼Œæ— éœ€é‡å¤ä¸‹è½½
        self.logger.info(f"æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {embedding_model_name}")
        self.embedding_model = SentenceTransformer(embedding_model_name, device=device)
        self.embedding_model.max_seq_length = 4096  # bge-m3æ”¯æŒæ›´é•¿çš„åºåˆ—
        
        # çŸ¥è¯†å›¾è°±é…ç½®
        self.kg_working_dir = kg_working_dir
        
        # æ„å»ºæ³•å¾‹è¯å…¸ï¼ˆæ”¹è¿›åˆ†è¯ï¼‰
        self._build_law_dictionary()
        
        # LLMé…ç½®
        self.llm = None
        self.sampling_params = None
        if llm_model_path:
            self._init_llm(llm_model_path, llm_params or {})
        
        self.logger.info("æ··åˆRAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _build_law_dictionary(self):
        """æ„å»ºæ³•å¾‹ä¸“ç”¨è¯å…¸ï¼Œæ”¹è¿›åˆ†è¯æ•ˆæœ"""
        self.logger.info("æ­£åœ¨æ„å»ºæ³•å¾‹è¯å…¸...")
        
        try:
            # å°è¯•ä»chunksæ–‡ä»¶æå–æ³•å¾‹æœ¯è¯­
            chunks_file = "datasets/chunks/basic_laws_social_only_chunk.json"
            if os.path.exists(chunks_file):
                with open(chunks_file, 'r', encoding='utf-8') as f:
                    chunks = json.load(f)
                
                law_names = set()
                article_numbers = set()
                
                for chunk in chunks:
                    source_name = chunk.get('source_name', '')
                    
                    # æå–æ³•å¾‹åç§°
                    match = re.search(r'ã€Š(.+?)ã€‹', source_name)
                    if match:
                        law_name = match.group(1)
                        law_names.add(law_name)
                        # æ·»åŠ ç®€ç§°
                        if law_name.startswith("ä¸­åäººæ°‘å…±å’Œå›½"):
                            short_name = law_name.replace("ä¸­åäººæ°‘å…±å’Œå›½", "")
                            law_names.add(short_name)
                    
                    # æå–æ³•æ¡å·
                    match = re.search(r'ç¬¬(.+?)æ¡', source_name)
                    if match:
                        article_num = match.group(1)
                        article_numbers.add(f"ç¬¬{article_num}æ¡")
                
                # æ·»åŠ åˆ°jieba
                for law in law_names:
                    jieba.add_word(law, freq=10000, tag='law')
                
                for article in article_numbers:
                    jieba.add_word(article, freq=10000, tag='article')
                
                self.logger.info(f"   âœ… æ·»åŠ  {len(law_names)} ä¸ªæ³•å¾‹åç§°, {len(article_numbers)} ä¸ªæ³•æ¡å·")
            else:
                self.logger.warning(f"   âš ï¸ chunksæ–‡ä»¶ä¸å­˜åœ¨: {chunks_file}")
        
        except Exception as e:
            self.logger.warning(f"   âš ï¸ æ„å»ºæ³•å¾‹è¯å…¸å¤±è´¥: {e}")
        
        # æ·»åŠ å¸¸è§æ³•å¾‹æœ¯è¯­
        common_terms = [
            "åŠ³åŠ¨æŠ¥é…¬", "åŠ ç­è´¹", "ç»æµè¡¥å¿", "åŠ³åŠ¨åˆåŒ", "åŠ³åŠ¨å…³ç³»",
            "ç”¨äººå•ä½", "åŠ³åŠ¨è€…", "ç¤¾ä¼šä¿é™©", "å·¥ä¼¤", "èŒä¸šç—…",
            "æœªæˆå¹´äºº", "ç›‘æŠ¤äºº", "å®‰å…¨ç”Ÿäº§", "æ³•å¾‹è´£ä»»", "è¡Œæ”¿å¤„ç½š",
        ]
        
        for term in common_terms:
            jieba.add_word(term, freq=5000, tag='term')
        
        self.logger.info(f"   âœ… æ·»åŠ  {len(common_terms)} ä¸ªå¸¸è§æ³•å¾‹æœ¯è¯­")
    
    def _init_llm(self, model_path: str, params: dict):
        """åˆå§‹åŒ–LLM"""
        from vllm import LLM, SamplingParams
        
        self.logger.info(f"æ­£åœ¨åŠ è½½LLMæ¨¡å‹: {model_path}")
        self.llm = LLM(
            model=model_path,
            tensor_parallel_size=params.get("tp", 1),
            gpu_memory_utilization=params.get("gpu_mem_util", 0.75),
            max_model_len=params.get("max_model_len", 8192),
            dtype="auto",
        )
        
        self.sampling_params = SamplingParams(
            temperature=params.get("temperature", 0.3),
            top_p=params.get("top_p", 0.9),
            max_tokens=params.get("max_new_tokens", 1024),
            repetition_penalty=1.1,
        )
        
        self.logger.info("LLMæ¨¡å‹åŠ è½½å®Œæˆ")
    
    def rewrite_query_for_consistency(self, query: str, instruction: str = "") -> str:
        """
        æ··åˆæŸ¥è¯¢é‡å†™ï¼šç®€å•é—®é¢˜ç”¨è§„åˆ™ï¼Œå¤æ‚é—®é¢˜ç”¨LLM
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        - ç®€å•é—®é¢˜ï¼šåŸºäºè§„åˆ™é‡å†™ï¼Œå»é™¤å£è¯­åŒ–è¡¨è¾¾ â†’ æé«˜ç›¸å…³ç³»æ•°
        - å¤æ‚é—®é¢˜ï¼šä½¿ç”¨LLMé‡å†™ï¼Œæå–å…³é”®ä¿¡æ¯ â†’ æé«˜ç›¸å…³ç³»æ•°
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            instruction: æŒ‡ä»¤
            
        Returns:
            é‡å†™åçš„æŸ¥è¯¢
        """
        original_query = query
        
        # ==================== æ­¥éª¤1: å°è¯•åŸºäºè§„åˆ™çš„é‡å†™ ====================
        # æ£€æµ‹ç®€å•æ¦‚å¿µæŸ¥è¯¢æ¨¡å¼å¹¶é‡å†™
        simple_patterns = [
            # åŸºæœ¬æ¦‚å¿µæŸ¥è¯¢
            (r'^ä»€ä¹ˆæ˜¯(.+?)[\?ï¼Ÿ]?$', r'\1çš„å®šä¹‰'),
            (r'^(.+?)çš„å®šä¹‰æ˜¯ä»€ä¹ˆ[\?ï¼Ÿ]?$', r'\1çš„å®šä¹‰'),
            (r'^(.+?)çš„å«ä¹‰æ˜¯ä»€ä¹ˆ[\?ï¼Ÿ]?$', r'\1çš„å«ä¹‰'),
            
            # åˆ†ç±»/åˆ—ä¸¾æŸ¥è¯¢
            (r'^(.+?)åŒ…æ‹¬å“ªäº›(.+?)[\?ï¼Ÿ]?$', r'\1çš„\2ç§ç±»'),
            (r'^(.+?)åˆ†ä¸ºå“ªå‡ ç§[\?ï¼Ÿ]?$', r'\1çš„åˆ†ç±»'),
            (r'^(.+?)åˆ†ä¸ºå“ª(.+?)[\?ï¼Ÿ]?$', r'\1çš„\2åˆ†ç±»'),
            (r'^(.+?)æœ‰å“ªäº›[\?ï¼Ÿ]?$', r'\1çš„ç§ç±»'),
            
            # ç«‹æ³•ç›®çš„/å®—æ—¨æŸ¥è¯¢
            (r'^(.+?æ³•)çš„ç«‹æ³•ç›®çš„æ˜¯ä»€ä¹ˆ[\?ï¼Ÿ]?$', r'\1ç¬¬ä¸€æ¡ç«‹æ³•ç›®çš„'),
            (r'^(.+?æ³•)çš„ç«‹æ³•å®—æ—¨æ˜¯ä»€ä¹ˆ[\?ï¼Ÿ]?$', r'\1ç¬¬ä¸€æ¡ç«‹æ³•å®—æ—¨'),
            (r'^(.+?æ³•)çš„åˆ¶å®šç›®çš„[\?ï¼Ÿ]?$', r'\1ç¬¬ä¸€æ¡ç«‹æ³•ç›®çš„'),
            
            # é€‚ç”¨èŒƒå›´æŸ¥è¯¢
            (r'^(.+?æ³•)é€‚ç”¨äºå“ªäº›(.+?)[\?ï¼Ÿ]?$', r'\1ç¬¬äºŒæ¡é€‚ç”¨èŒƒå›´ \2'),
            (r'^(.+?æ³•)é€‚ç”¨äºå“ª[\?ï¼Ÿ]?$', r'\1ç¬¬äºŒæ¡é€‚ç”¨èŒƒå›´'),
            (r'^(.+?æ³•)çš„é€‚ç”¨èŒƒå›´[\?ï¼Ÿ]?$', r'\1ç¬¬äºŒæ¡é€‚ç”¨èŒƒå›´'),
            
            # æƒåˆ©/ä¹‰åŠ¡æŸ¥è¯¢
            (r'^(.+?)äº«æœ‰å“ªäº›(.+?)æƒåˆ©[\?ï¼Ÿ]?$', r'\1çš„\2æƒåˆ©'),
            (r'^(.+?)æœ‰å“ªäº›(.+?)æƒåˆ©[\?ï¼Ÿ]?$', r'\1çš„\2æƒåˆ©'),
            (r'^(.+?)åº”å½“æ‰¿æ‹…å“ªäº›(.+?)ä¹‰åŠ¡[\?ï¼Ÿ]?$', r'\1çš„\2ä¹‰åŠ¡'),
            (r'^(.+?)çš„(.+?)æƒåˆ©æœ‰å“ªäº›[\?ï¼Ÿ]?$', r'\1çš„\2æƒåˆ©'),
        ]
        
        # å°è¯•åŒ¹é…ç®€å•æ¨¡å¼å¹¶é‡å†™
        for pattern, replacement in simple_patterns:
            match = re.match(pattern, query.strip())
            if match:
                rewritten = re.sub(pattern, replacement, query.strip())
                self.logger.info(f"   ğŸ”„ è§„åˆ™é‡å†™: '{original_query}' â†’ '{rewritten}'")
                return rewritten
        
        # æ£€æŸ¥æŒ‡ä»¤ä¸­æ˜¯å¦æœ‰"ç›´æ¥å¼•ç”¨"ç­‰å…³é”®è¯
        if instruction and any(keyword in instruction for keyword in ['ç›´æ¥å¼•ç”¨', 'å¼•ç”¨ç›¸å…³æ³•å¾‹æ¡æ–‡', 'å¼•ç”¨ç›¸å…³æ³•å¾‹', 'ç›´æ¥ç»™å‡ºæ³•æ¡']):
            cleaned = query
            for word in ['æ˜¯ä»€ä¹ˆ', 'æœ‰å“ªäº›', 'åŒ…æ‹¬', 'çš„', 'ï¼Ÿ', '?', 'å—', 'å‘¢']:
                cleaned = cleaned.replace(word, ' ')
            cleaned = ' '.join(cleaned.split())
            
            if cleaned != query and len(cleaned) > 2:
                self.logger.info(f"   ğŸ”„ è§„åˆ™ç®€åŒ–: '{original_query}' â†’ '{cleaned}'")
                return cleaned
        
        # ==================== æ­¥éª¤2: å¤æ‚é—®é¢˜ä½¿ç”¨LLMé‡å†™ ====================
        # æ£€æµ‹æ˜¯å¦ä¸ºå¤æ‚é—®é¢˜ï¼ˆåœºæ™¯æè¿°ã€æ¨ç†é—®é¢˜ã€å’¨è¯¢é—®é¢˜ï¼‰
        is_complex = self._is_complex_query(query, instruction)
        
        if is_complex and self.llm is not None:
            self.logger.info(f"   ğŸ¤– æ£€æµ‹åˆ°å¤æ‚æŸ¥è¯¢ï¼Œä½¿ç”¨LLMé‡å†™...")
            rewritten = self._llm_rewrite_query(query, instruction)
            if rewritten and rewritten != query:
                self.logger.info(f"   ğŸ”„ LLMé‡å†™: '{original_query}' â†’ '{rewritten}'")
                return rewritten
            else:
                self.logger.info(f"   âš ï¸ LLMé‡å†™å¤±è´¥æˆ–æ— å˜åŒ–ï¼Œä¿æŒåŸæŸ¥è¯¢")
        
        # å¦‚æœä¸æ˜¯ç®€å•é—®é¢˜ä¹Ÿä¸æ˜¯å¤æ‚é—®é¢˜ï¼Œæˆ–è€…LLMä¸å¯ç”¨ï¼Œä¿æŒåŸæ ·
        return query
    
    def _is_complex_query(self, query: str, instruction: str = "") -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºå¤æ‚æŸ¥è¯¢
        
        å¤æ‚æŸ¥è¯¢ç‰¹å¾ï¼š
        - åŒ…å«"åœºæ™¯"å…³é”®è¯
        - æŸ¥è¯¢é•¿åº¦è¶…è¿‡50å­—ç¬¦
        - åŒ…å«å¤šä¸ªå¥å­
        - æŒ‡ä»¤ä¸­åŒ…å«"åœºæ™¯"ã€"å’¨è¯¢"ã€"ç±»åˆ«"ç­‰å…³é”®è¯
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            instruction: æŒ‡ä»¤
            
        Returns:
            æ˜¯å¦ä¸ºå¤æ‚æŸ¥è¯¢
        """
        # ç‰¹å¾1: åŒ…å«"åœºæ™¯"å…³é”®è¯
        if 'åœºæ™¯' in query or 'åœºæ™¯:' in query or 'åœºæ™¯ï¼š' in query:
            return True
        
        # ç‰¹å¾2: æŸ¥è¯¢å¾ˆé•¿ï¼ˆè¶…è¿‡50å­—ç¬¦ï¼‰
        if len(query) > 50:
            return True
        
        # ç‰¹å¾3: åŒ…å«å¤šä¸ªå¥å­ï¼ˆå¤šä¸ªå¥å·ã€é—®å·ï¼‰
        sentence_count = query.count('ã€‚') + query.count('ï¼Ÿ') + query.count('?') + query.count('ï¼Œ')
        if sentence_count >= 2:
            return True
        
        # ç‰¹å¾4: æŒ‡ä»¤ä¸­åŒ…å«ç‰¹å®šå…³é”®è¯
        if instruction:
            complex_keywords = ['åœºæ™¯', 'å’¨è¯¢', 'ç±»åˆ«', 'ç¡®å®š', 'åˆ†ç±»', 'åˆ¤æ–­']
            if any(keyword in instruction for keyword in complex_keywords):
                return True
        
        # ç‰¹å¾5: åŒ…å«å…·ä½“çš„äººåã€åœ°åã€æ—¶é—´ç­‰ï¼ˆè¡¨ç¤ºå…·ä½“æ¡ˆä¾‹ï¼‰
        # ç®€å•æ£€æµ‹ï¼šåŒ…å«"æˆ‘"ã€"ä»–"ã€"å¥¹"ã€"å…¬å¸"ã€"å•ä½"ç­‰
        case_keywords = ['æˆ‘', 'ä»–', 'å¥¹', 'å…¬å¸', 'å•ä½', 'å·¥å‚', 'å­¦æ ¡', 'åŒ»é™¢']
        if any(keyword in query for keyword in case_keywords):
            return True
        
        return False
    
    def _llm_rewrite_query(self, query: str, instruction: str = "") -> str:
        """
        ä½¿ç”¨LLMé‡å†™å¤æ‚æŸ¥è¯¢
        
        ç›®æ ‡ï¼š
        - æå–æŸ¥è¯¢ä¸­çš„å…³é”®æ³•å¾‹æ¦‚å¿µ
        - å»é™¤å†—ä½™çš„åœºæ™¯æè¿°
        - ä¿ç•™æ ¸å¿ƒæ³•å¾‹é—®é¢˜
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            instruction: æŒ‡ä»¤
            
        Returns:
            é‡å†™åçš„æŸ¥è¯¢
        """
        if self.llm is None:
            return query
        
        # æ„å»ºé‡å†™æç¤ºè¯
        rewrite_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹æŸ¥è¯¢é‡å†™ä¸ºæ›´é€‚åˆæ³•å¾‹çŸ¥è¯†åº“æ£€ç´¢çš„å½¢å¼ã€‚

é‡å†™è¦æ±‚ï¼š
1. æå–æ ¸å¿ƒæ³•å¾‹æ¦‚å¿µå’Œå…³é”®è¯
2. å»é™¤å†—ä½™çš„åœºæ™¯æè¿°å’Œå£è¯­åŒ–è¡¨è¾¾
3. ä¿ç•™å…³é”®çš„æ³•å¾‹è¦ç´ ï¼ˆå¦‚ä¸»ä½“ã€è¡Œä¸ºã€åæœç­‰ï¼‰
4. é‡å†™åçš„æŸ¥è¯¢åº”è¯¥ç®€æ´ã€ç²¾ç¡®ï¼Œä¾¿äºæ£€ç´¢
5. å¦‚æœæ˜¯å’¨è¯¢ç±»é—®é¢˜ï¼Œæå–å…¶ä¸­æ¶‰åŠçš„æ³•å¾‹é—®é¢˜

åŸå§‹æŸ¥è¯¢ï¼š{query}

é‡å†™åçš„æŸ¥è¯¢ï¼ˆåªè¾“å‡ºé‡å†™ç»“æœï¼Œä¸è¦è§£é‡Šï¼‰ï¼š"""
        
        try:
            # ä½¿ç”¨LLMç”Ÿæˆé‡å†™
            outputs = self.llm.generate(
                [rewrite_prompt], 
                self.sampling_params
            )
            
            if outputs and outputs[0].outputs:
                rewritten = outputs[0].outputs[0].text.strip()
                
                # æ¸…ç†è¾“å‡ºï¼ˆå»é™¤å¯èƒ½çš„å‰ç¼€ï¼‰
                for prefix in ['é‡å†™åçš„æŸ¥è¯¢ï¼š', 'é‡å†™åï¼š', 'æŸ¥è¯¢ï¼š', 'ç­”ï¼š', 'A:', 'Answer:']:
                    if rewritten.startswith(prefix):
                        rewritten = rewritten[len(prefix):].strip()
                
                # éªŒè¯é‡å†™ç»“æœçš„åˆç†æ€§
                if len(rewritten) > 5 and len(rewritten) < len(query) * 2:
                    return rewritten
                else:
                    self.logger.warning(f"   âš ï¸ LLMé‡å†™ç»“æœä¸åˆç†: '{rewritten}'")
                    return query
            else:
                return query
                
        except Exception as e:
            self.logger.error(f"   âŒ LLMé‡å†™å¤±è´¥: {e}")
            return query
    
    def _is_classification_task(self, instruction: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡
        
        åˆ†ç±»ä»»åŠ¡ç‰¹å¾ï¼š
        - instructionä¸­åŒ…å«"ç¡®å®šä»¥ä¸‹å’¨è¯¢çš„ç±»åˆ«"
        - instructionä¸­åŒ…å«"ç±»åˆ«åŒ…æ‹¬"
        - instructionä¸­åŒ…å«"å°†ç­”æ¡ˆå†™åœ¨[ç±»åˆ«]"
        
        Args:
            instruction: æŒ‡ä»¤æ–‡æœ¬
            
        Returns:
            æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡
        """
        if not instruction:
            return False
        
        classification_keywords = [
            'ç¡®å®šä»¥ä¸‹å’¨è¯¢çš„ç±»åˆ«',
            'ç±»åˆ«åŒ…æ‹¬',
            'å°†ç­”æ¡ˆå†™åœ¨[ç±»åˆ«]',
            'ç¡®å®šå’¨è¯¢çš„ç±»åˆ«',
            'åˆ¤æ–­ä»¥ä¸‹å’¨è¯¢å±äº',
            'åˆ†ç±»ä¸ºä»¥ä¸‹ç±»åˆ«'
        ]
        
        return any(keyword in instruction for keyword in classification_keywords)
    
    def _add_classification_examples(self, instruction: str, question: str) -> str:
        """
        ä¸ºåˆ†ç±»ä»»åŠ¡æ·»åŠ Few-shotç¤ºä¾‹
        
        Args:
            instruction: åŸå§‹æŒ‡ä»¤
            question: é—®é¢˜æ–‡æœ¬
            
        Returns:
            å¢å¼ºåçš„æŒ‡ä»¤
        """
        # Few-shotç¤ºä¾‹ - é‡ç‚¹å¼ºè°ƒåŒ»ç–—çº çº·çš„åˆ¤æ–­æ ‡å‡†
        examples = """
ã€åˆ†ç±»ç¤ºä¾‹ã€‘ä»¥ä¸‹æ˜¯ä¸€äº›å…¸å‹æ¡ˆä¾‹ï¼Œå¸®åŠ©ä½ æ›´å‡†ç¡®åœ°åˆ¤æ–­ï¼š

ç¤ºä¾‹1 - åŒ»ç–—çº çº·:
é—®é¢˜ï¼šåŒ»ç–—äº‹æ•…èµ·è¯‰æœŸå¤šé•¿æ—¶é—´ï¼Ÿä¸¤å¹´å‰ï¼Œå­©å­å‡ºç”Ÿæ—¶è…¿éª¨æ–­äº†ï¼Œç°åœ¨èµ·è¯‰è¿˜æœ‰æ•ˆå—ï¼Ÿ
åˆ†æï¼šæ˜ç¡®æåˆ°"åŒ»ç–—äº‹æ•…"ï¼Œæ¶‰åŠåŒ»ç–—æœºæ„çš„åŒ»ç–—è¡Œä¸ºå¯¼è‡´çš„æŸå®³
ç±»åˆ«ï¼šåŒ»ç–—çº çº·

ç¤ºä¾‹2 - åŒ»ç–—çº çº·:
é—®é¢˜ï¼šæˆ‘åœ¨åŒ»é™¢åšç»“è‚²æ‰‹æœ¯è¢«åŒ»ç”ŸæŠ…äº†å¤§è‚ é€ æˆäº†ä¸‰çº§ä¼¤ç–¾ï¼Œè¿™èƒ½èµ”å¿å¤šå°‘è¡¥å¿
åˆ†æï¼šåŒ»ç”Ÿåœ¨æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„å¤±è¯¯å¯¼è‡´æŸå®³ï¼Œå±äºåŒ»ç–—è¡Œä¸ºå¯¼è‡´çš„æŸå®³
ç±»åˆ«ï¼šåŒ»ç–—çº çº·

ç¤ºä¾‹3 - åŠ³åŠ¨çº çº·ï¼ˆéåŒ»ç–—çº çº·ï¼‰:
é—®é¢˜ï¼šæˆ‘åœ¨åŒ»é™¢å·¥ä½œäº†ä¸€å¹´æ²¡æœ‰ç»™æˆ‘ç­¾åˆåŒï¼Œåé¢æƒ³èµ°äº†å«æˆ‘è¡¥ç­¾ï¼Œè¿™ä¸ªæ€ä¹ˆåšåˆæ³•ï¼Œæ€ä¹ˆè¦æ±‚èµ”å¿
åˆ†æï¼šè™½ç„¶åœ¨åŒ»é™¢å·¥ä½œï¼Œä½†æ ¸å¿ƒé—®é¢˜æ˜¯åŠ³åŠ¨åˆåŒã€åŠ³åŠ¨å…³ç³»ï¼Œä¸æ¶‰åŠåŒ»ç–—è¡Œä¸º
ç±»åˆ«ï¼šåŠ³åŠ¨çº çº·

ç¤ºä¾‹4 - äººèº«æŸå®³ï¼ˆå·¥ä¼¤ï¼ŒéåŒ»ç–—çº çº·ï¼‰:
é—®é¢˜ï¼šæˆ‘åœ¨å·¥åœ°æ‘”ä¼¤ï¼Œè¿›åŒ»é™¢åšäº†å¼€é¢…æ‰‹æœ¯ï¼ŒåŒ»è¯è´¹å·¥åœ°è€æ¿å«ä»˜äº†ï¼Œè¯·é—®æˆ‘åšæ³•åŒ»é‰´å®šè¿˜è¦æ³¨æ„äº›ä»€ä¹ˆï¼Ÿ
åˆ†æï¼šåœ¨å·¥åœ°å—ä¼¤å±äºå·¥ä¼¤ï¼Œè™½ç„¶åœ¨åŒ»é™¢æ²»ç–—ï¼Œä½†ä¸æ˜¯åŒ»ç–—è¡Œä¸ºå¯¼è‡´çš„æŸå®³
ç±»åˆ«ï¼šäººèº«æŸå®³

ç¤ºä¾‹5 - åŠ³åŠ¨çº çº·ï¼ˆå·¥ä¼¤ï¼‰:
é—®é¢˜ï¼šæˆ‘åœ¨å·¥åœ°å—ä¼¤ï¼Œè€æ¿ä¸ç»™èµ”å¿ï¼Œæ€ä¹ˆåŠï¼Ÿ
åˆ†æï¼šå·¥ä½œåœºæ‰€å—ä¼¤ï¼Œæ¶‰åŠå·¥ä¼¤èµ”å¿ã€åŠ³åŠ¨å…³ç³»
ç±»åˆ«ï¼šåŠ³åŠ¨çº çº·

ç¤ºä¾‹6 - æ¶ˆè´¹æƒç›Š:
é—®é¢˜ï¼šæˆ‘åœ¨è¶…å¸‚ä¹°çš„é¢åŒ…é‡Œé¢å‘éœ‰äº†ï¼Œæˆ‘ä¸å°å¿ƒåƒäº†ä¸€å£ï¼Œè¿˜åœ¨ä¿è´¨æœŸå†…è¯¥æ€ä¹ˆåŠï¼Ÿ
åˆ†æï¼šæ¶‰åŠå•†å“è´¨é‡ã€æ¶ˆè´¹è€…æƒç›Šã€é£Ÿå“å®‰å…¨
ç±»åˆ«ï¼šæ¶ˆè´¹æƒç›Š

ã€æ ¸å¿ƒåˆ¤æ–­æ ‡å‡†ã€‘
1. åŒ»ç–—çº çº·çš„å…³é”®ç‰¹å¾ï¼š
   âœ“ åŒ»ç–—æœºæ„çš„åŒ»ç–—è¡Œä¸ºï¼ˆè¯Šæ–­ã€æ²»ç–—ã€æ‰‹æœ¯ã€æŠ¤ç†ï¼‰å¯¼è‡´çš„æŸå®³
   âœ“ åŒ»ç”Ÿã€æŠ¤å£«ç­‰åŒ»åŠ¡äººå‘˜çš„åŒ»ç–—è¿‡é”™
   âœ“ æ˜ç¡®æåˆ°"åŒ»ç–—äº‹æ•…"ã€"åŒ»ç–—çº çº·"ã€"è¯¯è¯Š"ã€"æ‰‹æœ¯å¤±è¯¯"
   
2. ä¸æ˜¯åŒ»ç–—çº çº·çš„æƒ…å†µï¼š
   âœ— åœ¨åŒ»é™¢å·¥ä½œä½†æ¶‰åŠåŠ³åŠ¨å…³ç³»é—®é¢˜ â†’ åŠ³åŠ¨çº çº·
   âœ— å·¥ä¼¤ååœ¨åŒ»é™¢æ²»ç–— â†’ äººèº«æŸå®³æˆ–åŠ³åŠ¨çº çº·
   âœ— åœ¨åŒ»é™¢å‘ç”Ÿçš„éåŒ»ç–—è¡Œä¸ºå¯¼è‡´çš„ä¼¤å®³ â†’ äººèº«æŸå®³

3. äººèº«æŸå®³ vs åŠ³åŠ¨çº çº·ï¼š
   - äººèº«æŸå®³ï¼šä¾§é‡ä¼¤å®³èµ”å¿ã€ä¼¤æ®‹é‰´å®š
   - åŠ³åŠ¨çº çº·ï¼šä¾§é‡åŠ³åŠ¨å…³ç³»ã€å·¥èµ„ã€åˆåŒã€å·¥ä¼¤è®¤å®š

ç°åœ¨è¯·åˆ†ç±»ä»¥ä¸‹å’¨è¯¢ï¼š
"""
        
        # åœ¨instructionåæ·»åŠ ç¤ºä¾‹
        enhanced_instruction = instruction + "\n\n" + examples
        
        return enhanced_instruction
    
    def _detect_medical_dispute(self, question: str) -> Tuple[bool, float]:
        """
        æ£€æµ‹æ˜¯å¦ä¸ºåŒ»ç–—çº çº·ï¼ˆç‰¹æ®Šå¤„ç†ï¼‰
        
        åŒ»ç–—çº çº·çš„ç‰¹å¾ï¼š
        - åŒ…å«"åŒ»é™¢"ã€"åŒ»ç”Ÿ"ã€"æ‰‹æœ¯"ã€"æ²»ç–—"ç­‰åŒ»ç–—å…³é”®è¯
        - åŒæ—¶åŒ…å«"è¯¯ä¼¤"ã€"äº‹æ•…"ã€"å¤±è¯¯"ç­‰æŸå®³å…³é”®è¯
        - æˆ–è€…æ˜ç¡®æåˆ°"åŒ»ç–—äº‹æ•…"ã€"åŒ»ç–—çº çº·"
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            
        Returns:
            (is_medical, confidence): æ˜¯å¦ä¸ºåŒ»ç–—çº çº·åŠç½®ä¿¡åº¦
        """
        medical_keywords = ['åŒ»é™¢', 'åŒ»ç”Ÿ', 'æ‰‹æœ¯', 'æ²»ç–—', 'ç—…äºº', 'åŒ»ç–—', 'è¯Šæ–­', 'è¯å“', 'æŠ¤å£«']
        damage_keywords = ['è¯¯ä¼¤', 'äº‹æ•…', 'å¤±è¯¯', 'ä¼¤æ®‹', 'æ­»äº¡', 'æŸå®³', 'èµ”å¿']
        explicit_keywords = ['åŒ»ç–—äº‹æ•…', 'åŒ»ç–—çº çº·', 'åŒ»ç–—æŸå®³', 'åŒ»ç–—è¿‡é”™']
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜ç¡®çš„åŒ»ç–—çº çº·å…³é”®è¯
        if any(keyword in question for keyword in explicit_keywords):
            return True, 0.9
        
        # æ£€æŸ¥æ˜¯å¦åŒæ—¶åŒ…å«åŒ»ç–—å’ŒæŸå®³å…³é”®è¯
        has_medical = sum(1 for kw in medical_keywords if kw in question)
        has_damage = sum(1 for kw in damage_keywords if kw in question)
        
        if has_medical >= 1 and has_damage >= 1:
            # æ’é™¤å·¥ä¼¤åœºæ™¯ï¼ˆåœ¨åŒ»é™¢æ²»ç–—å·¥ä¼¤ä¸ç®—åŒ»ç–—çº çº·ï¼‰
            work_injury_keywords = ['å·¥åœ°', 'å·¥å‚', 'å·¥ä½œ', 'ä¸Šç­', 'å•ä½', 'å…¬å¸']
            has_work_injury = any(kw in question for kw in work_injury_keywords)
            
            if has_work_injury:
                # è¿›ä¸€æ­¥åˆ¤æ–­ï¼šå¦‚æœæ˜ç¡®æåˆ°åŒ»ç–—è¡Œä¸ºå¯¼è‡´çš„æŸå®³ï¼Œä»ç„¶æ˜¯åŒ»ç–—çº çº·
                if 'åŒ»ç”Ÿ' in question and any(kw in question for kw in ['è¯¯ä¼¤', 'å¤±è¯¯', 'äº‹æ•…']):
                    return True, 0.7
                else:
                    return False, 0.3
            else:
                confidence = min(0.8, 0.4 + has_medical * 0.1 + has_damage * 0.2)
                return True, confidence
        
        return False, 0.0
    
    def _extract_law_article_info(self, query: str) -> Tuple[str, str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–æ³•å¾‹åç§°å’Œæ¡æ–‡å·
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            (law_name, article_num): æ³•å¾‹åç§°å’Œæ¡æ–‡å·
        """
        law_name = ""
        article_num = ""
        
        # æå–æ³•å¾‹åç§°ï¼ˆã€Šxxxã€‹æ ¼å¼ï¼‰
        law_match = re.search(r'ã€Š(.+?)ã€‹', query)
        if law_match:
            law_name = law_match.group(1)
        else:
            # å°è¯•åŒ¹é…ä¸å¸¦ä¹¦åå·çš„æ³•å¾‹åç§°
            # å¸¸è§æ³•å¾‹åç§°åˆ—è¡¨
            law_patterns = [
                r'(ä¸­åäººæ°‘å…±å’Œå›½)?åŠ³åŠ¨åˆåŒæ³•',
                r'(ä¸­åäººæ°‘å…±å’Œå›½)?åŠ³åŠ¨æ³•',
                r'(ä¸­åäººæ°‘å…±å’Œå›½)?ç¤¾ä¼šä¿é™©æ³•',
                r'(ä¸­åäººæ°‘å…±å’Œå›½)?æœªæˆå¹´äººä¿æŠ¤æ³•',
                r'(ä¸­åäººæ°‘å…±å’Œå›½)?å¦‡å¥³æƒç›Šä¿éšœæ³•',
                r'(ä¸­åäººæ°‘å…±å’Œå›½)?è€å¹´äººæƒç›Šä¿éšœæ³•',
                r'(ä¸­åäººæ°‘å…±å’Œå›½)?é£Ÿå“å®‰å…¨æ³•',
                r'(ä¸­åäººæ°‘å…±å’Œå›½)?è¯å“ç®¡ç†æ³•',
                r'(ä¸­åäººæ°‘å…±å’Œå›½)?èŒä¸šç—…é˜²æ²»æ³•',
                r'(ä¸­åäººæ°‘å…±å’Œå›½)?æ•™è‚²æ³•',
                r'(ä¸­åäººæ°‘å…±å’Œå›½)?å®‰å…¨ç”Ÿäº§æ³•',
                r'åŠ³åŠ¨åˆåŒæ³•å®æ–½æ¡ä¾‹',
            ]
            for pattern in law_patterns:
                match = re.search(pattern, query)
                if match:
                    law_name = match.group(0)
                    # è¡¥å…¨ä¸ºå®Œæ•´åç§°
                    if not law_name.startswith("ä¸­åäººæ°‘å…±å’Œå›½") and "å®æ–½æ¡ä¾‹" not in law_name:
                        law_name = "ä¸­åäººæ°‘å…±å’Œå›½" + law_name
                    break
        
        # æå–æ¡æ–‡å·ï¼ˆç¬¬xxæ¡ï¼‰
        article_match = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+)æ¡', query)
        if article_match:
            article_num = article_match.group(1)
        
        return law_name, article_num
    
    def semantic_search(self, query: str, top_k: int = 10, rewritten_query: str = None) -> List[Dict]:
        """
        è¯­ä¹‰å‘é‡æ£€ç´¢
        
        Args:
            query: åŸå§‹æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›Top-Kç»“æœ
            rewritten_query: é‡å†™åçš„æŸ¥è¯¢ï¼ˆå¦‚æœæä¾›ï¼Œç”¨äºembeddingï¼‰
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«text, semantic_score, semantic_rank, chunk_id
        """
        self.logger.info(f"æ­¥éª¤1: è¯­ä¹‰å‘é‡æ£€ç´¢ Top{top_k}...")
        
        # ä½¿ç”¨é‡å†™åçš„æŸ¥è¯¢ï¼ˆå¦‚æœæœ‰ï¼‰
        search_query = rewritten_query if rewritten_query else query
        
        if rewritten_query and rewritten_query != query:
            self.logger.info(f"   [è°ƒè¯•] ä½¿ç”¨é‡å†™æŸ¥è¯¢è¿›è¡Œembedding: '{search_query}'")
        
        # ç”Ÿæˆquery embeddingï¼ˆä½¿ç”¨é‡å†™åçš„æŸ¥è¯¢ï¼‰
        query_embedding = self.embedding_model.encode(
            search_query,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        
        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 10}
        }
        
        # æ‰§è¡Œè¯­ä¹‰æ£€ç´¢
        results = self.milvus_client.search(
            collection_name=self.collection_name,
            data=[query_embedding.tolist()],
            anns_field="vector",
            limit=top_k,
            output_fields=["text", "hash_code", "source_name"],
            search_params=search_params
        )
        
        semantic_results = []
        for i, hit in enumerate(results[0]):
            semantic_results.append({
                'text': hit['entity'].get('text', ''),
                'semantic_score': hit['distance'],
                'semantic_rank': i,
                'hash_code': hit['entity'].get('hash_code', f'chunk_{i}'),
                'source_name': hit['entity'].get('source_name', ''),
            })
        
        self.logger.info(f"   âœ… æ£€ç´¢åˆ° {len(semantic_results)} ä¸ªç»“æœ")
        return semantic_results
    
    def bm25_search(self, query: str, top_k: int = 10, rewritten_query: str = None) -> List[Dict]:
        """
        ç‹¬ç«‹çš„BM25æ£€ç´¢ï¼ˆä¸ä¾èµ–è¯­ä¹‰æ£€ç´¢ç»“æœï¼‰
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        - ä»å‘é‡åº“ä¸­æ£€ç´¢æ‰€æœ‰å€™é€‰æ–‡æ¡£ï¼ˆTop-100ï¼‰
        - ä½¿ç”¨BM25å¯¹æ‰€æœ‰å€™é€‰æ–‡æ¡£æ’åº
        - è¿”å›Top-Kç»“æœ
        
        è¿™æ ·å¯ä»¥ä¸è¯­ä¹‰æ£€ç´¢å®Œå…¨ç‹¬ç«‹ï¼Œæ›´å¥½åœ°åˆ¤æ–­æ˜¯å¦éœ€è¦KG
        
        Args:
            query: åŸå§‹æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›Top-Kç»“æœ
            rewritten_query: é‡å†™åçš„æŸ¥è¯¢
            
        Returns:
            BM25æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        self.logger.info(f"æ­¥éª¤2: ç‹¬ç«‹BM25æ£€ç´¢ Top{top_k}...")
        
        # ä½¿ç”¨é‡å†™åçš„æŸ¥è¯¢ï¼ˆå¦‚æœæœ‰ï¼‰
        search_query = rewritten_query if rewritten_query else query
        if rewritten_query and rewritten_query != query:
            self.logger.info(f"   [è°ƒè¯•] ä½¿ç”¨é‡å†™æŸ¥è¯¢è¿›è¡ŒBM25: '{search_query}'")
        
        # å…ˆç”¨è¯­ä¹‰æ£€ç´¢è·å–å€™é€‰æ–‡æ¡£æ± ï¼ˆTop-100ï¼Œæ‰©å¤§å€™é€‰èŒƒå›´ï¼‰
        query_embedding = self.embedding_model.encode(
            search_query,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        
        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 10}
        }
        
        # æ£€ç´¢Top-200ä½œä¸ºå€™é€‰æ± ï¼ˆæ‰©å¤§å€™é€‰èŒƒå›´ï¼‰
        candidate_size = min(200, top_k * 20)
        results = self.milvus_client.search(
            collection_name=self.collection_name,
            data=[query_embedding.tolist()],
            anns_field="vector",
            limit=candidate_size,
            output_fields=["text", "hash_code", "source_name"],
            search_params=search_params
        )
        
        self.logger.info(f"   è·å– {len(results[0])} ä¸ªå€™é€‰æ–‡æ¡£")
        
        # æå–æ–‡æ¡£æ–‡æœ¬
        docs = [hit['entity'].get('text', '') for hit in results[0]]
        
        # åˆ†è¯
        tokenized_docs = [list(jieba.cut(doc)) for doc in docs]
        tokenized_query = list(jieba.cut(search_query))
        
        self.logger.info(f"   æŸ¥è¯¢åˆ†è¯: {tokenized_query[:10]}...")
        
        # BM25è®¡ç®—ï¼ˆä½¿ç”¨å›ºå®šå‚æ•°ï¼‰
        bm25 = BM25Okapi(tokenized_docs, k1=1.5, b=0.75)
        bm25_scores = bm25.get_scores(tokenized_query)
        
        # æ„å»ºç»“æœåˆ—è¡¨
        bm25_results = []
        for i, hit in enumerate(results[0]):
            bm25_results.append({
                'text': hit['entity'].get('text', ''),
                'bm25_score': float(bm25_scores[i]),
                'bm25_rank': 0,  # ç¨åæ’åºåæ›´æ–°
                'hash_code': hit['entity'].get('hash_code', f'chunk_{i}'),
                'source_name': hit['entity'].get('source_name', ''),
            })
        
        # æŒ‰BM25åˆ†æ•°æ’åº
        bm25_results = sorted(bm25_results, key=lambda x: x['bm25_score'], reverse=True)
        
        # æ›´æ–°æ’å
        for i, result in enumerate(bm25_results):
            result['bm25_rank'] = i
        
        # è¿”å›Top-K
        bm25_results = bm25_results[:top_k]
        
        self.logger.info(f"   âœ… BM25æ£€ç´¢å®Œæˆ")
        # æ˜¾ç¤ºTop3åˆ†æ•°
        top3_bm25 = [f"{r['bm25_score']:.1f}" for r in bm25_results[:3]]
        self.logger.info(f"   Top3 BM25åˆ†æ•°: {top3_bm25}")
        
        return bm25_results
    
    def create_hybrid_results(self, semantic_results: List[Dict], bm25_results: List[Dict], alpha: float = 0.7) -> List[Dict]:
        """
        åˆ›å»ºæ··åˆæ£€ç´¢ç»“æœï¼ˆèåˆè¯­ä¹‰æ£€ç´¢å’ŒBM25æ£€ç´¢ï¼‰
        
        Args:
            semantic_results: è¯­ä¹‰æ£€ç´¢ç»“æœ
            bm25_results: BM25æ£€ç´¢ç»“æœ
            alpha: è¯­ä¹‰æƒé‡ï¼ˆ0-1ï¼‰ï¼Œæ¨è0.7è¡¨ç¤º70%è¯­ä¹‰+30%BM25
            
        Returns:
            æ··åˆæ£€ç´¢ç»“æœåˆ—è¡¨
        """
        self.logger.info(f"æ­¥éª¤4: åˆ›å»ºæ··åˆæ£€ç´¢ç»“æœ (alpha={alpha})...")
        
        # åˆ›å»ºhash_codeåˆ°ç»“æœçš„æ˜ å°„
        semantic_map = {r['hash_code']: r for r in semantic_results}
        bm25_map = {r['hash_code']: r for r in bm25_results}
        
        # åˆå¹¶æ‰€æœ‰æ–‡æ¡£
        all_hash_codes = set(semantic_map.keys()) | set(bm25_map.keys())
        
        hybrid_results = []
        
        for hash_code in all_hash_codes:
            # è·å–è¯­ä¹‰åˆ†æ•°å’ŒBM25åˆ†æ•°
            semantic_score = semantic_map[hash_code]['semantic_score'] if hash_code in semantic_map else 0.0
            bm25_score = bm25_map[hash_code]['bm25_score'] if hash_code in bm25_map else 0.0
            
            # è·å–æ–‡æ¡£ä¿¡æ¯ï¼ˆä¼˜å…ˆä»è¯­ä¹‰æ£€ç´¢ç»“æœï¼‰
            if hash_code in semantic_map:
                doc_info = semantic_map[hash_code]
            else:
                doc_info = bm25_map[hash_code]
            
            hybrid_results.append({
                'text': doc_info['text'],
                'hash_code': hash_code,
                'source_name': doc_info['source_name'],
                'semantic_score': semantic_score,
                'bm25_score': bm25_score,
            })
        
        # å½’ä¸€åŒ–åˆ†æ•°
        semantic_scores = [r['semantic_score'] for r in hybrid_results]
        bm25_scores = [r['bm25_score'] for r in hybrid_results]
        
        sem_min, sem_max = min(semantic_scores), max(semantic_scores)
        if sem_max > sem_min:
            semantic_norm = [(s - sem_min) / (sem_max - sem_min) for s in semantic_scores]
        else:
            semantic_norm = [1.0] * len(semantic_scores)
        
        bm25_min, bm25_max = min(bm25_scores), max(bm25_scores)
        if bm25_max > bm25_min:
            bm25_norm = [(s - bm25_min) / (bm25_max - bm25_min) for s in bm25_scores]
        else:
            bm25_norm = [1.0] * len(bm25_scores)
        
        # è®¡ç®—æ··åˆåˆ†æ•°
        for i, result in enumerate(hybrid_results):
            result['semantic_norm'] = semantic_norm[i]
            result['bm25_norm'] = bm25_norm[i]
            result['hybrid_score'] = alpha * semantic_norm[i] + (1 - alpha) * bm25_norm[i]
        
        # æŒ‰æ··åˆåˆ†æ•°æ’åº
        hybrid_results = sorted(hybrid_results, key=lambda x: x['hybrid_score'], reverse=True)
        
        # æ›´æ–°æ’å
        for i, result in enumerate(hybrid_results):
            result['hybrid_rank'] = i
        
        self.logger.info(f"   âœ… æ··åˆæ£€ç´¢å®Œæˆï¼Œå…± {len(hybrid_results)} ä¸ªæ–‡æ¡£")
        # æ˜¾ç¤ºTop3åˆ†æ•°
        top3_hybrid = [f"{r['hybrid_score']:.3f}" for r in hybrid_results[:3]]
        self.logger.info(f"   Top3 æ··åˆåˆ†æ•°: {top3_hybrid}")
        
        return hybrid_results
    
    def compare_independent_rankings(self, semantic_results: List[Dict], bm25_results: List[Dict], query: str) -> Dict:
        """
        å¤šç»´åº¦æ¯”è¾ƒä¸¤ä¸ªç‹¬ç«‹æ£€ç´¢ç»“æœçš„ç›¸ä¼¼åº¦ï¼ˆ8ä¸ªæŒ‡æ ‡ï¼Œ4ä¸ªç»´åº¦ï¼‰
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        - å¦‚æœä¸¤ç§æ–¹æ³•æ£€ç´¢åˆ°çš„æ–‡æ¡£é‡å åº¦é«˜ â†’ ç®€å•é—®é¢˜ï¼Œä¼ ç»ŸRAGè¶³å¤Ÿ
        - å¦‚æœä¸¤ç§æ–¹æ³•æ£€ç´¢åˆ°çš„æ–‡æ¡£å·®å¼‚å¤§ â†’ å¤æ‚é—®é¢˜ï¼Œéœ€è¦KG
        
        Args:
            semantic_results: è¯­ä¹‰æ£€ç´¢ç»“æœ
            bm25_results: BM25æ£€ç´¢ç»“æœ
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            åŒ…å«æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        self.logger.info("æ­¥éª¤3: æ¯”è¾ƒä¸¤ç§æ£€ç´¢ç»“æœ...")
        
        metrics = {}
        
        # æå–æ–‡æ¡£IDåˆ—è¡¨
        semantic_ids = [r['hash_code'] for r in semantic_results]
        bm25_ids = [r['hash_code'] for r in bm25_results]
        overlap_ids = set(semantic_ids) & set(bm25_ids)
        
        # ========== æ ¸å¿ƒæŒ‡æ ‡1: BM25 Top1åˆ†æ•° ==========
        if len(bm25_results) > 0:
            metrics['bm25_top1_score'] = bm25_results[0].get('bm25_score', 10.0)
        else:
            metrics['bm25_top1_score'] = 10.0
        
        # ========== æ ¸å¿ƒæŒ‡æ ‡2: æ–‡æ¡£é‡å ç‡ ==========
        metrics['overlap_ratio'] = len(overlap_ids) / len(semantic_ids) if len(semantic_ids) > 0 else 0.0
        self.logger.info(f"   - æ–‡æ¡£é‡å ç‡: {metrics['overlap_ratio']:.3f} ({len(overlap_ids)}/{len(semantic_ids)})")
        
        # ========== æ ¸å¿ƒæŒ‡æ ‡3: Top-3é‡å ç‡ ==========
        top3_semantic = set(semantic_ids[:3])
        top3_bm25 = set(bm25_ids[:3])
        top3_overlap = len(top3_semantic & top3_bm25)
        metrics['top3_overlap'] = top3_overlap / 3
        self.logger.info(f"   - Top-3é‡å ç‡: {metrics['top3_overlap']:.3f} ({top3_overlap}/3)")
        
        # ========== è®¡ç®—ç»¼åˆåˆ†æ•° ==========
        try:
            metrics['combined_score'] = self._calculate_combined_score(metrics, query=query)
            
            self.logger.info(f"\n   ğŸ“Š ç»¼åˆç›¸ä¼¼åº¦: {metrics['combined_score']:.3f}")
            self.logger.info(f"   è§£è¯»: {'ç®€å•é—®é¢˜ï¼Œä¼ ç»ŸRAGè¶³å¤Ÿ' if metrics['combined_score'] >= 0.7 else 'å¤æ‚é—®é¢˜ï¼Œéœ€è¦KGè¾…åŠ©'}")
        except Exception as e:
            self.logger.error(f"   âŒ è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
            self.logger.error(f"   æŒ‡æ ‡å€¼: {metrics}")
            metrics['combined_score'] = 0.0
        
        return metrics
    

    
    def _calculate_combined_score(self, metrics: Dict, query: str = "") -> float:
        """
        è®¡ç®—ç»¼åˆå¤æ‚åº¦åˆ†æ•°ï¼ˆé›†æˆç»Ÿä¸€å¤æ‚åº¦è¯„ä¼°ï¼‰
        
        æ–°ç‰ˆæœ¬ï¼šä½¿ç”¨ç»Ÿä¸€å¤æ‚åº¦è¯„åˆ†ä½“ç³»
        - ä¸‰å±‚è¯„ä¼°æ¶æ„ï¼šé—®é¢˜ç±»å‹åˆ†ç±» â†’ å¤æ‚é—®é¢˜ç»†åˆ† â†’ äº”ç»´åº¦è¯„ä¼°
        - é—®é¢˜æœ¬è´¨å¤æ‚åº¦ = ç›´æ¥è®¡ç®—ï¼ˆåˆ†æ•°è¶Šé«˜è¶Šå¤æ‚ï¼‰
        - æ£€ç´¢ä¸ä¸€è‡´æ€§ = åè½¬åçš„RCCè®¡ç®—ï¼ˆåˆ†æ•°è¶Šé«˜è¶Šä¸ä¸€è‡´ï¼‰
        - æœ€ç»ˆå¤æ‚åº¦ = 0.5 Ã— é—®é¢˜æœ¬è´¨å¤æ‚åº¦ + 0.5 Ã— æ£€ç´¢ä¸ä¸€è‡´æ€§
        - å†³ç­–è§„åˆ™ï¼šæœ€ç»ˆå¤æ‚åº¦ â‰¥ 0.4 â†’ KGï¼›< 0.4 â†’ ä¼ ç»ŸRAG
        
        Args:
            metrics: æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            æœ€ç»ˆå¤æ‚åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰ï¼Œåˆ†æ•°è¶Šé«˜è¶Šå¤æ‚
        """
        # å®‰å…¨è·å–æŒ‡æ ‡å€¼
        def safe_get(key, default=0.0):
            val = metrics.get(key, default)
            if val is None or (isinstance(val, float) and np.isnan(val)):
                return default
            return float(val)
        
        bm25_top1 = safe_get('bm25_top1_score', 10.0)
        overlap_ratio = safe_get('overlap_ratio')
        top3_overlap = safe_get('top3_overlap')
        
        # ğŸ”¬ æ¶ˆèå®éªŒ1ï¼šåªç”¨æ£€ç´¢ä¸€è‡´æ€§
        if self.use_retrieval_only:
            self.logger.info("   ğŸ”¬ æ¶ˆèæ¨¡å¼ï¼šåªç”¨æ£€ç´¢ä¸€è‡´æ€§è¯„ä¼°")
            # åªè®¡ç®—æ£€ç´¢ä¸ä¸€è‡´æ€§
            result = calculate_combined_score_with_simplicity(
                query=query,
                bm25_top1_score=bm25_top1,
                overlap_ratio=overlap_ratio,
                top3_overlap=top3_overlap,
                llm=None,  # ä¸ä½¿ç”¨LLM
                sampling_params=None,
                use_five_dimensions=False
            )
            # åªè¿”å›æ£€ç´¢ä¸ä¸€è‡´æ€§
            metrics.update(result)
            return result.get('retrieval_inconsistency', 0.5)
        
        # ğŸ”¬ æ¶ˆèå®éªŒ2ï¼šåªç”¨é—®é¢˜æœ¬è´¨
        if self.use_intrinsic_only:
            self.logger.info("   ğŸ”¬ æ¶ˆèæ¨¡å¼ï¼šåªç”¨é—®é¢˜æœ¬è´¨è¯„ä¼°")
            # åªè®¡ç®—é—®é¢˜æœ¬è´¨å¤æ‚åº¦
            result = calculate_combined_score_with_simplicity(
                query=query,
                bm25_top1_score=bm25_top1,
                overlap_ratio=overlap_ratio,
                top3_overlap=top3_overlap,
                llm=self.llm,
                sampling_params=self.sampling_params,
                use_five_dimensions=True
            )
            # åªè¿”å›é—®é¢˜æœ¬è´¨å¤æ‚åº¦
            metrics.update(result)
            return result.get('question_nature_complexity', 0.5)
        
        # æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨å®Œæ•´è¯„ä¼°
        result = calculate_combined_score_with_simplicity(
            query=query,
            bm25_top1_score=bm25_top1,
            overlap_ratio=overlap_ratio,
            top3_overlap=top3_overlap,
            llm=self.llm,  # ä¼ å…¥LLMå®ä¾‹
            sampling_params=self.sampling_params,
            use_five_dimensions=True  # å¯ç”¨ç»Ÿä¸€å¤æ‚åº¦è¯„ä¼°
        )
        
        # è®°å½•è¯¦ç»†ä¿¡æ¯
        self.logger.info(f"\n   ğŸ“Š ç»Ÿä¸€å¤æ‚åº¦è¯„ä¼°:")
        
        # å­é—®é¢˜æ‹†åˆ†ä¿¡æ¯
        if result.get('decomposition'):
            decomp = result['decomposition']
            self.logger.info(f"      å­é—®é¢˜æ‹†åˆ†: {decomp['num_sub_questions']}ä¸ªå­é—®é¢˜")
            if decomp['num_sub_questions'] > 1:
                for i, sq in enumerate(decomp['sub_questions'][:3], 1):
                    self.logger.info(f"        {i}. {sq}")
                if len(decomp['sub_questions']) > 3:
                    self.logger.info(f"        ...")
        
        # é—®é¢˜ç±»å‹ä¿¡æ¯
        self.logger.info(f"      é—®é¢˜ç±»å‹: {result['question_type']} (ç¬¬{result['evaluation_layer']}å±‚è¯„ä¼°)")
        
        # é—®é¢˜æœ¬è´¨å¤æ‚åº¦
        self.logger.info(f"      é—®é¢˜æœ¬è´¨å¤æ‚åº¦: {result['question_nature_complexity']:.3f}")
        if result.get('base_complexity') is not None:
            self.logger.info(f"        - åŸºç¡€å¤æ‚åº¦: {result['base_complexity']:.3f}")
        if result.get('five_dimension_score') is not None:
            self.logger.info(f"        - äº”ç»´åº¦åˆ†æ•°: {result['five_dimension_score']:.3f}")
        
        # æ£€ç´¢ä¸ä¸€è‡´æ€§
        self.logger.info(f"      æ£€ç´¢ä¸ä¸€è‡´æ€§: {result['retrieval_inconsistency']:.3f}")
        
        # æœ€ç»ˆå¤æ‚åº¦
        self.logger.info(f"      æœ€ç»ˆå¤æ‚åº¦: {result['final_complexity']:.3f} (é˜ˆå€¼={result['threshold']})")
        self.logger.info(f"      å†³ç­–: {result['decision']}")
        self.logger.info(f"      ç†ç”±: {result['reason']}")
        
        # å°†è¯¦ç»†ç»“æœä¿å­˜åˆ°metricsä¸­
        metrics.update(result)
        
        # è¿”å›æœ€ç»ˆå¤æ‚åº¦ï¼ˆæ³¨æ„ï¼šæ–°ç‰ˆæœ¬ä½¿ç”¨final_complexityï¼‰
        return result['final_complexity']
    
    def _rerank_and_select(
        self, 
        hybrid_results: List[Dict], 
        query: str, 
        combined_score: float,
        is_simple: bool,
        max_docs: int = 10
    ) -> Tuple[List[Dict], int]:
        """
        é‡æ’åºå¹¶åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„æ–‡æ¡£ï¼ˆæ”¯æŒæ¶ˆèå®éªŒï¼‰
        
        ç­–ç•¥ï¼š
        1. æ£€æµ‹æ˜¯å¦ä¸ºæ³•æ¡æŸ¥è¯¢ï¼ˆç‰¹æ®Šå¤„ç†ï¼‰
        2. æ ¹æ®é—®é¢˜ç±»å‹ï¼ˆç®€å•/å¤æ‚ï¼‰ç¡®å®šåŸºç¡€æ–‡æ¡£æ•°é‡
        3. åŸºäºæ··åˆåˆ†æ•°è¿›è¡Œé‡æ’åºï¼ˆå·²ç»å®Œæˆï¼‰
        4. ä½¿ç”¨åˆ†æ•°é˜ˆå€¼è¿‡æ»¤ä½ç›¸å…³æ–‡æ¡£
        5. åŠ¨æ€è°ƒæ•´æœ€ç»ˆæ–‡æ¡£æ•°é‡
        
        æ¶ˆèå®éªŒæ”¯æŒï¼š
        - fixed_topk: å›ºå®šè¿”å›10ä¸ªæ–‡æ¡£ï¼Œè·³è¿‡è‡ªé€‚åº”é€‰æ‹©
        
        Args:
            hybrid_results: æ··åˆæ£€ç´¢ç»“æœï¼ˆå·²æŒ‰hybrid_scoreæ’åºï¼‰
            query: æŸ¥è¯¢æ–‡æœ¬
            combined_score: ç»¼åˆç›¸ä¼¼åº¦åˆ†æ•°
            is_simple: æ˜¯å¦ä¸ºç®€å•é—®é¢˜
            max_docs: æœ€å¤§æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤10ï¼Œåˆ†ç±»ä»»åŠ¡å¯ä»¥è®¾ç½®ä¸º15ï¼‰
            
        Returns:
            (é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨, é€‰æ‹©çš„æ–‡æ¡£æ•°é‡)
        """
        if not hybrid_results:
            return [], 0
        
        # ğŸ”¬ æ¶ˆèå®éªŒ3ï¼šå›ºå®šTop-K
        if self.fixed_topk:
            self.logger.info(f"   ğŸ”¬ æ¶ˆèæ¨¡å¼ï¼šå›ºå®šTop-Kæ–‡æ¡£æ•°é‡")
            fixed_count = 10  # å›ºå®šä½¿ç”¨10ä¸ªæ–‡æ¡£
            selected_results = hybrid_results[:fixed_count]
            self.logger.info(f"   ğŸ“Š å›ºå®šé€‰æ‹©: {fixed_count} ä¸ªæ–‡æ¡£")
            selected_scores = [f"{r['hybrid_score']:.3f}" for r in selected_results]
            self.logger.info(f"      - é€‰æ‹©æ–‡æ¡£åˆ†æ•°: {selected_scores}")
            return selected_results, fixed_count
        
        # æ£€æµ‹æ˜¯å¦ä¸ºæ³•æ¡æŸ¥è¯¢ï¼ˆåŒ…å«"ç¬¬XXæ¡"çš„æ¨¡å¼ï¼‰
        import re
        is_law_article_query = bool(re.search(r'ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+\s*æ¡', query))
        
        # ç­–ç•¥1: æ ¹æ®é—®é¢˜ç±»å‹ç¡®å®šåŸºç¡€æ•°é‡
        if is_simple:
            if is_law_article_query:
                # æ³•æ¡æŸ¥è¯¢ï¼šä¿ç•™æ›´å¤šæ–‡æ¡£ï¼Œç¡®ä¿ä¸é—æ¼æ­£ç¡®æ³•æ¡
                base_count = min(10, max_docs)
                min_count = 8
                max_count = max_docs
                self.logger.info(f"   ğŸ” æ£€æµ‹åˆ°æ³•æ¡æŸ¥è¯¢ï¼Œä½¿ç”¨ä¿å®ˆç­–ç•¥")
            else:
                # æ™®é€šç®€å•é—®é¢˜ï¼šæ³•æ¡æŸ¥è¯¢ï¼Œéœ€è¦è¾ƒå¤šæ–‡æ¡£ç¡®ä¿è¦†ç›–
                base_count = min(8, max_docs)
                min_count = 5
                max_count = max_docs
        else:
            # å¤æ‚é—®é¢˜ï¼šåœºæ™¯æ¨ç†ï¼Œéœ€è¦ç²¾å‡†æ–‡æ¡£é¿å…å™ªéŸ³
            base_count = min(5, max_docs)
            min_count = 3
            max_count = min(7, max_docs)
        
        # ç­–ç•¥2: åŸºäºåˆ†æ•°åˆ†å¸ƒåŠ¨æ€è°ƒæ•´
        # è®¡ç®—åˆ†æ•°çš„ç›¸å¯¹ä¸‹é™ï¼Œæ‰¾åˆ°"æ–­å´–"ä½ç½®
        scores = [r['hybrid_score'] for r in hybrid_results]
        
        # æ‰¾åˆ°åˆ†æ•°æ˜¾è‘—ä¸‹é™çš„ä½ç½®
        score_drops = []
        for i in range(len(scores) - 1):
            if scores[i] > 0:
                drop_ratio = (scores[i] - scores[i+1]) / scores[i]
                score_drops.append((i+1, drop_ratio))
        
        # ç­–ç•¥3: ä½¿ç”¨ç›¸å¯¹é˜ˆå€¼è¿‡æ»¤
        # æ³•æ¡æŸ¥è¯¢ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ï¼ˆ50%ï¼‰ï¼Œæ™®é€šæŸ¥è¯¢ä½¿ç”¨60%
        if is_law_article_query:
            threshold_ratio = 0.5  # æ›´å®½æ¾ï¼Œé¿å…è¿‡æ»¤æ‰æ­£ç¡®æ³•æ¡
        else:
            threshold_ratio = 0.6
        
        if scores[0] > 0:
            relative_threshold = scores[0] * threshold_ratio
        else:
            relative_threshold = 0.5
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä½äºé˜ˆå€¼çš„ä½ç½®
        cutoff_by_threshold = len(scores)
        for i, score in enumerate(scores):
            if score < relative_threshold:
                cutoff_by_threshold = i
                break
        
        # æ‰¾åˆ°åˆ†æ•°æ˜¾è‘—ä¸‹é™çš„ä½ç½®ï¼ˆä¸‹é™è¶…è¿‡20%ï¼‰
        cutoff_by_drop = len(scores)
        for i, drop_ratio in score_drops:
            if drop_ratio > 0.2 and i >= min_count:
                cutoff_by_drop = i
                break
        
        # ç»¼åˆå†³ç­–ï¼šå–å¤šä¸ªç­–ç•¥çš„ä¸­é—´å€¼
        # æ³•æ¡æŸ¥è¯¢æ—¶ï¼Œä¼˜å…ˆä¿ç•™æ›´å¤šæ–‡æ¡£
        if is_law_article_query:
            # æ³•æ¡æŸ¥è¯¢ï¼šå–æœ€å¤§å€¼è€Œéä¸­ä½æ•°ï¼Œç¡®ä¿ä¸é—æ¼
            cutoff_candidates = [
                base_count,
                cutoff_by_threshold,
                cutoff_by_drop
            ]
            selected_count = max(cutoff_candidates)
        else:
            # æ™®é€šæŸ¥è¯¢ï¼šå–ä¸­ä½æ•°
            cutoff_candidates = [
                base_count,
                cutoff_by_threshold,
                cutoff_by_drop
            ]
            cutoff_candidates.sort()
            selected_count = cutoff_candidates[len(cutoff_candidates) // 2]
        
        # é™åˆ¶åœ¨[min_count, max_count]èŒƒå›´å†…
        selected_count = max(min_count, min(selected_count, max_count))
        
        # ç¡®ä¿ä¸è¶…è¿‡å®é™…æ–‡æ¡£æ•°é‡
        selected_count = min(selected_count, len(hybrid_results))
        
        # è®°å½•å†³ç­–è¿‡ç¨‹
        self.logger.info(f"   ğŸ“Š é‡æ’åºå†³ç­–:")
        self.logger.info(f"      - é—®é¢˜ç±»å‹: {'ç®€å•é—®é¢˜' if is_simple else 'å¤æ‚é—®é¢˜'}")
        if is_law_article_query:
            self.logger.info(f"      - æ³•æ¡æŸ¥è¯¢: æ˜¯ (ä½¿ç”¨ä¿å®ˆç­–ç•¥)")
        self.logger.info(f"      - åŸºç¡€æ•°é‡: {base_count}")
        self.logger.info(f"      - é˜ˆå€¼æˆªæ–­: {cutoff_by_threshold} (é˜ˆå€¼={relative_threshold:.3f}, æ¯”ä¾‹={threshold_ratio})")
        self.logger.info(f"      - åˆ†æ•°æ–­å´–: {cutoff_by_drop}")
        self.logger.info(f"      - æœ€ç»ˆé€‰æ‹©: {selected_count} ä¸ªæ–‡æ¡£")
        
        # è¿”å›é€‰æ‹©çš„æ–‡æ¡£
        selected_results = hybrid_results[:selected_count]
        
        # æ˜¾ç¤ºé€‰æ‹©çš„æ–‡æ¡£åˆ†æ•°
        selected_scores = [f"{r['hybrid_score']:.3f}" for r in selected_results]
        self.logger.info(f"      - é€‰æ‹©æ–‡æ¡£åˆ†æ•°: {selected_scores}")
        
        return selected_results, selected_count

    
    def _kg_search_flat(self, query: str, top_k: int = 10) -> str:
        """
        ä¸€ç»´å›¾è°±æ£€ç´¢ï¼ˆæ‰å¹³ç»“æ„ï¼‰
        
        åªä½¿ç”¨å®ä½“æè¿°å’Œæ–‡æœ¬å•å…ƒï¼Œä¸åŒ…å«å±‚æ¬¡ç»“æ„ï¼ˆç¤¾åŒºèšåˆã€æ¨ç†è·¯å¾„ï¼‰
        é€‚ç”¨äºè½»åº¦å¤æ‚é—®é¢˜
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: Top-Kå®ä½“
            
        Returns:
            ä¸€ç»´å›¾è°±æ£€ç´¢çš„ä¸Šä¸‹æ–‡æè¿°
        """
        # ç”Ÿæˆquery embedding
        query_embedding = self.embedding_model.encode(
            query,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        
        # å‘é‡æ£€ç´¢å®ä½“
        entity_results = search_vector_search(
            self.kg_working_dir,
            [query_embedding.tolist()],
            topk=top_k,
            level_mode=1
        )
        
        res_entity = [i[0] for i in entity_results]
        chunks = [i[-1] for i in entity_results]
        
        self.logger.info(f"   âœ… æ£€ç´¢åˆ° {len(res_entity)} ä¸ªç›¸å…³å®ä½“")
        
        # ç”Ÿæˆå®ä½“æè¿°
        entity_descriptions = self._get_entity_description(entity_results)
        
        # æå–æ–‡æœ¬å•å…ƒ
        text_units = get_text_units(self.kg_working_dir, chunks, None, k=5)
        
        # ç»„ç»‡ä¸Šä¸‹æ–‡ï¼ˆä¸€ç»´ç»“æ„ï¼‰
        kg_context = f"""
entity_information:
{entity_descriptions}

text_units:
{text_units}
"""
        
        self.logger.info("   âœ… ä¸€ç»´å›¾è°±æ£€ç´¢å®Œæˆ")
        return kg_context
    
    def kg_search(self, query: str, top_k: int = 10) -> str:
        """
        å±‚æ¬¡åŒ–çŸ¥è¯†å›¾è°±æ£€ç´¢ï¼ˆå®Œæ•´ç»“æ„ï¼‰
        
        åŒ…å«å®ä½“æè¿°ã€ç¤¾åŒºèšåˆã€æ¨ç†è·¯å¾„ï¼ˆLCAç®—æ³•ï¼‰å’Œæ–‡æœ¬å•å…ƒ
        é€‚ç”¨äºæå¤æ‚é—®é¢˜
        
        æ¶ˆèå®éªŒæ”¯æŒï¼š
        - flat_kg: åªä½¿ç”¨å®ä½“æè¿°ï¼Œè·³è¿‡å±‚æ¬¡ç»“æ„ï¼ˆç¤¾åŒºèšåˆã€æ¨ç†è·¯å¾„ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: Top-Kå®ä½“
            
        Returns:
            å±‚æ¬¡åŒ–å›¾è°±æ£€ç´¢çš„ä¸Šä¸‹æ–‡æè¿°
        """
        # ç”Ÿæˆquery embedding
        query_embedding = self.embedding_model.encode(
            query,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        
        # å‘é‡æ£€ç´¢å®ä½“
        entity_results = search_vector_search(
            self.kg_working_dir,
            [query_embedding.tolist()],
            topk=top_k,
            level_mode=1
        )
        
        res_entity = [i[0] for i in entity_results]
        chunks = [i[-1] for i in entity_results]
        
        self.logger.info(f"   âœ… æ£€ç´¢åˆ° {len(res_entity)} ä¸ªç›¸å…³å®ä½“")
        
        # ç”Ÿæˆå®ä½“æè¿°
        entity_descriptions = self._get_entity_description(entity_results)
        
        # ğŸ”¬ æ¶ˆèå®éªŒ4ï¼šæ‰å¹³KG
        if self.flat_kg:
            self.logger.info("   ğŸ”¬ æ¶ˆèæ¨¡å¼ï¼šæ‰å¹³KGç»“æ„ï¼ˆè·³è¿‡å±‚æ¬¡ç»“æ„ï¼‰")
            # åªä½¿ç”¨å®ä½“æè¿°ï¼Œè·³è¿‡å±‚æ¬¡ç»“æ„
            text_units = get_text_units(self.kg_working_dir, chunks, None, k=5)
            
            kg_context = f"""
entity_information:
{entity_descriptions}

text_units:
{text_units}
"""
            self.logger.info("   âœ… æ‰å¹³KGæ£€ç´¢å®Œæˆ")
            return kg_context
        
        # æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨å®Œæ•´å±‚æ¬¡ç»“æ„
        # æ„å»ºæ¨ç†è·¯å¾„ï¼ˆä½¿ç”¨LCAç®—æ³•ï¼‰
        self.logger.info("   æ„å»ºæ¨ç†è·¯å¾„ï¼ˆLCAç®—æ³•ï¼‰...")
        reasoning_path, reasoning_path_info = self._get_reasoning_chain(res_entity)
        
        # èšåˆç¤¾åŒºä¿¡æ¯
        self.logger.info("   èšåˆç¤¾åŒºä¿¡æ¯...")
        aggregation_descriptions, aggregation = self._get_aggregation_description(reasoning_path)
        
        # æå–æ–‡æœ¬å•å…ƒ
        text_units = get_text_units(self.kg_working_dir, chunks, None, k=5)
        
        # ç»„ç»‡ä¸Šä¸‹æ–‡ï¼ˆå±‚æ¬¡åŒ–ç»“æ„ï¼‰
        kg_context = f"""
entity_information:
{entity_descriptions}

aggregation_entity_information:
{aggregation_descriptions}

reasoning_path_information:
{reasoning_path_info}

text_units:
{text_units}
"""
        
        self.logger.info("   âœ… å±‚æ¬¡åŒ–å›¾è°±æ£€ç´¢å®Œæˆ")
        return kg_context
    
    def _get_entity_description(self, entity_results: List[tuple]) -> str:
        """ç”Ÿæˆå®ä½“æè¿°"""
        columns = ["entity_name", "parent", "description"]
        entity_descriptions = "\t\t".join(columns) + "\n"
        entity_descriptions += "\n".join([
            info[0] + "\t\t" + info[1] + "\t\t" + info[2]
            for info in entity_results
        ])
        return entity_descriptions
    
    def _get_reasoning_chain(self, entities_set: List[str]) -> Tuple[List, str]:
        """æ„å»ºæ¨ç†è·¯å¾„ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        from itertools import combinations
        
        # é™åˆ¶å®ä½“æ•°é‡ä»¥æé«˜é€Ÿåº¦
        if len(entities_set) > 5:
            entities_set = entities_set[:5]
        
        maybe_edges = list(combinations(entities_set, 2))
        reasoning_path = []
        reasoning_path_information = []
        db_name = os.path.basename(self.kg_working_dir.rstrip("/"))
        
        for node1, node2 in maybe_edges:
            # æŸ¥æ‰¾æ ‘æ ¹
            node1_tree = find_tree_root(db_name, node1)
            node2_tree = find_tree_root(db_name, node2)
            
            a_path = []
            b_path = []
            for i, j in zip(node1_tree, node2_tree):
                if i == j:
                    a_path.append(i)
                    break
                if i in b_path or j in a_path:
                    break
                if i != j:
                    a_path.append(i)
                    b_path.append(j)
            
            path = a_path + [b_path[len(b_path) - 1 - i] for i in range(len(b_path))]
            reasoning_path.append(path)
            
            # æŸ¥è¯¢å…³ç³»
            all_nodes = list(set(a_path + b_path))
            if len(all_nodes) > 5:
                all_nodes = all_nodes[:5]
            
            for maybe_edge in combinations(all_nodes, 2):
                if maybe_edge[0] != maybe_edge[1]:
                    info = search_nodes_link(maybe_edge[0], maybe_edge[1], self.kg_working_dir)
                    if info is not None:
                        reasoning_path_information.append([maybe_edge[0], maybe_edge[1], info[2]])
        
        temp_relations_information = list(set([info[2] for info in reasoning_path_information]))
        reasoning_path_information_description = "\n".join(temp_relations_information)
        
        return reasoning_path, reasoning_path_information_description
    
    def _get_aggregation_description(self, reasoning_path: List) -> Tuple[str, set]:
        """èšåˆç¤¾åŒºä¿¡æ¯"""
        aggregation_results = []
        communities = set([community for each_path in reasoning_path for community in each_path])
        
        for community in communities:
            temp = search_community(community, self.kg_working_dir)
            if temp == "":
                continue
            aggregation_results.append(temp)
        
        columns = ["entity_name", "entity_description"]
        aggregation_descriptions = "\t\t".join(columns) + "\n"
        aggregation_descriptions += "\n".join([
            info[0] + "\t\t" + str(info[1])
            for info in aggregation_results
        ])
        
        return aggregation_descriptions, communities
    
    def _clean_answer(self, answer: str, query: str) -> str:
        """
        æ¸…ç†ç­”æ¡ˆä¸­å¯èƒ½å‡ºç°çš„é—®é¢˜é‡å¤ï¼ˆä¿å®ˆç‰ˆæœ¬ï¼‰
        
        Args:
            answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            æ¸…ç†åçš„ç­”æ¡ˆ
        """
        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœç­”æ¡ˆå¤ªçŸ­ï¼Œç›´æ¥è¿”å›ï¼Œä¸åšä»»ä½•æ¸…ç†
        if not answer or len(answer.strip()) < 10:
            return answer
        
        original_answer = answer
        
        # æ£€æŸ¥æ˜¯å¦ä»¥"å›ç­”ï¼š"ã€"ç­”æ¡ˆï¼š"ç­‰å¼€å¤´ï¼ˆåªæ¸…ç†è¿™äº›æ˜ç¡®çš„å‰ç¼€ï¼‰
        prefixes_to_remove = ['å›ç­”ï¼š', 'ç­”æ¡ˆï¼š', 'å›ç­”:', 'ç­”æ¡ˆ:']
        for prefix in prefixes_to_remove:
            if answer.startswith(prefix):
                self.logger.info(f"   ğŸ§¹ ç§»é™¤ç­”æ¡ˆå‰ç¼€: {prefix}")
                cleaned = answer[len(prefix):].strip()
                # å¦‚æœæ¸…ç†åç­”æ¡ˆå¤ªçŸ­ï¼Œè¿”å›åŸç­”æ¡ˆ
                if len(cleaned) >= 10:
                    return cleaned
                else:
                    self.logger.warning(f"   âš ï¸ æ¸…ç†åç­”æ¡ˆè¿‡çŸ­ï¼Œä¿ç•™åŸç­”æ¡ˆ")
                    return original_answer
        
        # ğŸ”§ ä¸å†ç§»é™¤é—®é¢˜é‡å¤ï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šè¯¯åˆ æœ‰æ•ˆå†…å®¹
        # ç›´æ¥è¿”å›åŸç­”æ¡ˆ
        return answer
    
    def _extract_law_article_info(self, query: str) -> Dict:
        """
        ä»æŸ¥è¯¢ä¸­æå–æ³•æ¡ä¿¡æ¯
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            åŒ…å«æ³•å¾‹åç§°å’Œæ¡æ¬¾ç¼–å·çš„å­—å…¸
        """
        import re
        
        # æå–æ³•å¾‹åç§°ï¼ˆã€Šxxxã€‹ï¼‰
        law_name_match = re.search(r'ã€Š([^ã€‹]+)ã€‹', query)
        law_name = law_name_match.group(1) if law_name_match else None
        
        # æå–æ¡æ¬¾ç¼–å·ï¼ˆç¬¬XXæ¡ï¼‰
        article_match = re.search(r'ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+)\s*æ¡', query)
        article_num = article_match.group(1) if article_match else None
        
        return {
            'law_name': law_name,
            'article_num': article_num,
            'has_law_info': law_name is not None and article_num is not None
        }
    
    def _find_exact_law_article(self, query: str, all_results: List[Dict]) -> Optional[Dict]:
        """
        åœ¨æ£€ç´¢ç»“æœä¸­ç²¾ç¡®åŒ¹é…æ³•æ¡
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            all_results: æ‰€æœ‰æ£€ç´¢ç»“æœï¼ˆåŒ…æ‹¬semanticå’Œbm25ï¼‰
            
        Returns:
            åŒ¹é…çš„æ³•æ¡æ–‡æ¡£ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å›None
        """
        import re
        
        # æå–æŸ¥è¯¢ä¸­çš„æ³•æ¡ä¿¡æ¯
        law_info = self._extract_law_article_info(query)
        
        if not law_info['has_law_info']:
            return None
        
        law_name = law_info['law_name']
        article_num = law_info['article_num']
        
        self.logger.info(f"   ğŸ” ç²¾ç¡®åŒ¹é…æ³•æ¡: ã€Š{law_name}ã€‹ç¬¬{article_num}æ¡")
        
        # æ„å»ºç²¾ç¡®åŒ¹é…çš„source_nameæ¨¡å¼
        target_source_name = f"ã€Š{law_name}ã€‹ç¬¬{article_num}æ¡"
        
        # åœ¨æ‰€æœ‰ç»“æœä¸­æŸ¥æ‰¾ç²¾ç¡®åŒ¹é…
        # ä¼˜å…ˆçº§1: ç²¾ç¡®åŒ¹é…source_nameï¼ˆæœ€å¯é ï¼‰
        for result in all_results:
            source_name = result.get('source_name', '')
            if source_name == target_source_name:
                self.logger.info(f"   âœ… æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼ˆsource_nameï¼‰: {source_name}")
                return result
        
        # ä¼˜å…ˆçº§2: æ£€æŸ¥source_nameæ˜¯å¦åŒ…å«æ³•å¾‹åç§°å’Œæ¡æ¬¾å·
        article_pattern = rf'ç¬¬\s*{re.escape(article_num)}\s*æ¡'
        for result in all_results:
            source_name = result.get('source_name', '')
            if law_name in source_name and re.search(article_pattern, source_name):
                self.logger.info(f"   âœ… æ‰¾åˆ°åŒ¹é…ï¼ˆsource_nameæ¨¡ç³Šï¼‰: {source_name}")
                return result
        
        # ä¼˜å…ˆçº§3: æ£€æŸ¥textå­—æ®µï¼ˆä½œä¸ºåå¤‡ï¼‰
        for result in all_results:
            text = result.get('text', '')
            source_name = result.get('source_name', '')
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ³•å¾‹åç§°å’Œæ¡æ¬¾ç¼–å·
            if law_name in text and re.search(article_pattern, text):
                self.logger.info(f"   âœ… æ‰¾åˆ°åŒ¹é…ï¼ˆtextå­—æ®µï¼‰: {source_name[:50]}...")
                return result
        
        self.logger.warning(f"   âš ï¸ æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„æ³•æ¡")
        return None
    
    def generate_answer(self, query: str, context: str, instruction: str = "", use_kg: bool = False, 
                       semantic_results: List[Dict] = None, bm25_results: List[Dict] = None) -> str:
        """
        ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            instruction: æŒ‡ä»¤ï¼ˆæ¥è‡ªæ•°æ®é›†ï¼‰
            use_kg: æ˜¯å¦ä½¿ç”¨äº†çŸ¥è¯†å›¾è°±
            semantic_results: è¯­ä¹‰æ£€ç´¢ç»“æœï¼ˆç”¨äºæ³•æ¡ç²¾ç¡®åŒ¹é…ï¼‰
            bm25_results: BM25æ£€ç´¢ç»“æœï¼ˆç”¨äºæ³•æ¡ç²¾ç¡®åŒ¹é…ï¼‰
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        self.logger.info("æ­¥éª¤5: LLMç”Ÿæˆç­”æ¡ˆ...")
        
        if self.llm is None:
            self.logger.warning("LLMæœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºç­”æ¡ˆ")
            return ""
        
        # æ£€æµ‹æ˜¯å¦ä¸ºæ³•æ¡æŸ¥è¯¢
        is_law_article_query = bool(re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+æ¡', query))
        is_simple_article_query = "åªéœ€ç›´æ¥ç»™å‡ºæ³•æ¡å†…å®¹" in instruction or "åªéœ€è¦ç»™å‡ºå…·ä½“æ³•æ¡å†…å®¹" in instruction
        
        # ğŸ”§ æ–°å¢ï¼šæ³•æ¡ç²¾ç¡®åŒ¹é… - ç›´æ¥è¿”å›æ³•æ¡åŸæ–‡
        if (is_law_article_query or is_simple_article_query) and semantic_results and bm25_results:
            # åˆå¹¶æ‰€æœ‰æ£€ç´¢ç»“æœè¿›è¡Œç²¾ç¡®åŒ¹é…
            all_results = semantic_results + bm25_results
            exact_match = self._find_exact_law_article(query, all_results)
            
            if exact_match:
                # æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œç›´æ¥è¿”å›æ³•æ¡åŸæ–‡ï¼Œä¸ç»è¿‡LLM
                self.logger.info(f"   âœ… æ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„æ³•æ¡ï¼Œç›´æ¥è¿”å›åŸæ–‡")
                law_text = exact_match['text'].strip()
                
                # æ£€æŸ¥law_textæ˜¯å¦å·²ç»åŒ…å«æ ¼å¼åŒ–çš„æ³•å¾‹åç§°å’Œæ¡æ¬¾å·
                # å¦‚æœå·²ç»åŒ…å«ï¼ˆå¦‚ï¼šã€Šæ³•å¾‹åç§°ã€‹ç¬¬XXæ¡: å†…å®¹ï¼‰ï¼Œåˆ™ç›´æ¥è¿”å›
                if re.match(r'ã€Š.+ã€‹ç¬¬.+æ¡:', law_text):
                    formatted_answer = law_text
                    self.logger.info(f"   ğŸ“„ æ³•æ¡å·²æ ¼å¼åŒ–ï¼Œç›´æ¥è¿”å›ï¼ˆé•¿åº¦: {len(formatted_answer)}å­—ç¬¦ï¼‰")
                else:
                    # å¦‚æœæ²¡æœ‰æ ¼å¼åŒ–ï¼Œåˆ™è¿›è¡Œæ ¼å¼åŒ–
                    law_info = self._extract_law_article_info(query)
                    if law_info['has_law_info']:
                        formatted_answer = f"ã€Š{law_info['law_name']}ã€‹ç¬¬{law_info['article_num']}æ¡: {law_text}"
                    else:
                        formatted_answer = law_text
                    self.logger.info(f"   ğŸ“„ æ ¼å¼åŒ–åè¿”å›æ³•æ¡ï¼ˆé•¿åº¦: {len(formatted_answer)}å­—ç¬¦ï¼‰")
                
                return formatted_answer
        
        if is_law_article_query or is_simple_article_query:
            # ğŸ”§ æ³•æ¡æŸ¥è¯¢ï¼šä½¿ç”¨æç®€æç¤ºè¯ï¼Œè¦æ±‚ç›´æ¥è¿”å›æ³•æ¡åŸæ–‡
            system_message = "ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·é—®é¢˜ï¼Œåªè¿”å›æŒ‡å®šçš„æ³•æ¡å†…å®¹ï¼Œä¸è¦è¿”å›å…¶ä»–æ³•æ¡ã€‚"
            
            # ğŸ”§ ç®€åŒ–ä¸Šä¸‹æ–‡ï¼šåªä¿ç•™æ–‡æœ¬å†…å®¹ï¼Œå»æ‰ã€æ–‡æ¡£Xã€‘æ ‡è®°
            # æ£€æŸ¥contextæ˜¯å¦åŒ…å«ã€æ–‡æ¡£ã€‘æ ‡è®°
            if "ã€æ–‡æ¡£" in context:
                # ç§»é™¤ã€æ–‡æ¡£Xã€‘æ ‡è®°ï¼Œåªä¿ç•™æ³•æ¡å†…å®¹
                clean_context = re.sub(r'ã€æ–‡æ¡£\d+ã€‘[^\n]*\n', '', context)
            else:
                clean_context = context
            
            # æå–æ³•æ¡ä¿¡æ¯
            law_info = self._extract_law_article_info(query)
            
            # å¦‚æœèƒ½æå–åˆ°æ³•æ¡ä¿¡æ¯ï¼Œåœ¨æç¤ºè¯ä¸­æ˜ç¡®æŒ‡å®š
            if law_info['has_law_info']:
                target_article = f"ã€Š{law_info['law_name']}ã€‹ç¬¬{law_info['article_num']}æ¡"
                user_message = f"""ä»¥ä¸‹æ˜¯æ³•å¾‹æ–‡æœ¬ï¼š

{clean_context}

é—®é¢˜ï¼š{query}

é‡è¦æç¤ºï¼šè¯·åªè¿”å› {target_article} çš„å†…å®¹ï¼Œä¸è¦è¿”å›å…¶ä»–æ³•æ¡ã€‚
å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰è¯¥æ³•æ¡ï¼Œè¯·å›ç­”"æœªæ‰¾åˆ°è¯¥æ³•æ¡"ã€‚

æ ¼å¼è¦æ±‚ï¼š
ã€Šæ³•å¾‹åç§°ã€‹ç¬¬XXæ¡: [æ³•æ¡åŸæ–‡]"""
            else:
                user_message = f"""ä»¥ä¸‹æ˜¯æ³•å¾‹æ–‡æœ¬ï¼š

{clean_context}

é—®é¢˜ï¼š{query}

è¯·ç›´æ¥ç»™å‡ºè¯¥æ³•æ¡çš„å®Œæ•´å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€åˆ†ææˆ–é¢å¤–è¯´æ˜ã€‚æ ¼å¼å¦‚ä¸‹ï¼š
ã€Šæ³•å¾‹åç§°ã€‹ç¬¬XXæ¡: [æ³•æ¡åŸæ–‡]"""
            
            # Qwen2å¯¹è¯æ ¼å¼
            composed = f"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant\n"
            
        else:
            # ä½¿ç”¨å®Œæ•´çš„æç¤ºè¯æ¨¡æ¿
            system_prompt = PROMPTS.get("rag_response_zh", PROMPTS.get("local_rag_response", "")).format(
                context_data=context
            )
            
            # å¦‚æœæœ‰instructionï¼Œå°†å…¶åŠ å…¥åˆ°queryä¸­
            if instruction:
                full_query = f"{instruction}\n{query}"
            else:
                full_query = query
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨Qwen2çš„å¯¹è¯æ ¼å¼
            if system_prompt:
                # ç®€åŒ–system_promptï¼Œå»æ‰å¤æ‚çš„markdownæ ¼å¼
                simple_system = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ³•å¾‹çŸ¥è¯†å’Œä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
                user_message = f"""ä»¥ä¸‹æ˜¯ç›¸å…³çš„æ³•å¾‹çŸ¥è¯†å’Œä¸Šä¸‹æ–‡ï¼š

{context}

ç”¨æˆ·é—®é¢˜ï¼š{full_query}

è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯å›ç­”é—®é¢˜ã€‚"""
                
                composed = f"<|im_start|>system\n{simple_system}<|im_end|>\n<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant\n"
            else:
                system_message = "ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹åŠ©æ‰‹ã€‚"
                user_message = f"""ä»¥ä¸‹æ˜¯ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼š

{context}

ç”¨æˆ·é—®é¢˜ï¼š{full_query}

è¯·å›ç­”ï¼š"""
                
                composed = f"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant\n"
        
        # ğŸ”§ è°ƒè¯•ï¼šè®°å½•æç¤ºè¯é•¿åº¦
        self.logger.info(f"   æç¤ºè¯é•¿åº¦: {len(composed)} å­—ç¬¦")
        self.logger.info(f"   ç®€åŒ–æ¨¡å¼: {is_simple_article_query}")
        
        # ç”Ÿæˆç­”æ¡ˆ
        try:
            outputs = self.llm.generate([composed], self.sampling_params)
            if outputs and outputs[0].outputs:
                raw_answer = outputs[0].outputs[0].text
                answer = raw_answer.strip()
                
                # ğŸ”§ è°ƒè¯•ï¼šè®°å½•åŸå§‹è¾“å‡ºï¼ˆå‰200å­—ç¬¦ï¼‰
                self.logger.info(f"   åŸå§‹è¾“å‡º: {raw_answer[:200]}")
                self.logger.info(f"   âœ… ç”Ÿæˆå®Œæˆï¼ˆé•¿åº¦: {len(answer)}å­—ç¬¦ï¼‰")
                
                # ğŸ”§ ä¿®å¤ï¼šå¦‚æœç­”æ¡ˆä¸ºç©ºï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯ä½†ä¸è¦ç«‹å³è¿”å›é”™è¯¯
                if not answer:
                    self.logger.warning(f"   âš ï¸ ç”Ÿæˆçš„ç­”æ¡ˆä¸ºç©ºï¼")
                    self.logger.warning(f"   åŸå§‹è¾“å‡ºå®Œæ•´å†…å®¹: {repr(raw_answer)}")
                    self.logger.warning(f"   æç¤ºè¯å‰500å­—ç¬¦: {composed[:500]}")
                    # è¿”å›æ›´æœ‰ç”¨çš„é”™è¯¯ä¿¡æ¯
                    return "æŠ±æ­‰ï¼Œæ¨¡å‹æœªèƒ½ç”Ÿæˆæœ‰æ•ˆå›ç­”ã€‚è¯·æ£€æŸ¥æ—¥å¿—äº†è§£è¯¦æƒ…ã€‚"
                
                # åå¤„ç†ï¼šç§»é™¤ç­”æ¡ˆå¼€å¤´å¯èƒ½å‡ºç°çš„é—®é¢˜é‡å¤
                cleaned_answer = self._clean_answer(answer, query)
                
                # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæ¸…ç†åç­”æ¡ˆå˜ç©ºäº†ï¼Œè¿”å›åŸå§‹ç­”æ¡ˆ
                if not cleaned_answer or len(cleaned_answer.strip()) < 5:
                    self.logger.warning(f"   âš ï¸ æ¸…ç†åç­”æ¡ˆè¿‡çŸ­ï¼Œä½¿ç”¨åŸå§‹ç­”æ¡ˆ")
                    self.logger.warning(f"   åŸå§‹ç­”æ¡ˆ: {answer[:100]}")
                    self.logger.warning(f"   æ¸…ç†åç­”æ¡ˆ: {cleaned_answer}")
                    return answer
                
                return cleaned_answer
            else:
                self.logger.warning("LLMè¿”å›ç©ºè¾“å‡º")
                return "æŠ±æ­‰ï¼Œæ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚"
        except Exception as e:
            self.logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def retrieve_and_answer(
        self,
        query: str,
        instruction: str = "",
        top_k: int = 10,
        correlation_threshold: float = 0.35,
        alpha: float = 0.7
    ) -> Dict:
        """
        å®Œæ•´çš„æ£€ç´¢å’Œå›ç­”æµç¨‹ï¼ˆç‹¬ç«‹BM25æ£€ç´¢ç‰ˆæœ¬ï¼‰
        
        æµç¨‹ï¼š
        1. æŸ¥è¯¢é‡å†™ï¼ˆæé«˜æ£€ç´¢æ•ˆæœï¼‰
        2. è¯­ä¹‰æ£€ç´¢ â†’ Top-Kæ–‡æ¡£A
        3. ç‹¬ç«‹BM25æ£€ç´¢ â†’ Top-Kæ–‡æ¡£B
        4. æ¯”è¾ƒä¸¤ç§æ£€ç´¢ç»“æœçš„ç›¸ä¼¼åº¦
        5. å¦‚æœç›¸ä¼¼åº¦é«˜ï¼ˆ>=é˜ˆå€¼ï¼‰ï¼šä½¿ç”¨æ··åˆæ£€ç´¢ç»“æœ
        6. å¦‚æœç›¸ä¼¼åº¦ä½ï¼ˆ<é˜ˆå€¼ï¼‰ï¼šè°ƒç”¨çŸ¥è¯†å›¾è°±
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            instruction: æŒ‡ä»¤ï¼ˆæ¥è‡ªæ•°æ®é›†ï¼‰
            top_k: Top-K
            correlation_threshold: å¤æ‚åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.35ï¼Œä¼˜åŒ–åï¼‰
            alpha: æ··åˆæƒé‡ï¼ˆ0-1ï¼‰ï¼Œæ¨è0.7
            
        Returns:
            ç»“æœå­—å…¸ï¼ŒåŒ…å«answer, similarity, used_kgç­‰ä¿¡æ¯
        """
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ” æŸ¥è¯¢: {query[:50]}...")
        if instruction:
            self.logger.info(f"ğŸ“‹ æŒ‡ä»¤: {instruction[:50]}...")
        self.logger.info(f"{'='*60}")
        
        start_time = time.time()
        
        # ğŸ†• æ£€æµ‹æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡
        is_classification = self._is_classification_task(instruction)
        
        # ğŸ†• åˆ†ç±»ä»»åŠ¡ä½¿ç”¨ä¸åŒçš„å‚æ•°
        if is_classification:
            self.logger.info(f"   ğŸ·ï¸ æ£€æµ‹åˆ°åˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨ä¼˜åŒ–å‚æ•°")
            original_top_k = top_k
            original_alpha = alpha
            
            # è°ƒæ•´å‚æ•°ï¼šé€‚åº¦å¢åŠ ï¼Œé¿å…è¿‡å¤šå™ªéŸ³
            top_k = 15  # ä»10å¢åŠ åˆ°15ï¼ˆè€Œä¸æ˜¯20ï¼‰
            # æ›´ä¾èµ–è¯­ä¹‰æ£€ç´¢ï¼ˆåˆ†ç±»ä»»åŠ¡æ›´éœ€è¦è¯­ä¹‰ç†è§£ï¼‰
            alpha = 0.75  # ä»0.7å¢åŠ åˆ°0.75ï¼ˆè€Œä¸æ˜¯0.8ï¼‰
            # æé«˜é˜ˆå€¼ï¼ˆåˆ†ç±»ä»»åŠ¡å¯¹å¤æ‚åº¦è¦æ±‚å¯ä»¥é€‚å½“æé«˜ï¼‰
            adjusted_threshold = 0.45  # ä½¿ç”¨0.45ï¼ˆåè½¬åï¼‰
            
            self.logger.info(f"   - Top-K: {original_top_k} â†’ {top_k}")
            self.logger.info(f"   - Alpha: {original_alpha} â†’ {alpha}")
            self.logger.info(f"   - é˜ˆå€¼: 0.7 â†’ {adjusted_threshold}")
            
            # ğŸ†• æ·»åŠ Few-shotç¤ºä¾‹
            instruction = self._add_classification_examples(instruction, query)
            self.logger.info(f"   âœ… å·²æ·»åŠ åˆ†ç±»ç¤ºä¾‹åˆ°æŒ‡ä»¤")
            
            # ğŸ†• æ£€æµ‹åŒ»ç–—çº çº·ï¼ˆç‰¹æ®Šå¤„ç†ï¼‰
            is_medical, medical_confidence = self._detect_medical_dispute(query)
            if is_medical and medical_confidence > 0.6:
                self.logger.info(f"   âš•ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„åŒ»ç–—çº çº·ï¼ˆç½®ä¿¡åº¦: {medical_confidence:.2f}ï¼‰")
                instruction += "\n\nã€ç‰¹åˆ«æç¤ºã€‘è¯¥é—®é¢˜å¯èƒ½æ¶‰åŠåŒ»ç–—çº çº·ï¼Œè¯·ä»”ç»†åˆ¤æ–­æ˜¯å¦ä¸ºåŒ»ç–—æœºæ„çš„åŒ»ç–—è¡Œä¸ºå¯¼è‡´çš„æŸå®³ã€‚å¦‚æœæ˜¯åŒ»ç”Ÿæ‰‹æœ¯å¤±è¯¯ã€è¯¯è¯Šç­‰åŒ»ç–—è¡Œä¸ºå¯¼è‡´çš„æŸå®³ï¼Œåº”åˆ¤æ–­ä¸ºåŒ»ç–—çº çº·ï¼›å¦‚æœåªæ˜¯åœ¨åŒ»é™¢æ²»ç–—å·¥ä¼¤ï¼Œåº”åˆ¤æ–­ä¸ºäººèº«æŸå®³æˆ–åŠ³åŠ¨çº çº·ã€‚"
        else:
            adjusted_threshold = 0.70  # éåˆ†ç±»ä»»åŠ¡ä½¿ç”¨åŸé˜ˆå€¼
        
        # æ­¥éª¤0: æŸ¥è¯¢é‡å†™
        rewritten_query = self.rewrite_query_for_consistency(query, instruction)
        
        # æ­¥éª¤1: è¯­ä¹‰å‘é‡æ£€ç´¢
        semantic_results = self.semantic_search(query, top_k=top_k, rewritten_query=rewritten_query)
        
        # æ­¥éª¤2: ç‹¬ç«‹BM25æ£€ç´¢
        bm25_results = self.bm25_search(query, top_k=top_k, rewritten_query=rewritten_query)
        
        # æ­¥éª¤3: å¤šç»´åº¦æ¯”è¾ƒä¸¤ç§ç‹¬ç«‹æ£€ç´¢ç»“æœ
        metrics = self.compare_independent_rankings(
            semantic_results, bm25_results, query
        )
        
        # ğŸ†• åˆ†ç±»ä»»åŠ¡ä½¿ç”¨è°ƒæ•´åçš„é˜ˆå€¼
        if not is_classification:
            # éåˆ†ç±»ä»»åŠ¡ï¼šä½¿ç”¨ç»Ÿä¸€é˜ˆå€¼0.35ï¼ˆä¼˜åŒ–åï¼‰
            adjusted_threshold = 0.35
        
        self.logger.info(f"   ä½¿ç”¨{'åˆ†ç±»ä»»åŠ¡' if is_classification else 'ç»Ÿä¸€'}é˜ˆå€¼: {adjusted_threshold}")
        
        # æ­¥éª¤4: è‡ªé€‚åº”ç­–ç•¥è·¯ç”±ä¸å†³ç­–ï¼ˆä¸‰é‡é˜ˆå€¼ï¼‰
        combined_score = metrics['combined_score']
        
        # å®šä¹‰ä¸‰é‡é˜ˆå€¼
        theta_low = 0.25      # ç®€å•äº‹å®é—®é¢˜é˜ˆå€¼
        theta_medium = 0.45   # è¯­ä¹‰æ¨¡ç³Šé—®é¢˜é˜ˆå€¼
        theta_high = 0.65     # è½»åº¦å¤æ‚é—®é¢˜é˜ˆå€¼
        
        self.logger.info(f"\næ­¥éª¤4: è‡ªé€‚åº”ç­–ç•¥è·¯ç”±ä¸å†³ç­–")
        self.logger.info(f"   æœ€ç»ˆå¤æ‚åº¦: {combined_score:.3f}")
        self.logger.info(f"   ä¸‰é‡é˜ˆå€¼: Î¸_low={theta_low}, Î¸_medium={theta_medium}, Î¸_high={theta_high}")
        
        # ==================== ç­–ç•¥1: ç®€å•äº‹å®é—®é¢˜ ====================
        if combined_score <= theta_low:
            self.logger.info(f"âœ“ C_final ({combined_score:.3f}) â‰¤ Î¸_low ({theta_low})")
            self.logger.info(f"   â†’ ç­–ç•¥1: ç®€å•äº‹å®é—®é¢˜ï¼Œå¯ç”¨å¯†é›†å‘é‡æ£€ç´¢")
            
            # åªä½¿ç”¨è¯­ä¹‰æ£€ç´¢ç»“æœï¼Œä¸éœ€è¦æ··åˆæ£€ç´¢
            final_context = "\n\n".join([
                f"ã€æ–‡æ¡£{i+1}ã€‘{r['source_name']}\n{r['text']}"
                for i, r in enumerate(semantic_results[:8])  # ä½¿ç”¨Top-8è¯­ä¹‰æ£€ç´¢ç»“æœ
            ])
            use_kg = False
            strategy = "dense_vector"
            
        # ==================== ç­–ç•¥2: è¯­ä¹‰æ¨¡ç³Šé—®é¢˜ ====================
        elif theta_low < combined_score <= theta_medium:
            self.logger.info(f"âœ“ Î¸_low ({theta_low}) < C_final ({combined_score:.3f}) â‰¤ Î¸_medium ({theta_medium})")
            self.logger.info(f"   â†’ ç­–ç•¥2: è¯­ä¹‰æ¨¡ç³Šé—®é¢˜ï¼Œå¯ç”¨æ··åˆæ£€ç´¢+äºŒæ¬¡æ’åº")
            
            # åˆ›å»ºæ··åˆæ£€ç´¢ç»“æœ
            self.logger.info(f"   4.1: åˆ›å»ºæ··åˆæ£€ç´¢ç»“æœ (alpha={alpha})...")
            hybrid_results = self.create_hybrid_results(semantic_results, bm25_results, alpha=alpha)
            
            # äºŒæ¬¡æ’åºï¼šå»é™¤å°¾éƒ¨å™ªå£°
            self.logger.info(f"   4.2: äºŒæ¬¡æ’åºï¼Œå»é™¤å°¾éƒ¨å™ªå£°...")
            reranked_results, selected_count = self._rerank_and_select(
                hybrid_results, 
                query, 
                combined_score,
                is_simple=False,  # è¯­ä¹‰æ¨¡ç³Šé—®é¢˜éœ€è¦ç²¾å‡†å®šä½
                max_docs=10
            )
            self.logger.info(f"   âœ… é€‰æ‹© {selected_count} ä¸ªæœ€ç›¸å…³æ–‡æ¡£")
            
            final_context = "\n\n".join([
                f"ã€æ–‡æ¡£{i+1}ã€‘{r['source_name']}\n{r['text']}"
                for i, r in enumerate(reranked_results)
            ])
            use_kg = False
            strategy = "hybrid_rerank"
            
        # ==================== ç­–ç•¥3: è½»åº¦å¤æ‚é—®é¢˜ ====================
        elif theta_medium < combined_score <= theta_high:
            self.logger.info(f"âœ“ Î¸_medium ({theta_medium}) < C_final ({combined_score:.3f}) â‰¤ Î¸_high ({theta_high})")
            self.logger.info(f"   â†’ ç­–ç•¥3: è½»åº¦å¤æ‚é—®é¢˜ï¼Œä½¿ç”¨ä¸€ç»´å›¾è°±æ£€ç´¢")
            
            # åˆ›å»ºæ··åˆæ£€ç´¢ç»“æœ
            hybrid_results = self.create_hybrid_results(semantic_results, bm25_results, alpha=alpha)
            reranked_results, selected_count = self._rerank_and_select(
                hybrid_results, query, combined_score, is_simple=False, max_docs=8
            )
            
            # ä¸€ç»´å›¾è°±æ£€ç´¢ï¼ˆæ‰å¹³KGï¼Œåªä½¿ç”¨å®ä½“æè¿°ï¼‰
            self.logger.info(f"   4.1: ä¸€ç»´å›¾è°±æ£€ç´¢ï¼ˆæ‰å¹³ç»“æ„ï¼‰...")
            kg_context = self._kg_search_flat(query, top_k=top_k)
            
            # èåˆå‘é‡æ£€ç´¢å’Œå›¾è°±æ£€ç´¢
            vector_context = "\n\n".join([
                f"ã€æ–‡æ¡£{i+1}ã€‘{r['source_name']}\n{r['text']}"
                for i, r in enumerate(reranked_results)
            ])
            
            final_context = f"""
## å‘é‡æ£€ç´¢ç»“æœ

{vector_context}

## çŸ¥è¯†å›¾è°±æ£€ç´¢ç»“æœï¼ˆä¸€ç»´ï¼‰

{kg_context}
"""
            use_kg = True
            strategy = "flat_kg"
            
        # ==================== ç­–ç•¥4: æå¤æ‚é—®é¢˜ ====================
        else:  # combined_score > theta_high
            self.logger.info(f"âœ“ C_final ({combined_score:.3f}) â‰¥ Î¸_high ({theta_high})")
            self.logger.info(f"   â†’ ç­–ç•¥4: æå¤æ‚é—®é¢˜ï¼Œå¯ç”¨å±‚æ¬¡åŒ–å›¾è°±æ£€ç´¢+LCAæ¨ç†è·¯å¾„")
            
            # åˆ›å»ºæ··åˆæ£€ç´¢ç»“æœ
            hybrid_results = self.create_hybrid_results(semantic_results, bm25_results, alpha=alpha)
            reranked_results, selected_count = self._rerank_and_select(
                hybrid_results, query, combined_score, is_simple=False, max_docs=6
            )
            
            # å±‚æ¬¡åŒ–å›¾è°±æ£€ç´¢ï¼ˆå®Œæ•´ç»“æ„ï¼šå®ä½“+ç¤¾åŒºèšåˆ+æ¨ç†è·¯å¾„ï¼‰
            self.logger.info(f"   4.1: å±‚æ¬¡åŒ–å›¾è°±æ£€ç´¢ï¼ˆå®Œæ•´ç»“æ„+LCAæ¨ç†ï¼‰...")
            kg_context = self.kg_search(query, top_k=top_k)
            
            # èåˆå‘é‡æ£€ç´¢å’Œå›¾è°±æ£€ç´¢
            vector_context = "\n\n".join([
                f"ã€æ–‡æ¡£{i+1}ã€‘{r['source_name']}\n{r['text']}"
                for i, r in enumerate(reranked_results)
            ])
            
            final_context = f"""
## å‘é‡æ£€ç´¢ç»“æœ

{vector_context}

## çŸ¥è¯†å›¾è°±æ£€ç´¢ç»“æœï¼ˆå±‚æ¬¡åŒ–ï¼‰

{kg_context}
"""
            use_kg = True
            strategy = "hierarchical_kg"
        
        # æ­¥éª¤5: ç”Ÿæˆç­”æ¡ˆï¼ˆä¼ å…¥æ£€ç´¢ç»“æœç”¨äºæ³•æ¡ç²¾ç¡®åŒ¹é…ï¼‰
        answer = self.generate_answer(
            query, 
            final_context, 
            instruction, 
            use_kg,
            semantic_results=semantic_results,
            bm25_results=bm25_results
        )
        
        elapsed_time = time.time() - start_time
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"âœ… å®Œæˆï¼è€—æ—¶: {elapsed_time:.2f}ç§’")
        self.logger.info(f"   ä½¿ç”¨ç­–ç•¥: {strategy}")
        self.logger.info(f"{'='*60}\n")
        
        return {
            'query': query,
            'rewritten_query': rewritten_query,
            'instruction': instruction,
            'answer': answer,
            # æ ¸å¿ƒæŒ‡æ ‡
            'bm25_top1_score': metrics['bm25_top1_score'],
            'overlap_ratio': metrics['overlap_ratio'],
            'top3_overlap': metrics['top3_overlap'],
            'combined_score': metrics['combined_score'],
            # æ–°å¢ï¼šç»Ÿä¸€å¤æ‚åº¦è¯„åˆ†ç›¸å…³æŒ‡æ ‡
            'question_type': metrics.get('question_type', 'unknown'),
            'question_nature_complexity': metrics.get('question_nature_complexity', 0.0),
            'retrieval_inconsistency': metrics.get('retrieval_inconsistency', 0.0),
            'final_complexity': metrics.get('final_complexity', 0.0),
            'evaluation_layer': metrics.get('evaluation_layer', 0),
            'needs_kg': metrics.get('needs_kg', False),
            'used_kg': use_kg,
            'strategy': strategy,  # æ–°å¢ï¼šä½¿ç”¨çš„ç­–ç•¥
            'elapsed_time': elapsed_time,
            'semantic_results': semantic_results,
            'bm25_results': bm25_results,
            'hybrid_results': hybrid_results if strategy in ['hybrid_rerank', 'flat_kg', 'hierarchical_kg'] else [],
            'alpha': alpha,
        }


def setup_logging(log_dir: str = "logs") -> Tuple[logging.Logger, str]:
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"hybrid_rag_{timestamp}.log")
    
    log_format = '%(asctime)s - %(levelname)s - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    
    # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
    root_logger = logging.getLogger()
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        datefmt=date_format,
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info("="*60)
    
    return logger, log_file


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ··åˆRAGæ£€ç´¢ç³»ç»Ÿ")
    
    # æ•°æ®è·¯å¾„
    parser.add_argument(
        "--input",
        type=str,
        default="E:/MyPrograms/LeanRAG/datasets/query_social.json",
        help="è¾“å…¥æŸ¥è¯¢æ•°æ®é›†è·¯å¾„"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¾“å‡ºç»“æœè·¯å¾„ï¼ˆé»˜è®¤ä¸ºè¾“å…¥æ–‡ä»¶å_hybrid_pred.jsonï¼‰"
    )
    
    # å‘é‡æ•°æ®åº“é…ç½®
    parser.add_argument(
        "--vector-db",
        type=str,
        default=VECTOR_DB_PATH,
        help="Milvuså‘é‡æ•°æ®åº“è·¯å¾„"
    )
    parser.add_argument(
        "--collection",
        type=str,
        default=COLLECTION_NAME,
        help="Collectionåç§°"
    )
    
    # çŸ¥è¯†å›¾è°±é…ç½®
    parser.add_argument(
        "--kg-dir",
        type=str,
        default=KG_WORKING_DIR,
        help="çŸ¥è¯†å›¾è°±å·¥ä½œç›®å½•"
    )
    
    # æ¨¡å‹é…ç½®
    parser.add_argument(
        "--embedding-model",
        type=str,
        default=EMBEDDING_MODEL,
        help="Embeddingæ¨¡å‹åç§°"
    )
    parser.add_argument(
        "--llm-model",
        type=str,
        default="/newdatad/WHH/MyEmoHH/models/Qwen2-7B-Instruct",
        help="LLMæ¨¡å‹è·¯å¾„"
    )
    
    # æ£€ç´¢å‚æ•°
    parser.add_argument(
        "--top-k",
        type=int,
        default=TOP_K,
        help="æ£€ç´¢Top-K"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=CORRELATION_THRESHOLD,
        help="å¤æ‚åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.35ï¼‰"
    )
    parser.add_argument(
        "--alpha",
        type=float,
        default=0.7,
        help="æ··åˆæƒé‡alphaï¼ˆ0-1ï¼‰ï¼Œæ¨è0.7è¡¨ç¤º70%%è¯­ä¹‰+30%%BM25"
    )
    
    # LLMå‚æ•°
    parser.add_argument("--tp", type=int, default=1, help="tensor_parallel_sizeï¼ˆä½¿ç”¨çš„GPUæ•°é‡ï¼Œé»˜è®¤1å¼ å¡ï¼‰")
    parser.add_argument("--gpu-mem-util", type=float, default=0.75, help="GPUæ˜¾å­˜å ç”¨æ¯”ä¾‹ï¼ˆå•å¡ä½¿ç”¨0.75ï¼‰")
    parser.add_argument("--max-model-len", type=int, default=4096, help="æœ€å¤§æ¨¡å‹åºåˆ—é•¿åº¦")
    parser.add_argument("--max-new-tokens", type=int, default=1024, help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.3, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top-p", type=float, default=0.9, help="top_pé‡‡æ ·")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--device", type=str, default=DEVICE, help="è®¾å¤‡ï¼ˆcpu/cudaï¼‰")
    parser.add_argument("--log-dir", type=str, default="logs", help="æ—¥å¿—ç›®å½•")
    
    # ==================== æ¶ˆèå®éªŒå‚æ•° ====================
    parser.add_argument(
        "--use-retrieval-only",
        action="store_true",
        help="æ¶ˆè1ï¼šåªç”¨æ£€ç´¢ä¸€è‡´æ€§è¯„ä¼°ï¼ˆç§»é™¤é—®é¢˜æœ¬è´¨è¯„ä¼°ï¼‰"
    )
    parser.add_argument(
        "--use-intrinsic-only",
        action="store_true",
        help="æ¶ˆè2ï¼šåªç”¨é—®é¢˜æœ¬è´¨è¯„ä¼°ï¼ˆç§»é™¤æ£€ç´¢ä¸€è‡´æ€§ï¼‰"
    )
    parser.add_argument(
        "--fixed-topk",
        action="store_true",
        help="æ¶ˆè3ï¼šå›ºå®šTop-Kæ–‡æ¡£æ•°é‡ï¼ˆç§»é™¤è‡ªé€‚åº”é€‰æ‹©ï¼‰"
    )
    parser.add_argument(
        "--flat-kg",
        action="store_true",
        help="æ¶ˆè4ï¼šæ‰å¹³KGç»“æ„ï¼ˆç§»é™¤å±‚æ¬¡ç»“æ„ï¼‰"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger, log_file = setup_logging(args.log_dir)
    logger.info("æ··åˆRAGæ£€ç´¢ç³»ç»Ÿå¯åŠ¨")
    logger.info(f"ğŸ”§ GPUé…ç½®: ä½¿ç”¨ {len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))} å¼ GPUå¡")
    logger.info(f"   - CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}")
    logger.info(f"   - Tensor Parallel Size: {args.tp}")
    logger.info(f"è¾“å…¥æ–‡ä»¶: {args.input}")
    
    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if args.output is None:
        input_dir = os.path.dirname(args.input)
        input_name = os.path.splitext(os.path.basename(args.input))[0]
        args.output = os.path.join(input_dir, f"{input_name}_hybrid_pred.json")
    
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    
    # åˆå§‹åŒ–æ··åˆRAGç³»ç»Ÿ
    llm_params = {
        "tp": args.tp,
        "gpu_mem_util": args.gpu_mem_util,
        "max_model_len": args.max_model_len,
        "max_new_tokens": args.max_new_tokens,
        "temperature": args.temperature,
        "top_p": args.top_p,
    }
    
    rag_system = HybridLegalRAG(
        vector_db_path=args.vector_db,
        collection_name=args.collection,
        kg_working_dir=args.kg_dir,
        embedding_model_name=args.embedding_model,
        device=args.device,
        llm_model_path=args.llm_model,
        llm_params=llm_params,
        # æ¶ˆèå®éªŒå‚æ•°
        use_retrieval_only=args.use_retrieval_only,
        use_intrinsic_only=args.use_intrinsic_only,
        fixed_topk=args.fixed_topk,
        flat_kg=args.flat_kg
    )
    
    # åŠ è½½æŸ¥è¯¢æ•°æ®é›†
    logger.info(f"æ­£åœ¨åŠ è½½æŸ¥è¯¢æ•°æ®é›†: {args.input}")
    with open(args.input, 'r', encoding='utf-8') as f:
        queries = json.load(f)
    
    logger.info(f"å…±åŠ è½½ {len(queries)} æ¡æŸ¥è¯¢")
    
    # æ‰¹é‡å¤„ç†
    results = []
    question_times = []  # è®°å½•æ¯ä¸ªé—®é¢˜çš„å¤„ç†æ—¶é—´
    
    for i, item in enumerate(queries):
        logger.info(f"\nå¤„ç†ç¬¬ {i+1}/{len(queries)} æ¡æŸ¥è¯¢")
        
        query = item.get("question", "").strip()
        instruction = item.get("instruction", "").strip()
        
        if not query:
            logger.warning("æŸ¥è¯¢ä¸ºç©ºï¼Œè·³è¿‡")
            new_item = dict(item)
            new_item["prediction"] = ""
            # æ ¸å¿ƒæŒ‡æ ‡
            new_item["bm25_top1_score"] = 10.0
            new_item["overlap_ratio"] = 0.0
            new_item["top3_overlap"] = 0.0
            new_item["combined_score"] = 0.0
            new_item["used_kg"] = False
            new_item["answer_time"] = 0.0
            results.append(new_item)
            question_times.append({"question_id": i+1, "question": query[:50], "time": 0.0})
            continue
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æ‰§è¡Œæ£€ç´¢å’Œå›ç­”
        try:
            result = rag_system.retrieve_and_answer(
                query=query,
                instruction=instruction,
                top_k=args.top_k,
                correlation_threshold=args.threshold,
                alpha=args.alpha
            )
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            answer_time = time.time() - start_time
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            answer_time = time.time() - start_time
            
            # ä¿å­˜ç»“æœ
            new_item = dict(item)
            new_item["prediction"] = result["answer"]
            # æ ¸å¿ƒæŒ‡æ ‡
            new_item["bm25_top1_score"] = result["bm25_top1_score"]
            new_item["overlap_ratio"] = result["overlap_ratio"]
            new_item["top3_overlap"] = result["top3_overlap"]
            new_item["combined_score"] = result["combined_score"]
            
            # æ–°å¢ï¼š5ç»´åº¦æŒ‡æ ‡
            if result.get('num_covered_dimensions') is not None:
                new_item["num_covered_dimensions"] = result.get("num_covered_dimensions", 0)
                new_item["question_complexity"] = result.get("question_complexity", 0.0)
                new_item["num_sub_questions"] = result.get("decomposition", {}).get("num_sub_questions", 1)
                new_item["covered_dimensions"] = result.get("covered_dimensions", [])
            else:
                # å…¼å®¹æ—§æ¨¡å¼
                new_item["query_structure_simplicity"] = result.get("query_structure_simplicity", 0.0)
                new_item["retrieval_consistency_confidence"] = result.get("retrieval_consistency_confidence", 0.0)
                new_item["dimension_difference"] = result.get("dimension_difference", 0.0)
                new_item["confidence_level"] = result.get("confidence_level", "unknown")
            
            new_item["used_kg"] = result["used_kg"]
            new_item["elapsed_time"] = result["elapsed_time"]
            new_item["answer_time"] = answer_time  # æ·»åŠ å›ç­”æ—¶é—´
            results.append(new_item)
            
            # è®°å½•é—®é¢˜æ—¶é—´
            question_times.append({
                "question_id": i+1,
                "question": query[:50] + ("..." if len(query) > 50 else ""),
                "time": answer_time,
                "used_kg": result["used_kg"]
            })
            
            logger.info(f"â±ï¸  æœ¬é—®é¢˜å¤„ç†æ—¶é—´: {answer_time:.2f}ç§’")
            
        except Exception as e:
            # è®¡ç®—å¤„ç†æ—¶é—´ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿè®°å½•ï¼‰
            answer_time = time.time() - start_time
            
            logger.error(f"å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            new_item = dict(item)
            new_item["prediction"] = f"å¤„ç†å¤±è´¥: {str(e)}"
            # æ ¸å¿ƒæŒ‡æ ‡
            new_item["bm25_top1_score"] = 10.0
            new_item["overlap_ratio"] = 0.0
            new_item["top3_overlap"] = 0.0
            new_item["combined_score"] = 0.0
            new_item["used_kg"] = False
            new_item["answer_time"] = answer_time
            results.append(new_item)
            
            # è®°å½•é—®é¢˜æ—¶é—´
            question_times.append({
                "question_id": i+1,
                "question": query[:50] + ("..." if len(query) > 50 else ""),
                "time": answer_time,
                "used_kg": False
            })
    
    # ä¿å­˜ç»“æœ
    logger.info(f"\næ­£åœ¨ä¿å­˜ç»“æœåˆ°: {args.output}")
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_queries = len(results)
    kg_used_count = sum(1 for r in results if r.get("used_kg", False))
    
    # ç»Ÿè®¡å„ç­–ç•¥ä½¿ç”¨æ¬¡æ•°
    strategy_counts = {
        'dense_vector': 0,
        'hybrid_rerank': 0,
        'flat_kg': 0,
        'hierarchical_kg': 0
    }
    for r in results:
        strategy = r.get('strategy', 'unknown')
        if strategy in strategy_counts:
            strategy_counts[strategy] += 1
    
    # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡çš„å¹³å‡å€¼
    avg_bm25_top1 = np.mean([r.get("bm25_top1_score", 10.0) for r in results])
    avg_overlap_ratio = np.mean([r.get("overlap_ratio", 0.0) for r in results])
    avg_top3_overlap = np.mean([r.get("top3_overlap", 0.0) for r in results])
    avg_combined_score = np.mean([r.get("combined_score", 0.0) for r in results])
    
    # è®¡ç®—æ—¶é—´ç»Ÿè®¡
    total_time = sum(qt["time"] for qt in question_times)
    avg_time = total_time / len(question_times) if question_times else 0
    
    logger.info(f"\n{'='*60}")
    logger.info("å¤„ç†å®Œæˆï¼")
    logger.info(f"æ€»æŸ¥è¯¢æ•°: {total_queries}")
    logger.info(f"ä½¿ç”¨çŸ¥è¯†å›¾è°±: {kg_used_count} ({kg_used_count/total_queries*100:.1f}%)")
    
    logger.info(f"\nğŸ“Š ç­–ç•¥ä½¿ç”¨ç»Ÿè®¡:")
    logger.info(f"{'='*60}")
    logger.info(f"ç­–ç•¥1 - å¯†é›†å‘é‡æ£€ç´¢: {strategy_counts['dense_vector']} ({strategy_counts['dense_vector']/total_queries*100:.1f}%)")
    logger.info(f"ç­–ç•¥2 - æ··åˆæ£€ç´¢+äºŒæ¬¡æ’åº: {strategy_counts['hybrid_rerank']} ({strategy_counts['hybrid_rerank']/total_queries*100:.1f}%)")
    logger.info(f"ç­–ç•¥3 - ä¸€ç»´å›¾è°±æ£€ç´¢: {strategy_counts['flat_kg']} ({strategy_counts['flat_kg']/total_queries*100:.1f}%)")
    logger.info(f"ç­–ç•¥4 - å±‚æ¬¡åŒ–å›¾è°±æ£€ç´¢: {strategy_counts['hierarchical_kg']} ({strategy_counts['hierarchical_kg']/total_queries*100:.1f}%)")
    
    logger.info(f"\nğŸ“Š æ ¸å¿ƒæŒ‡æ ‡ç»Ÿè®¡:")
    logger.info(f"{'='*60}")
    logger.info(f"1ï¸âƒ£ BM25 Top1åˆ†æ•°:")
    logger.info(f"   - å¹³å‡å€¼: {avg_bm25_top1:.3f}")
    logger.info(f"2ï¸âƒ£ æ–‡æ¡£é‡å ç‡:")
    logger.info(f"   - å¹³å‡å€¼: {avg_overlap_ratio:.3f}")
    logger.info(f"3ï¸âƒ£ Top-3é‡å ç‡:")
    logger.info(f"   - å¹³å‡å€¼: {avg_top3_overlap:.3f}")
    logger.info(f"\nğŸ¯ ç»¼åˆå¤æ‚åº¦è¯„åˆ†:")
    logger.info(f"   - å¹³å‡ç»¼åˆå¤æ‚åº¦: {avg_combined_score:.3f}")
    
    logger.info(f"\nâ±ï¸  æ—¶é—´ç»Ÿè®¡:")
    logger.info(f"{'='*60}")
    logger.info(f"æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
    logger.info(f"å¹³å‡æ¯é¢˜æ—¶é—´: {avg_time:.2f}ç§’")
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
    logger.info(f"æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
    logger.info(f"{'='*60}")


if __name__ == "__main__":
    main()
