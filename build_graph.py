import argparse
from concurrent.futures import ProcessPoolExecutor,as_completed
from dataclasses import field
import json
import os
import logging
import numpy as np
from openai import OpenAI
import tiktoken
from tqdm import tqdm
import yaml
from sentence_transformers import SentenceTransformer
import torch
from openai import AsyncOpenAI, OpenAI
from _cluster_utils import Hierarchical_Clustering
from tools.utils import write_jsonl,InstanceManager
from database_utils import build_vector_search,create_db_table_mysql,insert_data_to_mysql
import requests
import multiprocessing
import gc
logger=logging.getLogger(__name__)

def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
    except Exception as e:
        print(f"Warning: Failed to clear GPU memory: {e}")

def safe_embedding_init(entities: list[dict]) -> list[dict]:
    """å®‰å…¨çš„embeddingåˆå§‹åŒ–ï¼ŒåŒ…å«é”™è¯¯å¤„ç†å’Œå†…å­˜ç®¡ç†"""
    global _ST_EMB
    try:
        clear_gpu_memory()
        # ç¡®ä¿ embedding æ¨¡å‹åœ¨ CPU ä¸Šï¼ˆé¿å…ä¸ vLLM äº‰æŠ¢ GPU æ˜¾å­˜ï¼‰
        if hasattr(_ST_EMB, 'device') and _ST_EMB.device.type != "cpu":
            print("âš ï¸  è­¦å‘Š: embedding æ¨¡å‹ä¸åœ¨ CPU ä¸Šï¼Œå¼ºåˆ¶ç§»åŠ¨åˆ° CPU")
            _ST_EMB = _ST_EMB.to("cpu")
        return embedding_init(entities)
    except torch.cuda.OutOfMemoryError as e:
        print(f"CUDA OOM in embedding_init: {e}")
        clear_gpu_memory()
        # å¼ºåˆ¶ä½¿ç”¨ CPU è¿›è¡Œ embedding
        print("ğŸ”„ åˆ‡æ¢åˆ° CPU è¿›è¡Œ embedding è®¡ç®—")
        # å¦‚æœè¿˜æ˜¯å†…å­˜ä¸è¶³ï¼Œå°è¯•æ›´å°çš„batch
        texts = [truncate_text(i['description']) for i in entities]
        batch_size = max(1, min(4, len(texts)))  # è¿›ä¸€æ­¥å‡å°‘batch_sizeï¼Œä½†è‡³å°‘ä¸º1
        # ç¡®ä¿ä½¿ç”¨ CPU
        if hasattr(_ST_EMB, 'device') and _ST_EMB.device.type != "cpu":
            _ST_EMB = _ST_EMB.to("cpu")
        vectors = _ST_EMB.encode(
            texts,
            batch_size=batch_size,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False,
        )
        for i, entity in enumerate(entities):
            entity['vector'] = np.array(vectors[i])
        return entities
    except Exception as e:
        print(f"Error in embedding_init: {e}")
        clear_gpu_memory()
        raise e

with open('config.yaml', 'r') as file:
    config = yaml.safe_load(file)
MODEL = config['deepseek']['model']
DEEPSEEK_API_KEY = config['deepseek']['api_key']
DEEPSEEK_URL = config['deepseek']['base_url']
EMBEDDING_MODEL = config['glm']['model']
EMBEDDING_URL = config['glm']['base_url']
TOTAL_TOKEN_COST = 0
TOTAL_API_CALL_COST = 0

# Initialize local sentence-transformers embedding model once
_force_cpu = os.environ.get("FORCE_CPU", "1") == "1"  # é»˜è®¤ä½¿ç”¨ CPUï¼Œé¿å…ä¸ vLLM äº‰æŠ¢ GPU æ˜¾å­˜
_device = "cpu" if _force_cpu else ("cuda" if torch.cuda.is_available() else "cpu")
_st_model_name = EMBEDDING_MODEL if isinstance(EMBEDDING_MODEL, str) and len(EMBEDDING_MODEL) > 0 else "BAAI/bge-m3"

# æ¸…ç†GPUç¼“å­˜
try:
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
except:
    pass

try:
    _ST_EMB = SentenceTransformer(_st_model_name, device=_device)
    print(f"âœ… æˆåŠŸåŠ è½½ Embedding æ¨¡å‹: {_st_model_name} (è®¾å¤‡: {_device})")
except Exception as e:
    print(f"Failed to load {_st_model_name}: {e}")
    print("Falling back to CPU and BAAI/bge-m3")
    _device = "cpu"
    _ST_EMB = SentenceTransformer("BAAI/bge-m3", device=_device)
    print(f"âœ… ä½¿ç”¨å¤‡ç”¨ Embedding æ¨¡å‹: BAAI/bge-m3 (è®¾å¤‡: {_device})")
_ST_EMB.max_seq_length = 4096
print(f"ğŸ“ Embedding æ¨¡å‹æœ€å¤§åºåˆ—é•¿åº¦: {_ST_EMB.max_seq_length}")

def get_common_rag_res(WORKING_DIR):
    entity_path=f"{WORKING_DIR}/entity.jsonl"
    relation_path=f"{WORKING_DIR}/relation.jsonl"
    # i=0
    e_dic={}
    with open(entity_path,"r")as f:
        for xline in f:
            
            line=json.loads(xline)
            entity_name=str(line['entity_name'])
            description=line['description']
            source_id=line['source_id']
            if entity_name not in e_dic.keys():
                e_dic[entity_name]=dict(
                    entity_name=str(entity_name),
                    description=description,
                    source_id=source_id,
                    degree=0,
                )
            else:
                e_dic[entity_name]['description']+="|Here is another description : "+ description
                if e_dic[entity_name]['source_id']!= source_id:
                    e_dic[entity_name]['source_id']+= "|"+source_id
                    
    #         i+=1
    #         if i==1000:
    #             break
    # i=0
    r_dic={}
    with open(relation_path,"r")as f:
        for xline in f:
            
            line=json.loads(xline)

            # å¤„ç†æ•°ç»„æ ¼å¼çš„å…³ç³»æ•°æ®
            if isinstance(line, list):
                # å¦‚æœ line æ˜¯æ•°ç»„ï¼Œéå†æ•°ç»„ä¸­çš„æ¯ä¸ªå…³ç³»å¯¹è±¡
                for relation in line:
                    src_tgt=str(relation['src_id'])
                    tgt_src=str(relation['tgt_id'])
                    description=relation['description']
                    weight=relation.get('weight', 1)
                    source_id=relation['source_id']
                    r_dic[(src_tgt,tgt_src)]={
                        'src_tgt':str(src_tgt),
                        'tgt_src':str(tgt_src),
                        'description':description,
                        'weight':weight,
                        'source_id':source_id
                    }
            else:
                # å¦‚æœ line æ˜¯å•ä¸ªå¯¹è±¡ï¼ŒæŒ‰åŸæ¥çš„æ–¹å¼å¤„ç†
                src_tgt=str(line['src_tgt'])
                tgt_src=str(line['tgt_src'])
                description=line['description']
                weight=1
                source_id=line['source_id']
                r_dic[(src_tgt,tgt_src)]={
                    'src_tgt':str(src_tgt),
                    'tgt_src':str(tgt_src),
                    'description':description,
                    'weight':weight,
                    'source_id':source_id
                }
            # e_dic[src_tgt]['degree']+=1
            # e_dic[tgt_src]['degree']+=1
            # i+=1
            # if i==1000:
            #     break
    
    
    return e_dic,r_dic


# Replace OpenAI embedding with local sentence-transformers

def embedding(texts: list[str]) -> np.ndarray:  # local embedding
    # å¤„ç†å•ä¸ªæ–‡æœ¬çš„æƒ…å†µ
    if isinstance(texts, str):
        texts = [texts]
    
    # ç¡®ä¿batch_sizeè‡³å°‘ä¸º1
    batch_size = max(1, min(16, len(texts)))  # ä»64å‡å°‘åˆ°16ï¼Œä½†è‡³å°‘ä¸º1
    vectors = _ST_EMB.encode(
        texts,
        batch_size=batch_size,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False,
    )
    return np.array(vectors)

def embedding_init(entities:list[dict])-> list[dict]: 
    global _ST_EMB
    # ç¡®ä¿ embedding æ¨¡å‹åœ¨ CPU ä¸Šï¼ˆé¿å…ä¸ vLLM äº‰æŠ¢ GPU æ˜¾å­˜ï¼‰
    if hasattr(_ST_EMB, 'device') and _ST_EMB.device.type != "cpu":
        print("âš ï¸  è­¦å‘Š: embedding æ¨¡å‹ä¸åœ¨ CPU ä¸Šï¼Œå¼ºåˆ¶ç§»åŠ¨åˆ° CPU")
        _ST_EMB = _ST_EMB.to("cpu")
    
    texts=[truncate_text(i['description']) for i in entities]
    # å‡å°‘batch_sizeä»¥èŠ‚çœå†…å­˜ï¼Œç¡®ä¿è‡³å°‘ä¸º1
    batch_size = max(1, min(8, len(texts)))  # è¿›ä¸€æ­¥å‡å°‘batch_sizeï¼Œé¿å…å†…å­˜é—®é¢˜
    vectors = _ST_EMB.encode(
        texts,
        batch_size=batch_size,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False,
    )
    for i, entity in enumerate(entities):
        entity['vector'] = np.array(vectors[i])
    return entities

tokenizer = tiktoken.get_encoding("cl100k_base")
def truncate_text(text, max_tokens=4096):
    tokens = tokenizer.encode(text)
    if len(tokens) > max_tokens:
        tokens = tokens[:max_tokens]
    truncated_text = tokenizer.decode(tokens)
    return truncated_text

def embedding_data(entity_results):
    entities = [v for k, v in entity_results.items()]
    entity_with_embeddings=[]
    # å‡å°‘batch_sizeå’Œmax_workersä»¥èŠ‚çœå†…å­˜
    embeddings_batch_size = int(os.environ.get("EMB_BATCH", "16"))  # é»˜è®¤16ï¼Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜å ç”¨
    num_embeddings_batches = (len(entities) + embeddings_batch_size - 1) // embeddings_batch_size
    
    batches = [
        entities[i * embeddings_batch_size : min((i + 1) * embeddings_batch_size, len(entities))]
        for i in range(num_embeddings_batches)
    ]

    # å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡å…³é—­å¤šè¿›ç¨‹ï¼Œæˆ–æ§åˆ¶å¹¶å‘æ•°
    emb_max_workers = int(os.environ.get("EMB_MAX_WORKERS", "1"))  # é»˜è®¤1ï¼Œé¿å…å¤šè¿›ç¨‹CUDAåˆå§‹åŒ–é—®é¢˜
    if emb_max_workers <= 0:
        # ä¸²è¡Œå¤„ç†ï¼Œæœ€ç¨³å¦¥ï¼ˆCPU/ä½æ˜¾å­˜ç¯å¢ƒï¼‰
        for batch in tqdm(batches):
            try:
                result = safe_embedding_init(batch)
                entity_with_embeddings.extend(result)
                clear_gpu_memory()
            except Exception as e:
                print(f"Error processing batch: {e}")
                clear_gpu_memory()
                raise e
    else:
        # å—æ§å¹¶å‘
        with ProcessPoolExecutor(max_workers=emb_max_workers) as executor:
            futures = [executor.submit(safe_embedding_init, batch) for batch in batches]
            for future in tqdm(as_completed(futures), total=len(futures)):
                try:
                    result = future.result()
                    entity_with_embeddings.extend(result)
                    clear_gpu_memory()
                except Exception as e:
                    print(f"Error processing batch: {e}")
                    clear_gpu_memory()
                    raise e

    for i in entity_with_embeddings:
        entiy_name=i['entity_name']
        vector=i['vector']
        entity_results[entiy_name]['vector']=vector
    return entity_results



    
            

def hierarchical_clustering(global_config):
    entity_results,relation_results=get_common_rag_res(global_config['working_dir'])
    all_entities=embedding_data(entity_results)
    hierarchical_cluster = Hierarchical_Clustering()
    all_entities,generate_relations,community =hierarchical_cluster.perform_clustering(global_config=global_config,entities=all_entities,relations=relation_results,\
        WORKING_DIR=WORKING_DIR,max_workers=global_config['max_workers'])
    try :
        all_entities[-1]['vector']=embedding(all_entities[-1]['description'])
        build_vector_search(all_entities, f"{WORKING_DIR}")
    except Exception as e:
        print(f"Error in build_vector_search: {e}")
    for layer in all_entities:
        if type(layer) != list :
            if "vector" in layer.keys():
                del layer["vector"]
            continue
        for item in layer:
            if "vector" in item.keys():
                del item["vector"]
            if len(layer)==1:
                item['parent']='root'
    save_relation=[
    v for k, v in generate_relations.items()
]
    save_community=[
    v for k, v in community.items()
]
    
    # åˆ é™¤æ—§æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œç¡®ä¿æ¯æ¬¡éƒ½æ˜¯å…¨æ–°æ„å»º
    relations_file = f"{global_config['working_dir']}/generate_relations.json"
    community_file = f"{global_config['working_dir']}/community.json"
    
    if os.path.exists(relations_file):
        os.remove(relations_file)
        print(f"ğŸ—‘ï¸  å·²åˆ é™¤æ—§çš„å…³ç³»æ–‡ä»¶: {relations_file}")
    
    if os.path.exists(community_file):
        os.remove(community_file)
        print(f"ğŸ—‘ï¸  å·²åˆ é™¤æ—§çš„ç¤¾åŒºæ–‡ä»¶: {community_file}")
    
    # å†™å…¥æ–°æ–‡ä»¶ï¼ˆä½¿ç”¨è¿½åŠ æ¨¡å¼ï¼Œä½†å› ä¸ºæ–‡ä»¶å·²åˆ é™¤ï¼Œå®é™…æ˜¯åˆ›å»ºæ–°æ–‡ä»¶ï¼‰
    write_jsonl(save_relation, relations_file)
    write_jsonl(save_community, community_file)
    
    try:
        # ä½¿ç”¨working_dirçš„basenameä½œä¸ºæ•°æ®åº“åç§°ï¼Œç¡®ä¿ä¸€è‡´æ€§
        db_name = os.path.basename(global_config['working_dir'].rstrip('/'))
        create_db_table_mysql(global_config['working_dir'], target_database=db_name)
        insert_data_to_mysql(global_config['working_dir'], target_database=db_name)
    except Exception as e:
        print(f"Error in database operations: {e}")
        print("Continuing without database operations...")
    
if __name__=="__main__":
    # ç¨‹åºå¼€å§‹æ—¶æ¸…ç†GPUå†…å­˜
    clear_gpu_memory()
    
    try:
        multiprocessing.set_start_method("spawn", force=True)  # å¼ºåˆ¶è®¾ç½®
    except RuntimeError:
        pass  # å·²ç»è®¾ç½®è¿‡ï¼Œå¿½ç•¥
    parser = argparse.ArgumentParser()
    parser.add_argument("-p", "--path", type=str, default="/newdataf/SJ/LeanRAG/GraphExtraction/ttt/")
    parser.add_argument("-n", "--num", type=int, default=2)
    args = parser.parse_args()

    WORKING_DIR = args.path
    num=args.num
    instanceManager=InstanceManager(
        url="http://10.61.2.49",  # æ›¿æ¢ä¸ºä½ çš„ ollama æœåŠ¡å™¨åœ°å€
        ports=[11434 for i in range(num)],  # ollama é»˜è®¤ç«¯å£
        gpus=[i for i in range(num)],
        generate_model="deepseek-r1-32b:latest",  # æ›¿æ¢ä¸ºä½ åœ¨ ollama ä¸­éƒ¨ç½²çš„æ¨¡å‹å
        startup_delay=30
    )
    global_config={}
    # å‡å°‘max_workersä»¥é¿å…GPUå†…å­˜ç«äº‰
    global_config['max_workers']=min(2, num*2)  # ä»num*4å‡å°‘åˆ°num*2ï¼Œæœ€å¤§ä¸è¶…è¿‡2
    global_config['working_dir']=WORKING_DIR
    global_config['use_llm_func']=instanceManager.generate_text
    global_config['embeddings_func']=embedding
    global_config["special_community_report_llm_kwargs"]=field(
        default_factory=lambda: {"response_format": {"type": "json_object"}}
    )
    hierarchical_clustering(global_config)