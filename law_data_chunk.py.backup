#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
法律数据处理脚本：从law_library.jsonl提取数据并分块
原始版本，没有融合ragflow
适用于中文法律知识图谱构建
"""

import json
import tiktoken
from hashlib import md5
import os


def compute_mdhash_id(content, prefix: str = ""):
    """计算内容的MD5哈希值"""
    return prefix + md5(content.encode()).hexdigest()


def extract_legal_data(input_jsonl, output_json, start_idx=0, end_idx=50):
    """
    从JSONL文件中提取指定范围的法律数据
    
    参数:
        input_jsonl: 输入的JSONL文件路径
        output_json: 输出的JSON文件路径
        start_idx: 起始索引
        end_idx: 结束索引（不包含）
    """
    print(f"正在从 {input_jsonl} 提取数据...")
    print(f"提取范围: 第 {start_idx} 到 {end_idx-1} 条")
    
    data = []
    with open(input_jsonl, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= end_idx:
                break
            if i >= start_idx and line.strip():
                try:
                    item = json.loads(line)
                    data.append(item)
                except json.JSONDecodeError as e:
                    print(f"警告: 第 {i} 行JSON解析失败: {e}")
                    continue
    
    # 保存提取的数据
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"成功提取 {len(data)} 条数据到 {output_json}")
    return data


def chunk_legal_documents(
    legal_items,
    model_name="cl100k_base",
    max_token_size=512,
    overlap_token_size=64,
    merge_small_items=True
):
    """
    对法律文档进行分块处理
    
    参数:
        legal_items: 法律条文列表，每项包含 id, name, content
        model_name: tiktoken编码器模型名称
        max_token_size: 每个块的最大token数
        overlap_token_size: 块之间的重叠token数
        merge_small_items: 是否合并小的法条
    
    返回:
        分块结果列表
    """
    ENCODER = tiktoken.get_encoding(model_name)
    results = []
    
    if merge_small_items:
        # 策略：合并小法条到合适大小
        current_batch = []
        current_tokens = 0
        current_ids = []
        
        for item in legal_items:
            # 组合法条标题和内容
            full_text = f"{item['name']}: {item['content']}"
            tokens = ENCODER.encode(full_text)
            token_count = len(tokens)
            
            # 如果单个法条就超过max_token_size，需要拆分
            if token_count > max_token_size:
                # 先保存当前批次
                if current_batch:
                    merged_text = " ".join(current_batch)
                    results.append({
                        "hash_code": compute_mdhash_id(merged_text),
                        "text": merged_text.strip(),
                        "source_type": "merged_articles",
                        "source_ids": current_ids,
                        "token_count": current_tokens
                    })
                    current_batch = []
                    current_tokens = 0
                    current_ids = []
                
                # 对大法条进行分块
                for start in range(0, token_count, max_token_size - overlap_token_size):
                    chunk_tokens = tokens[start : start + max_token_size]
                    chunk_text = ENCODER.decode(chunk_tokens)
                    results.append({
                        "hash_code": compute_mdhash_id(chunk_text),
                        "text": chunk_text.strip(),
                        "source_type": "split_article",
                        "source_id": item['id'],
                        "source_name": item['name'],
                        "token_count": len(chunk_tokens)
                    })
            
            # 如果加上当前法条会超过max_token_size，先保存当前批次
            elif current_tokens + token_count > max_token_size:
                if current_batch:
                    merged_text = " ".join(current_batch)
                    results.append({
                        "hash_code": compute_mdhash_id(merged_text),
                        "text": merged_text.strip(),
                        "source_type": "merged_articles",
                        "source_ids": current_ids,
                        "token_count": current_tokens
                    })
                current_batch = [full_text]
                current_tokens = token_count
                current_ids = [item['id']]
            
            # 否则加入当前批次
            else:
                current_batch.append(full_text)
                current_tokens += token_count
                current_ids.append(item['id'])
        
        # 保存最后一个批次
        if current_batch:
            merged_text = " ".join(current_batch)
            results.append({
                "hash_code": compute_mdhash_id(merged_text),
                "text": merged_text.strip(),
                "source_type": "merged_articles",
                "source_ids": current_ids,
                "token_count": current_tokens
            })
    
    else:
        # 策略：每个法条独立处理
        for item in legal_items:
            full_text = f"{item['name']}: {item['content']}"
            tokens = ENCODER.encode(full_text)
            token_count = len(tokens)
            
            # 如果法条较长，需要分块
            if token_count > max_token_size:
                for start in range(0, token_count, max_token_size - overlap_token_size):
                    chunk_tokens = tokens[start : start + max_token_size]
                    chunk_text = ENCODER.decode(chunk_tokens)
                    results.append({
                        "hash_code": compute_mdhash_id(chunk_text),
                        "text": chunk_text.strip(),
                        "source_type": "split_article",
                        "source_id": item['id'],
                        "source_name": item['name'],
                        "token_count": len(chunk_tokens)
                    })
            else:
                # 短法条直接保存
                results.append({
                    "hash_code": compute_mdhash_id(full_text),
                    "text": full_text.strip(),
                    "source_type": "single_article",
                    "source_id": item['id'],
                    "source_name": item['name'],
                    "token_count": token_count
                })
    
    return results


def main():
    """主函数"""
    print("=" * 60)
    print("法律数据处理与分块脚本")
    print("=" * 60)
    
    # 配置参数
    # 直接读取外部JSON文件（数组结构），跳过JSONL提取
    input_json = "/newdataf/SJ/LeanRAG/datasets/law_test.json"
    
    # 第二步：分块参数
    max_token_size = 1024          # 每个块最大token数
    overlap_token_size = 128       # 重叠token数
    merge_small_items = False       # 是否合并小法条（KG抽取阶段关闭合并，保持法条原子性）
    output_chunk_file = "datasets/law_test_chunk_v2.json"
    
    # 检查输入文件是否存在
    if not os.path.exists(input_json):
        print(f"错误: 输入文件不存在: {input_json}")
        print(f"当前工作目录: {os.getcwd()}")
        print(f"请确保文件路径正确!")
        return
    
    # 直接读取JSON数组
    print(f"\n{'='*60}")
    print("步骤1: 读取法律数据（JSON）")
    print(f"{'='*60}")
    with open(input_json, 'r', encoding='utf-8') as f:
        legal_data = json.load(f)
    
    if not legal_data:
        print("错误: 没有提取到任何数据!")
        return
    
    # 第二步：分块处理
    print(f"\n{'='*60}")
    print("步骤2: 分块处理")
    print(f"{'='*60}")
    print(f"分块参数:")
    print(f"  - 最大token数: {max_token_size}")
    print(f"  - 重叠token数: {overlap_token_size}")
    print(f"  - 合并策略: {'启用' if merge_small_items else '禁用'}")
    
    results = chunk_legal_documents(
        legal_data,
        max_token_size=max_token_size,
        overlap_token_size=overlap_token_size,
        merge_small_items=merge_small_items
    )
    
    # 保存分块结果
    with open(output_chunk_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # 统计信息
    print(f"\n{'='*60}")
    print("处理完成!")
    print(f"{'='*60}")
    print(f"输入条文数: {len(legal_data)}")
    print(f"输出块数: {len(results)}")
    print(f"输出文件: {output_chunk_file}")
    
    # 显示分块类型统计
    source_types = {}
    total_tokens = 0
    for item in results:
        source_type = item.get('source_type', 'unknown')
        source_types[source_type] = source_types.get(source_type, 0) + 1
        total_tokens += item.get('token_count', 0)
    
    print(f"\n分块类型统计:")
    for stype, count in source_types.items():
        print(f"  - {stype}: {count} 块")
    
    if results:
        print(f"\n总token数: {total_tokens}")
        print(f"平均每块token数: {total_tokens // len(results)}")
    
    # 显示前3个结果样例
    print(f"\n{'='*60}")
    print("前3个分块样例:")
    print(f"{'='*60}")
    for i, item in enumerate(results[:3]):
        print(f"\n[块 {i+1}]")
        print(f"  Hash: {item['hash_code'][:16]}...")
        print(f"  类型: {item['source_type']}")
        print(f"  Token数: {item.get('token_count', 0)}")
        text_preview = item['text'][:80].replace('\n', ' ')
        print(f"  文本预览: {text_preview}...")


if __name__ == "__main__":
    main()


