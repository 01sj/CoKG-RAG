import os
import json
from _utils import split_string_by_multi_markers,_handle_single_entity_extraction,\
    _handle_single_relationship_extraction,clean_str,pack_user_ass_to_openai_messages
import sys
from pathlib import Path
# åŠ¨æ€åŠ å…¥é¡¹ç›®æ ¹ç›®å½•ï¼Œé¿å…ç¡¬ç¼–ç è·¯å¾„å¯¼è‡´å¯¼å…¥å¤±è´¥
project_root = Path(__file__).resolve().parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
from tools.utils import InstanceManager,write_jsonl
from collections import Counter, defaultdict
from prompt import PROMPTS
import asyncio
import re
import copy
import time
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ç”¨äºæ–‡æœ¬é•¿åº¦æ£€æŸ¥å’Œæˆªæ–­
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    print("âš ï¸  tiktoken æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—ï¼ˆå¯èƒ½ä¸å¤Ÿå‡†ç¡®ï¼‰")


class LocalGenerator:
    def __init__(self, model_id: str, max_new_tokens: int = 1024, temperature: float = 0.2, top_p: float = 0.95):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto" if self.device == "cuda" else None,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            trust_remote_code=True,
        )
        if self.device != "cuda":
            self.model.to(self.device)
        self.generation_kwargs = dict(max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p)

    def __call__(self, prompt: str, system_prompt: str = None, history_messages = [], **kwargs):
        args = dict(self.generation_kwargs)
        if "max_new_tokens" in kwargs:
            args["max_new_tokens"] = kwargs["max_new_tokens"]
        if "temperature" in kwargs:
            args["temperature"] = kwargs["temperature"]
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                do_sample=args.get("temperature", 0.0) > 0,
                temperature=args.get("temperature", 0.2),
                top_p=args.get("top_p", 0.95),
                max_new_tokens=args.get("max_new_tokens", 1024),
                repetition_penalty=args.get("repetition_penalty", 1.1),
                eos_token_id=self.tokenizer.eos_token_id,
            )
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)



def get_chunk(chunk_file):
    doc_name=os.path.basename(chunk_file).rsplit(".",1)[0]
    
    # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(chunk_file):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {chunk_file}")
    
    print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {chunk_file}")
    
    # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–ï¼Œç„¶åå°è¯•ä¸åŒçš„è§£ç æ–¹å¼
    try:
        with open(chunk_file, "rb") as f:
            raw_data = f.read()
        
        print(f"æ–‡ä»¶å¤§å°: {len(raw_data)} å­—èŠ‚")
        
        # å°è¯•ä¸åŒçš„ç¼–ç ï¼Œä½¿ç”¨errors='ignore'æ¥è·³è¿‡æ— æ³•è§£ç çš„å­—ç¬¦
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'cp1252', 'utf-16', 'utf-32']
        
        for encoding in encodings:
            try:
                print(f"å°è¯•ç¼–ç : {encoding}")
                
                # ä½¿ç”¨errors='ignore'æ¥å¿½ç•¥æ— æ³•è§£ç çš„å­—ç¬¦
                content = raw_data.decode(encoding, errors='ignore')
                
                # æ¸…ç†æ§åˆ¶å­—ç¬¦ï¼Œä½†ä¿ç•™å¿…è¦çš„JSONå­—ç¬¦
                cleaned_content = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', content)
                
                # å°è¯•è§£æJSON
                corpus = json.loads(cleaned_content)
                print(f"æˆåŠŸä½¿ç”¨ç¼–ç  {encoding} è¯»å–æ–‡ä»¶ (å¿½ç•¥é”™è¯¯å­—ç¬¦)")
                chunks = {item["hash_code"]: item["text"] for item in corpus}
                return chunks
                
            except json.JSONDecodeError as e:
                print(f"ç¼–ç  {encoding} JSONè§£æå¤±è´¥: {e}")
                continue
            except Exception as e:
                print(f"ç¼–ç  {encoding} å…¶ä»–é”™è¯¯: {e}")
                continue
        
        # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•æ›´æ¿€è¿›çš„æ¸…ç†æ–¹æ³•
        print("å°è¯•æ¿€è¿›çš„å­—ç¬¦æ¸…ç†æ–¹æ³•...")
        try:
            # ä½¿ç”¨latin-1è¯»å–ï¼ˆå®ƒå¯ä»¥è¯»å–ä»»ä½•å­—èŠ‚ï¼‰ï¼Œç„¶åæ¸…ç†
            content = raw_data.decode('latin-1', errors='ignore')
            
            # åªä¿ç•™ASCIIå¯æ‰“å°å­—ç¬¦å’ŒåŸºæœ¬çš„JSONå­—ç¬¦
            import string
            allowed_chars = string.printable + '""''â€”â€“'  # åŒ…å«ä¸€äº›å¸¸è§çš„Unicodeå¼•å·å’Œç ´æŠ˜å·
            cleaned_content = ''.join(char for char in content if char in allowed_chars)
            
            # ä¿®å¤å¯èƒ½çš„JSONæ ¼å¼é—®é¢˜
            cleaned_content = re.sub(r'[\x00-\x1F\x7F]', '', cleaned_content)  # ç§»é™¤æ§åˆ¶å­—ç¬¦
            cleaned_content = re.sub(r'([^\\])"([^"]*?)"', r'\1"\2"', cleaned_content)  # ä¿®å¤å¼•å·
            
            corpus = json.loads(cleaned_content)
            print("é€šè¿‡æ¿€è¿›æ¸…ç†æ–¹æ³•æˆåŠŸè§£æJSON")
            chunks = {item["hash_code"]: item["text"] for item in corpus}
            return chunks
            
        except Exception as e:
            print(f"æ¿€è¿›æ¸…ç†æ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
    
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    raise ValueError(f"æ— æ³•è¯»å–æ–‡ä»¶ {chunk_file}ï¼Œå°è¯•äº†æ‰€æœ‰å¯èƒ½çš„æ–¹æ³•éƒ½å¤±è´¥")

def truncate_text_by_tokens(text, max_tokens, tokenizer=None, reserve_ratio=0.8):
    """
    æ ¹æ® token æ•°é‡æˆªæ–­æ–‡æœ¬ï¼Œé¿å…è¶…å‡º max_model_len
    
    å‚æ•°:
        text: è¦æˆªæ–­çš„æ–‡æœ¬
        max_tokens: æœ€å¤§ token æ•°é‡
        tokenizer: tiktoken tokenizerï¼ˆå¦‚æœå¯ç”¨ï¼‰
        reserve_ratio: ä¿ç•™æ¯”ä¾‹ï¼ˆ0.8 è¡¨ç¤ºä¿ç•™ 80% çš„ token ç©ºé—´ç»™æç¤ºè¯å’Œè¾“å‡ºï¼‰
    
    è¿”å›:
        (truncated_text, was_truncated, original_tokens, truncated_tokens)
    """
    if not text:
        return text, False, 0, 0
    
    # è®¡ç®—å®é™…å¯ç”¨çš„ token æ•°é‡ï¼ˆä¿ç•™ä¸€éƒ¨åˆ†ç»™æç¤ºè¯å’Œè¾“å‡ºï¼‰
    available_tokens = int(max_tokens * reserve_ratio)
    
    if TIKTOKEN_AVAILABLE and tokenizer is None:
        try:
            # ä½¿ç”¨ cl100k_baseï¼ˆGPT-4/DeepSeek ç­‰æ¨¡å‹å¸¸ç”¨ï¼‰
            tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception:
            tokenizer = None
    
    if tokenizer:
        # ä½¿ç”¨ tiktoken ç²¾ç¡®è®¡ç®—
        tokens = tokenizer.encode(text)
        original_token_count = len(tokens)
        
        if original_token_count <= available_tokens:
            return text, False, original_token_count, original_token_count
        
        # æˆªæ–­åˆ°å¯ç”¨ token æ•°é‡
        truncated_tokens = tokens[:available_tokens]
        truncated_text = tokenizer.decode(truncated_tokens)
        return truncated_text, True, original_token_count, available_tokens
    else:
        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—ï¼ˆä¸­æ–‡çº¦ 1.5 å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦ 4 å­—ç¬¦/tokenï¼‰
        # ä¿å®ˆä¼°è®¡ï¼šæŒ‰ 2 å­—ç¬¦/token è®¡ç®—
        estimated_tokens = len(text) // 2
        if estimated_tokens <= available_tokens:
            return text, False, estimated_tokens, estimated_tokens
        
        # æˆªæ–­æ–‡æœ¬
        max_chars = available_tokens * 2
        truncated_text = text[:max_chars]
        return truncated_text, True, estimated_tokens, available_tokens


async def triple_extraction(chunks,use_llm_func,output_dir,append_mode=True,max_model_len=3072,tokenizer=None):
    
    # extract entities
    # use_llm_func is wrapped in ascynio.Semaphore, limiting max_async callings
    
    # æ€§èƒ½ç›‘æ§ï¼šè®°å½•å¼€å§‹æ—¶é—´
    total_start_time = time.time()
    
    already_processed = 0
    already_entities = 0
    already_relations = 0
    ordered_chunks = list(chunks.items())
    print(f"\nå¼€å§‹å¤„ç† {len(ordered_chunks)} ä¸ªæ–‡æœ¬å—...")
    async def _process_single_content_entity(chunk_key_dp,use_llm_func):           # for each chunk, run the func
        nonlocal already_processed, already_entities, already_relations
        chunk_key = chunk_key_dp[0]
        content = chunk_key_dp[1]
        entity_extract_prompt = PROMPTS["entity_extraction"]        # give 3 examples in the prompt context
        relation_extract_prompt = PROMPTS["relation_extraction"]
        continue_prompt = PROMPTS["entiti_continue_extraction"]     # means low quality in the last extraction
        if_loop_prompt = PROMPTS["entiti_if_loop_extraction"] 
        context_base_entity = dict(
        tuple_delimiter=PROMPTS["DEFAULT_TUPLE_DELIMITER"],
        record_delimiter=PROMPTS["DEFAULT_RECORD_DELIMITER"],
        completion_delimiter=PROMPTS["DEFAULT_COMPLETION_DELIMITER"],
        entity_types=",".join(PROMPTS["META_ENTITY_TYPES"])
    )
        entity_extract_max_gleaning=1
        hint_prompt = entity_extract_prompt.format(**context_base_entity, input_text=content)      # fill in the parameter
        final_result = await use_llm_func(hint_prompt)                                      # feed into LLM with the prompt

        history = pack_user_ass_to_openai_messages(hint_prompt, final_result)               # set as history
        for now_glean_index in range(entity_extract_max_gleaning):
            glean_result = await use_llm_func(continue_prompt, history_messages=history)

            history += pack_user_ass_to_openai_messages(continue_prompt, glean_result)      # add to history
            final_result += glean_result
            if now_glean_index == entity_extract_max_gleaning - 1:
                break

            if_loop_result: str = await use_llm_func(                                       # judge if we still need the next iteration
                if_loop_prompt, history_messages=history
            )
            if_loop_result = if_loop_result.strip().strip('"').strip("'").lower()
            if if_loop_result != "yes":
                break

        records = split_string_by_multi_markers(                                            # split entities from result --> list of entities
            final_result,
            [context_base_entity["record_delimiter"], context_base_entity["completion_delimiter"]],
        )
        # resolve the entities
        maybe_nodes = defaultdict(list)
        maybe_edges = defaultdict(list)
        for record in records:
            record = re.search(r"\((.*)\)", record)
            if record is None:
                continue
            record = record.group(1)
            record_attributes = split_string_by_multi_markers(          # split entity
                record, [context_base_entity["tuple_delimiter"]]
            )
            if_entities = await _handle_single_entity_extraction(       # get the name, type, desc, source_id of entity--> dict
                record_attributes, chunk_key
            )
            if if_entities is not None:
                maybe_nodes[if_entities["entity_name"]].append(if_entities)
                continue

            if_relation = await _handle_single_relationship_extraction(
                record_attributes, chunk_key
            )
            if if_relation is not None:
                maybe_edges[(if_relation["src_id"], if_relation["tgt_id"])].append(
                    if_relation
                )
        already_processed += 1                                      # already processed chunks
        already_entities += len(maybe_nodes)
        already_relations += len(maybe_edges)
        now_ticks = PROMPTS["process_tickers"][                     # for visualization
            already_processed % len(PROMPTS["process_tickers"])
        ]
        print(
            f"{now_ticks} Processed {already_processed}({already_processed*100//len(ordered_chunks)}%) chunks,  {already_entities} entities(duplicated), {already_relations} relations(duplicated)\r",
            end="",
            flush=True,
        )
        return dict(maybe_nodes), dict(maybe_edges)
    
    # å®ä½“æå–é˜¶æ®µ
    entity_start_time = time.time()
    entity_results = await asyncio.gather(
        *[_process_single_content_entity(c,use_llm_func) for c in ordered_chunks]
    )
    entity_end_time = time.time()
    entity_duration = entity_end_time - entity_start_time
    print()  # clear the progress bar
    print(f"âœ… å®ä½“æå–å®Œæˆï¼Œè€—æ—¶: {entity_duration:.2f} ç§’ ({entity_duration/60:.2f} åˆ†é’Ÿ)")
    print(f"   å¹³å‡æ¯ä¸ªå—: {entity_duration/len(ordered_chunks):.2f} ç§’")

    # fetch all entities from results
    all_entities = {}
    for item in entity_results:
        for k, v in item[0].items():
            value = v[0]
            all_entities[k] = v[0]
    context_entities = {key[0]: list(x[0].keys()) for key, x in zip(ordered_chunks, entity_results)}
    already_processed = 0
    async def _process_single_content_relation(chunk_key_dp,use_llm_func):           # for each chunk, run the func
        nonlocal already_processed, already_entities, already_relations
        chunk_key = chunk_key_dp[0]
        content = chunk_key_dp[1]
        
        # æ£€æŸ¥å¹¶æˆªæ–­è¾“å…¥æ–‡æœ¬ï¼Œé¿å…è¶…å‡º max_model_len
        truncated_content, was_truncated, orig_tokens, trunc_tokens = truncate_text_by_tokens(
            content, max_model_len, tokenizer
        )
        if was_truncated:
            print(f"âš ï¸  æ–‡æœ¬å— {chunk_key[:16]}... è¶…å‡ºé•¿åº¦é™åˆ¶ï¼Œå·²æˆªæ–­: {orig_tokens} -> {trunc_tokens} tokens")
            content = truncated_content
        entity_extract_prompt = PROMPTS["entity_extraction"]        # give 3 examples in the prompt context
        relation_extract_prompt = PROMPTS["relation_extraction"]
        continue_prompt = PROMPTS["entiti_continue_extraction"]     # means low quality in the last extraction
        if_loop_prompt = PROMPTS["entiti_if_loop_extraction"] 
        entities = context_entities[chunk_key]
        context_base_relation = dict(
            tuple_delimiter=PROMPTS["DEFAULT_TUPLE_DELIMITER"],
            record_delimiter=PROMPTS["DEFAULT_RECORD_DELIMITER"],
            completion_delimiter=PROMPTS["DEFAULT_COMPLETION_DELIMITER"],
            entities=",".join(entities)
            )
        entity_extract_max_gleaning=1
        hint_prompt = relation_extract_prompt.format(**context_base_relation, input_text=content)      # fill in the parameter
        final_result = await use_llm_func(hint_prompt)                                      # feed into LLM with the prompt

        history = pack_user_ass_to_openai_messages(hint_prompt, final_result)               # set as history
        for now_glean_index in range(entity_extract_max_gleaning):
            glean_result = await use_llm_func(continue_prompt, history_messages=history)

            history += pack_user_ass_to_openai_messages(continue_prompt, glean_result)      # add to history
            final_result += glean_result
            if now_glean_index == entity_extract_max_gleaning - 1:
                break

            if_loop_result: str = await use_llm_func(                                       # judge if we still need the next iteration
                if_loop_prompt, history_messages=history
            )
            if_loop_result = if_loop_result.strip().strip('"').strip("'").lower()
            if if_loop_result != "yes":
                break

        records = split_string_by_multi_markers(                                            # split entities from result --> list of entities
            final_result,
            [context_base_relation["record_delimiter"], context_base_relation["completion_delimiter"]],
        )
        # resolve the entities
        maybe_nodes = defaultdict(list)
        maybe_edges = defaultdict(list)
        for record in records:
            record = re.search(r"\((.*)\)", record)
            if record is None:
                continue
            record = record.group(1)
            record_attributes = split_string_by_multi_markers(          # split entity
                record, [context_base_relation["tuple_delimiter"]]
            )
            if_entities = await _handle_single_entity_extraction(       # get the name, type, desc, source_id of entity--> dict
                record_attributes, chunk_key
            )
            if if_entities is not None:
                maybe_nodes[if_entities["entity_name"]].append(if_entities)
                continue

            if_relation = await _handle_single_relationship_extraction(
                record_attributes, chunk_key
            )
            if if_relation is not None:
                maybe_edges[(if_relation["src_id"], if_relation["tgt_id"])].append(
                    if_relation
                )
        already_processed += 1                                      # already processed chunks
        already_entities += len(maybe_nodes)
        already_relations += len(maybe_edges)
        now_ticks = PROMPTS["process_tickers"][                     # for visualization
            already_processed % len(PROMPTS["process_tickers"])
        ]
        print(
            f"{now_ticks} Processed {already_processed}({already_processed*100//len(ordered_chunks)}%) chunks,  {already_entities} entities(duplicated), {already_relations} relations(duplicated)\r",
            end="",
            flush=True,
        )
        return dict(maybe_nodes), dict(maybe_edges)
    # å…³ç³»æå–é˜¶æ®µ
    relation_start_time = time.time()
    relation_results = await asyncio.gather(
        *[_process_single_content_relation(c,use_llm_func) for c in ordered_chunks]
    )
    relation_end_time = time.time()
    relation_duration = relation_end_time - relation_start_time
    print()
    print(f"âœ… å…³ç³»æå–å®Œæˆï¼Œè€—æ—¶: {relation_duration:.2f} ç§’ ({relation_duration/60:.2f} åˆ†é’Ÿ)")
    print(f"   å¹³å‡æ¯ä¸ªå—: {relation_duration/len(ordered_chunks):.2f} ç§’")
    
    all_relations = {}
    for item in relation_results:
        for k, v in item[1].items():
            all_relations[k] = v
    save_entity=[]
    save_relation=[]
    for k,v in copy.deepcopy(all_entities).items():
    #     del v['embedding']
        save_entity.append(v)
    for k,v in copy.deepcopy(all_relations).items():
        # v æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œéœ€è¦å±•å¹³
        if isinstance(v, list):
            save_relation.extend(v)
        else:
            save_relation.append(v)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜ç»“æœ
    save_start_time = time.time()
    
    # æ–‡ä»¶è·¯å¾„
    entity_file = f"{output_dir}/entity.jsonl"
    relation_file = f"{output_dir}/relation.jsonl"
    
    # æ ¹æ®æ¨¡å¼å¤„ç†æ•°æ®
    existing_entities = {}
    existing_relations = set()
    
    if append_mode:
        # è¿½åŠ æ¨¡å¼ï¼šè¯»å–å·²å­˜åœ¨çš„æ•°æ®ç”¨äºå»é‡
        # è¯»å–å·²å­˜åœ¨çš„å®ä½“æ–‡ä»¶
        if os.path.exists(entity_file):
            try:
                from tools.utils import read_jsonl
                existing_entity_list = read_jsonl(entity_file) or []
                for entity in existing_entity_list:
                    entity_name = entity.get('entity_name', '')
                    if entity_name:
                        existing_entities[entity_name] = entity
                print(f"ğŸ“‚ å·²è¯»å– {len(existing_entities)} ä¸ªå·²å­˜åœ¨çš„å®ä½“")
            except Exception as e:
                print(f"âš ï¸  è¯»å–å·²å­˜åœ¨å®ä½“æ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œå°†è§†ä¸ºç©ºæ–‡ä»¶")
                existing_entities = {}
        
        # è¯»å–å·²å­˜åœ¨çš„å…³ç³»æ–‡ä»¶
        if os.path.exists(relation_file):
            try:
                from tools.utils import read_jsonl
                existing_relation_list = read_jsonl(relation_file) or []
                for rel in existing_relation_list:
                    # å¤„ç†å¯èƒ½å­˜åœ¨çš„åˆ—è¡¨æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                    if isinstance(rel, list):
                        rel = rel[0] if rel else {}
                    
                    # æ ¹æ®æ•°æ®æ ¼å¼é€‰æ‹©æ­£ç¡®çš„å­—æ®µå
                    if 'src_id' in rel:
                        src_id = str(rel.get('src_id', '')).replace('"', '')
                        tgt_id = str(rel.get('tgt_id', '')).replace('"', '')
                    else:
                        src_id = str(rel.get('src_tgt', '')).replace('"', '')
                        tgt_id = str(rel.get('tgt_src', '')).replace('"', '')
                    
                    if src_id and tgt_id:
                        existing_relations.add((src_id, tgt_id))
                print(f"ğŸ“‚ å·²è¯»å– {len(existing_relations)} ä¸ªå·²å­˜åœ¨çš„å…³ç³»")
            except Exception as e:
                print(f"âš ï¸  è¯»å–å·²å­˜åœ¨å…³ç³»æ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œå°†è§†ä¸ºç©ºæ–‡ä»¶")
                existing_relations = set()
        
        # è¿½åŠ æ¨¡å¼ï¼šè¿‡æ»¤æ–°æ•°æ®ï¼Œåªä¿ç•™ä¸é‡å¤çš„
        # è¿½åŠ æ¨¡å¼ï¼šè¿‡æ»¤æ–°æ•°æ®ï¼Œåªä¿ç•™ä¸é‡å¤çš„
        new_entities = []
        duplicate_entity_count = 0
        for entity in save_entity:
            entity_name = entity.get('entity_name', '')
            if entity_name and entity_name not in existing_entities:
                new_entities.append(entity)
                existing_entities[entity_name] = entity  # æ·»åŠ åˆ°å·²å­˜åœ¨é›†åˆä¸­ï¼Œé¿å…åŒæ‰¹æ¬¡é‡å¤
            else:
                duplicate_entity_count += 1
        
        new_relations = []
        duplicate_relation_count = 0
        for rel in save_relation:
            # å¤„ç†å¯èƒ½å­˜åœ¨çš„åˆ—è¡¨æ ¼å¼
            if isinstance(rel, list):
                rel = rel[0] if rel else {}
            
            # æ ¹æ®æ•°æ®æ ¼å¼é€‰æ‹©æ­£ç¡®çš„å­—æ®µå
            if 'src_id' in rel:
                src_id = str(rel.get('src_id', '')).replace('"', '')
                tgt_id = str(rel.get('tgt_id', '')).replace('"', '')
            else:
                src_id = str(rel.get('src_tgt', '')).replace('"', '')
                tgt_id = str(rel.get('tgt_src', '')).replace('"', '')
            
            relation_key = (src_id, tgt_id)
            if src_id and tgt_id and relation_key not in existing_relations:
                new_relations.append(rel)
                existing_relations.add(relation_key)  # æ·»åŠ åˆ°å·²å­˜åœ¨é›†åˆä¸­ï¼Œé¿å…åŒæ‰¹æ¬¡é‡å¤
            else:
                duplicate_relation_count += 1
        
        # æ˜¾ç¤ºå»é‡ç»Ÿè®¡
        print(f"\nğŸ“Š å»é‡ç»Ÿè®¡:")
        print(f"   å®ä½“: æ–°å¢ {len(new_entities)} æ¡ï¼Œè·³è¿‡é‡å¤ {duplicate_entity_count} æ¡")
        print(f"   å…³ç³»: æ–°å¢ {len(new_relations)} æ¡ï¼Œè·³è¿‡é‡å¤ {duplicate_relation_count} æ¡")
    else:
        # è¦†ç›–æ¨¡å¼ï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®
        new_entities = save_entity
        new_relations = save_relation
        print(f"\nğŸ“Š è¦†ç›–æ¨¡å¼: å°†å†™å…¥ {len(new_entities)} ä¸ªå®ä½“ï¼Œ{len(new_relations)} ä¸ªå…³ç³»")
    
    # æ ¹æ®æ¨¡å¼å†™å…¥æ–‡ä»¶
    write_mode = "a" if append_mode else "w"
    action_desc = "è¿½åŠ " if append_mode else "å†™å…¥"
    
    try:
        if new_entities:
            write_jsonl(new_entities, entity_file, mode=write_mode)
            print(f"âœ… å·²{action_desc}å®ä½“åˆ°æ–‡ä»¶: {entity_file} ({len(new_entities)} æ¡)")
        else:
            print(f"â„¹ï¸  æ²¡æœ‰æ–°å®ä½“éœ€è¦{action_desc}")
        
        # æ˜¾ç¤ºæ€»å®ä½“æ•°ï¼ˆè¿½åŠ æ¨¡å¼æ‰ç»Ÿè®¡æ€»æ•°ï¼‰
        if append_mode:
            # æ€»å®ä½“æ•° = å·²å­˜åœ¨çš„ + æ–°å¢çš„ï¼ˆå»é‡åï¼‰
            total_entities = len(existing_entities) + len(new_entities)
            print(f"   æ–‡ä»¶ä¸­å…±æœ‰å®ä½“: {total_entities} æ¡ï¼ˆåŸæœ‰ {len(existing_entities)} æ¡ + æ–°å¢ {len(new_entities)} æ¡ï¼‰")
    except PermissionError as e:
        print(f"âŒ å†™å…¥å®ä½“æ–‡ä»¶æ—¶æƒé™é”™è¯¯: {e}")
        print(f"   æ–‡ä»¶è·¯å¾„: {entity_file}")
        print(f"   è¯·æ£€æŸ¥ï¼š")
        print(f"   1. æ–‡ä»¶æ˜¯å¦è¢«å…¶ä»–ç¨‹åºå ç”¨ï¼ˆå¦‚ç¼–è¾‘å™¨ï¼‰")
        print(f"   2. ç›®å½•æ˜¯å¦æœ‰å†™æƒé™")
        print(f"   3. ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³")
        raise
    except Exception as e:
        print(f"âŒ å†™å…¥å®ä½“æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        raise
    
    try:
        if new_relations:
            write_jsonl(new_relations, relation_file, mode=write_mode)
            print(f"âœ… å·²{action_desc}å…³ç³»åˆ°æ–‡ä»¶: {relation_file} ({len(new_relations)} æ¡)")
        else:
            print(f"â„¹ï¸  æ²¡æœ‰æ–°å…³ç³»éœ€è¦{action_desc}")
        
        # æ˜¾ç¤ºæ€»å…³ç³»æ•°ï¼ˆè¿½åŠ æ¨¡å¼æ‰ç»Ÿè®¡æ€»æ•°ï¼‰
        if append_mode:
            # æ€»å…³ç³»æ•° = å·²å­˜åœ¨çš„ + æ–°å¢çš„ï¼ˆå»é‡åï¼‰
            total_relations = len(existing_relations) + len(new_relations)
            print(f"   æ–‡ä»¶ä¸­å…±æœ‰å…³ç³»: {total_relations} æ¡ï¼ˆåŸæœ‰ {len(existing_relations)} æ¡ + æ–°å¢ {len(new_relations)} æ¡ï¼‰")
    except PermissionError as e:
        print(f"âŒ å†™å…¥å…³ç³»æ–‡ä»¶æ—¶æƒé™é”™è¯¯: {e}")
        print(f"   æ–‡ä»¶è·¯å¾„: {relation_file}")
        print(f"   è¯·æ£€æŸ¥ï¼š")
        print(f"   1. æ–‡ä»¶æ˜¯å¦è¢«å…¶ä»–ç¨‹åºå ç”¨ï¼ˆå¦‚ç¼–è¾‘å™¨ï¼‰")
        print(f"   2. ç›®å½•æ˜¯å¦æœ‰å†™æƒé™")
        print(f"   3. ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³")
        raise
    except Exception as e:
        print(f"âŒ å†™å…¥å…³ç³»æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        raise
    
    save_duration = time.time() - save_start_time
    
    # æ€§èƒ½ç»Ÿè®¡
    total_duration = time.time() - total_start_time
    print(f"\n{'='*60}")
    print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"æ€»è€—æ—¶: {total_duration:.2f} ç§’ ({total_duration/60:.2f} åˆ†é’Ÿ)")
    print(f"  - å®ä½“æå–: {entity_duration:.2f} ç§’ ({entity_duration/total_duration*100:.1f}%)")
    print(f"  - å…³ç³»æå–: {relation_duration:.2f} ç§’ ({relation_duration/total_duration*100:.1f}%)")
    print(f"  - ä¿å­˜æ–‡ä»¶: {save_duration:.2f} ç§’ ({save_duration/total_duration*100:.1f}%)")
    print(f"å¹³å‡æ¯ä¸ªå—æ€»è€—æ—¶: {total_duration/len(ordered_chunks):.2f} ç§’")
    print(f"å¤„ç†é€Ÿåº¦: {len(ordered_chunks)/total_duration*60:.2f} ä¸ªå—/åˆ†é’Ÿ")
    print(f"{'='*60}\n")
            
    
    
    
    
    
if __name__ == "__main__":
    MODEL = "deepseek-r1-32b:latest"
    num=5
    instanceManager=InstanceManager(
        url="http://10.61.2.49",
        ports=[11434 for i in range(num)],
        gpus=[i for i in range(num)],
        generate_model=MODEL,
        startup_delay=30
    )
    use_llm=instanceManager.generate_text_asy
    chunk_file="/newdataf/SJ/LeanRAG/datasets/mix/mix_chunk.json"
    chunks=get_chunk(chunk_file)
    output_dir="ttt"
    loop = asyncio.get_event_loop()
    loop.run_until_complete(triple_extraction(chunks, use_llm,output_dir))



    