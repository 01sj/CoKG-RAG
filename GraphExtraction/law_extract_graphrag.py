#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤„ç†jsonï¼Œä½¿ç”¨ vLLM ç›´æ¥åŠ è½½æœ¬åœ°æ¨¡å‹è¿›è¡Œå®ä½“å’Œå…³ç³»æå–

ä¿®æ”¹å†å²ï¼š
- 2024-12-12: ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„ /newdatad/WHH/MyEmoHH/models/Qwen2-1.5B-Instruct/
- é…ç½®ï¼šGPU 3ï¼ˆå•å¡ï¼‰ï¼ŒQwen2-1.5B-Instructï¼ˆçº¦ 3-4GB æ˜¾å­˜ï¼‰
- å¤‡æ³¨ï¼š
  - DeepSeek-V2-Lite-Chat: MoE æ¨¡å‹ï¼Œéœ€è¦ 23GB+ æ˜¾å­˜ï¼Œå•å¡æ— æ³•åŠ è½½
  - Qwen2-7B-Instruct: éœ€è¦ 14-16GB æ˜¾å­˜ï¼Œå•å¡ä¼šå‡ºç° KV cache å†…å­˜ä¸è¶³
  - Qwen2-1.5B-Instruct: éœ€è¦ 3-4GB æ˜¾å­˜ï¼Œå•å¡å®Œå…¨å¤Ÿç”¨
"""

import os
import sys
import json
import asyncio
import threading
from pathlib import Path

# âš ï¸ é‡è¦ï¼šå¿…é¡»åœ¨å¯¼å…¥ä»»ä½• CUDA ç›¸å…³åº“ï¼ˆå¦‚ vLLMã€Rayã€torchï¼‰ä¹‹å‰è®¾ç½® CUDA_VISIBLE_DEVICES
# å¦åˆ™ç¯å¢ƒå˜é‡è®¾ç½®æ— æ•ˆï¼Œä¼šä½¿ç”¨é»˜è®¤ GPUï¼ˆé€šå¸¸æ˜¯ GPU 0ï¼‰
# é»˜è®¤é…ç½®ï¼šä½¿ç”¨ GPU 3ï¼ˆç¬¬å››å¼ å¡ï¼‰ï¼Œå¹¶è¡Œæ•°ä¸º 1ï¼ˆå•å¡æ¨¡å¼ï¼‰
# æ³¨æ„ï¼šæ ¹æ®å®é™… GPU ä½¿ç”¨æƒ…å†µé€‰æ‹©ç©ºé—²çš„ GPUï¼ˆé€šè¿‡ nvidia-smi æˆ– nvitop æŸ¥çœ‹ï¼‰
# å¦‚æœç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼Œä¼šå¼ºåˆ¶è¦†ç›–ä¸º GPU 3
default_gpu_ids = os.environ.get('VLLM_GPU_IDS', '3')

# å¼ºåˆ¶è®¾ç½® CUDA_VISIBLE_DEVICESï¼ˆè¦†ç›–ä»»ä½•å·²æœ‰è®¾ç½®ï¼‰
if 'CUDA_VISIBLE_DEVICES' in os.environ:
    old_value = os.environ['CUDA_VISIBLE_DEVICES']
    os.environ['CUDA_VISIBLE_DEVICES'] = default_gpu_ids
    print(f"ğŸ”§ è¦†ç›– CUDA_VISIBLE_DEVICES: {old_value} -> {default_gpu_ids}")
else:
    os.environ['CUDA_VISIBLE_DEVICES'] = default_gpu_ids
    print(f"ğŸ”§ åœ¨å¯¼å…¥ vLLM ä¹‹å‰è®¾ç½® CUDA_VISIBLE_DEVICES={default_gpu_ids}")

print(f"âœ… å½“å‰ CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}ï¼Œå°†ä½¿ç”¨ GPU 3")

# æ·»åŠ é¡¹ç›®è·¯å¾„
# è‡ªåŠ¨æ£€æµ‹è„šæœ¬ä½ç½®å¹¶è°ƒæ•´å¯¼å…¥
current_file = Path(__file__).resolve()
current_dir = current_file.parent

# å¦‚æœåœ¨ GraphExtraction ç›®å½•å†…ï¼Œé¡¹ç›®æ ¹ç›®å½•æ˜¯ä¸Šçº§ç›®å½•
if current_dir.name == "GraphExtraction":
    project_root = current_dir.parent
    sys.path.insert(0, str(project_root))
    from chunk import get_chunk, triple_extraction
else:
    # åœ¨é¡¹ç›®æ ¹ç›®å½•
    project_root = current_dir
    sys.path.insert(0, str(project_root))
    from GraphExtraction.chunk import get_chunk, triple_extraction

# å¯¼å…¥vLLMï¼ˆç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦APIæœåŠ¡ï¼‰
from vllm import LLM, SamplingParams


def setup_vllm_direct(config):
    """
    ç›´æ¥åŠ è½½vLLMæ¨¡å‹ï¼ˆä¸ä½¿ç”¨APIæœåŠ¡ï¼‰

    å‚æ•°:
        config: LLMé…ç½®å­—å…¸
            - model: æ¨¡å‹åç§°æˆ–è·¯å¾„
            - tensor_parallel_size: å¼ é‡å¹¶è¡ŒGPUæ•°é‡ï¼ˆé»˜è®¤2ï¼‰
            - gpu_memory_utilization: æ¯å¼ GPUçš„å†…å­˜åˆ©ç”¨ç‡ï¼ˆé»˜è®¤0.80ï¼‰
            - max_model_len: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤8192ï¼‰
            - temperature: é‡‡æ ·æ¸©åº¦ï¼ˆé»˜è®¤0.2ï¼‰
            - top_p: top_pé‡‡æ ·é˜ˆå€¼ï¼ˆé»˜è®¤0.9ï¼‰
            - max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆé»˜è®¤1024ï¼‰
            - cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨HuggingFaceé»˜è®¤ä½ç½®ï¼‰

    è¿”å›:
        å¼‚æ­¥LLMç”Ÿæˆå‡½æ•°
    """
    print(f"\n{'='*60}")
    print("åŠ è½½ vLLM æ¨¡å‹ï¼ˆç›´æ¥æ¨¡å¼ï¼Œä¸ä½¿ç”¨APIæœåŠ¡ï¼‰")
    print(f"{'='*60}")
    print(f"æ¨¡å‹: {config['model']}")
    print(f"å¼ é‡å¹¶è¡ŒGPUæ•°é‡: {config.get('tensor_parallel_size', 1)}")
    print(f"ä½¿ç”¨çš„GPU: {config.get('gpu_ids', '3')} (ç‰©ç†GPUç¼–å·)")
    # æ˜¾ç¤ºå®é™…é…ç½®çš„å†…å­˜åˆ©ç”¨ç‡ï¼ˆä» config ä¸­è·å–ï¼Œè€Œä¸æ˜¯é»˜è®¤å€¼ï¼‰
    actual_mem_util = config.get('gpu_memory_utilization')
    print(f"æ¯å¼ GPUå†…å­˜åˆ©ç”¨ç‡: {actual_mem_util}")
    actual_max_len = config.get('max_model_len')
    print(f"æœ€å¤§åºåˆ—é•¿åº¦: {actual_max_len}")
    
    # æ˜¾ç¤ºç¼“å­˜ç›®å½•ä¿¡æ¯
    cache_dir = config.get('cache_dir')
    if cache_dir:
        print(f"æ¨¡å‹ç¼“å­˜ç›®å½•: {cache_dir}")
        # è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•
        os.environ["HF_HOME"] = cache_dir
        os.environ["TRANSFORMERS_CACHE"] = cache_dir
        os.environ["HF_DATASETS_CACHE"] = cache_dir
    else:
        # ä½¿ç”¨é»˜è®¤ç¼“å­˜ä½ç½®
        default_cache = os.path.expanduser("~/.cache/huggingface")
        print(f"æ¨¡å‹ç¼“å­˜ç›®å½•: {default_cache} (é»˜è®¤)")
    
    print("=" * 60)
    print("\næ­£åœ¨åŠ è½½æ¨¡å‹...ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰")
    
    # è®¾ç½®PyTorchå†…å­˜åˆ†é…é…ç½®ï¼ˆé¿å…å†…å­˜ç¢ç‰‡ï¼‰
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    
    # è®¾ç½®ä½¿ç”¨çš„GPUï¼ˆä½¿ç”¨å•å¡ï¼šGPU 3ï¼Œç¬¬å››å¼ å¡ï¼‰
    # æ³¨æ„ï¼šCUDA_VISIBLE_DEVICES å·²ç»åœ¨æ–‡ä»¶å¼€å¤´è®¾ç½®ï¼ˆåœ¨å¯¼å…¥ vLLM ä¹‹å‰ï¼‰
    # è¿™é‡Œåªæ˜¯æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„ GPU é…ç½®
    # é‡è¦ï¼šç¡®ä¿é€‰æ‹©çš„ GPU æ˜¯ç©ºé—²çš„ï¼ˆé€šè¿‡ nvidia-smi æˆ– nvitop æŸ¥çœ‹ï¼‰
    gpu_ids = config.get('gpu_ids', '3')  # é»˜è®¤ä½¿ç”¨ GPU 3ï¼ˆç¬¬å››å¼ å¡ï¼Œå•å¡æ¨¡å¼ï¼‰
    tensor_parallel_size = config.get('tensor_parallel_size', 1)  # å•å¡æ¨¡å¼ï¼Œå¹¶è¡Œæ•°ä¸º 1
    
    # æ£€æŸ¥ CUDA_VISIBLE_DEVICES æ˜¯å¦ä¸é…ç½®åŒ¹é…
    current_cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if current_cuda_devices:
        # è§£æå½“å‰è®¾ç½®çš„ GPU æ•°é‡
        current_gpu_count = len(current_cuda_devices.split(','))
        if current_gpu_count != tensor_parallel_size:
            print(f"é”™è¯¯: CUDA_VISIBLE_DEVICES={current_cuda_devices} åªæä¾› {current_gpu_count} ä¸ª GPU")
            print(f"   ä½†é…ç½®è¦æ±‚ tensor_parallel_size={tensor_parallel_size} ä¸ª GPU")
            print(f"   è¯·å–æ¶ˆè®¾ç½®ç¯å¢ƒå˜é‡æˆ–ä¿®æ”¹é…ç½®ä»¥åŒ¹é…")
            print(f"   å–æ¶ˆæ–¹æ³•: unset CUDA_VISIBLE_DEVICES")
            raise ValueError(
                f"GPU æ•°é‡ä¸åŒ¹é…: CUDA_VISIBLE_DEVICES æä¾› {current_gpu_count} ä¸ª GPUï¼Œ"
                f"ä½† tensor_parallel_size éœ€è¦ {tensor_parallel_size} ä¸ª GPU"
            )
        print(f"å½“å‰ä½¿ç”¨çš„ GPU: {current_cuda_devices} (ç‰©ç† GPU ç¼–å·ï¼ŒvLLM ä¼šå°†å…¶è§†ä¸º GPU 0-{current_gpu_count-1})")
    else:
        print(f"ä½¿ç”¨é…ç½®çš„ GPU: {gpu_ids} (ç‰©ç† GPU ç¼–å·)")
    
    # åœ¨ä½¿ç”¨å¤š GPU æ—¶ï¼ŒvLLM ä¼šä½¿ç”¨ Ray æ¥åè°ƒï¼Œéœ€è¦æ­£ç¡®åˆå§‹åŒ– Ray
    if tensor_parallel_size > 1:
        try:
            import ray
            # æ£€æŸ¥ Ray æ˜¯å¦å·²ç»åˆå§‹åŒ–
            if not ray.is_initialized():
                print("ğŸ”§ åˆå§‹åŒ– Ray é›†ç¾¤ï¼ˆç”¨äºå¤š GPU å¼ é‡å¹¶è¡Œï¼‰...")
                # æ˜¾å¼åˆå§‹åŒ– Rayï¼ŒæŒ‡å®š GPU æ•°é‡
                # æ³¨æ„ï¼šRay ä¼šä½¿ç”¨ CUDA_VISIBLE_DEVICES ä¸­æŒ‡å®šçš„ GPU
                ray.init(
                    ignore_reinit_error=True,
                    num_gpus=tensor_parallel_size,
                    num_cpus=tensor_parallel_size,  # æ¯ä¸ª GPU åˆ†é… 1 ä¸ª CPU
                    object_store_memory=2 * 10**9,  # 2GB å¯¹è±¡å­˜å‚¨
                    _temp_dir="/tmp/ray",  # æŒ‡å®šä¸´æ—¶ç›®å½•
                )
                print("âœ… Ray é›†ç¾¤åˆå§‹åŒ–å®Œæˆ")
            else:
                print("âœ… Ray é›†ç¾¤å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âš ï¸  Ray åˆå§‹åŒ–è­¦å‘Š: {e}")
            print("   å°†å°è¯•ç»§ç»­è¿è¡Œï¼ŒvLLM å¯èƒ½ä¼šè‡ªåŠ¨åˆå§‹åŒ– Ray")
    
    # åŠ è½½vLLMæ¨¡å‹ï¼ˆå¸¦è‡ªåŠ¨é‡è¯•æœºåˆ¶ï¼Œå¦‚æœ KV cache å†…å­˜ä¸è¶³ä¼šè‡ªåŠ¨é™ä½ max_model_lenï¼‰
    max_retries = 2
    retry_count = 0
    llm = None
    original_max_len = config.get('max_model_len', 8192)
    
    while retry_count <= max_retries and llm is None:
        try:
            current_max_len = original_max_len
            if retry_count > 0:
                # æ¯æ¬¡é‡è¯•æ—¶é™ä½ max_model_lenï¼ˆæ¯æ¬¡é™ä½åˆ°åŸæ¥çš„ 70%ï¼‰
                current_max_len = int(original_max_len * (0.7 ** retry_count))
                print(f"ğŸ”„ é‡è¯• {retry_count}/{max_retries}: é™ä½ max_model_len åˆ° {current_max_len} (åŸå§‹: {original_max_len})")
            
            llm_kwargs = {
                'model': config['model'],
                'tensor_parallel_size': config.get('tensor_parallel_size', 2),
                'gpu_memory_utilization': config.get('gpu_memory_utilization', 0.80),
                'max_model_len': current_max_len,
                'trust_remote_code': True,
                'dtype': config.get('dtype', 'auto'),
            }
            
            # å¦‚æœæŒ‡å®šäº†ç¼“å­˜ç›®å½•ï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
            if config.get('cache_dir'):
                llm_kwargs['download_dir'] = config['cache_dir']
            
            llm = LLM(**llm_kwargs)
            
            if retry_count > 0:
                print(f"âœ… ä½¿ç”¨é™ä½åçš„ max_model_len={current_max_len} æˆåŠŸåŠ è½½æ¨¡å‹")
        except (ValueError, RuntimeError) as e:
            error_msg = str(e)
            if "No available memory for the cache blocks" in error_msg or "cache blocks" in error_msg.lower():
                retry_count += 1
                if retry_count <= max_retries:
                    print(f"âš ï¸  KV cache å†…å­˜ä¸è¶³ï¼Œå°†å°è¯•é™ä½ max_model_len åé‡è¯•...")
                    continue
                else:
                    print(f"âŒ ç»è¿‡ {max_retries} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥")
                    print(f"   å½“å‰é…ç½®: gpu_memory_utilization={config.get('gpu_memory_utilization', 0.80)}, max_model_len={current_max_len}")
                    print(f"   å»ºè®®ï¼š")
                    print(f"   1. æé«˜ gpu_memory_utilizationï¼ˆå¦‚ 0.85-0.90ï¼‰")
                    print(f"   2. æ‰‹åŠ¨é™ä½ max_model_lenï¼ˆå¦‚ 1536 æˆ– 1024ï¼‰")
                    print(f"   3. æ£€æŸ¥ GPU æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨ï¼ˆä½¿ç”¨ nvidia-smiï¼‰")
                    raise
            else:
                # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                raise
    
    if llm is None:
        raise RuntimeError("æ— æ³•åŠ è½½ vLLM æ¨¡å‹")
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=config.get('temperature', 0.2),
        top_p=config.get('top_p', 0.9),
        max_tokens=config.get('max_tokens', 1024),
    )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("=" * 60)
    print("")
    
    # åˆ›å»ºçº¿ç¨‹é”å’Œä¿¡å·é‡ï¼Œç¡®ä¿ vLLM è°ƒç”¨çš„çº¿ç¨‹å®‰å…¨
    # vLLM çš„ generate æ–¹æ³•ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œéœ€è¦åŠ é”ä¿æŠ¤
    # åŒæ—¶ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…è¿‡å¤šå¹¶å‘è¯·æ±‚å¯¼è‡´å†…éƒ¨çº¿ç¨‹é€šä¿¡é—®é¢˜
    vllm_lock = threading.Lock()
    # é™åˆ¶å¹¶å‘æ•°é‡ï¼Œé»˜è®¤4ï¼ˆå¯æ ¹æ®GPUæ•°é‡å’Œå†…å­˜è°ƒæ•´ï¼‰
    # å¦‚æœé‡åˆ°OOMé”™è¯¯ï¼Œå¯ä»¥é™ä½åˆ°2ï¼›å¦‚æœGPUåˆ©ç”¨ç‡ä¸é«˜ï¼Œå¯ä»¥æé«˜åˆ°6-8
    # å»ºè®®å€¼ï¼štensor_parallel_size=2 æ—¶ï¼Œå¹¶å‘æ•°è®¾ä¸º 2-4 è¾ƒåˆé€‚
    max_concurrent = int(os.environ.get('VLLM_MAX_CONCURRENT', '2'))
    vllm_semaphore = asyncio.Semaphore(max_concurrent)
    print(f"âœ… è®¾ç½® vLLM å¹¶å‘æ•°: {max_concurrent} (å¯é€šè¿‡ç¯å¢ƒå˜é‡ VLLM_MAX_CONCURRENT ä¿®æ”¹)")
    
    # åˆ›å»ºå¼‚æ­¥åŒ…è£…å‡½æ•°
    async def async_generate_text(prompt, system_prompt="", history_messages=None, **kwargs):
        """
        å¼‚æ­¥ç”Ÿæˆæ–‡æœ¬å‡½æ•°ï¼ˆåŒ…è£…vLLMçš„åŒæ­¥è°ƒç”¨ï¼‰
        
        å‚æ•°:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            history_messages: å†å²æ¶ˆæ¯åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼: [{"role": "user", "content": "..."}, ...]
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆæš‚æ—¶å¿½ç•¥ï¼Œä½¿ç”¨é»˜è®¤sampling_paramsï¼‰
        """
        # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘
        async with vllm_semaphore:
            # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæœ‰ï¼‰
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            
            # æ·»åŠ å†å²æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if history_messages:
                # history_messages å·²ç»æ˜¯åˆ—è¡¨æ ¼å¼: [{"role": "user", "content": "..."}, ...]
                messages.extend(history_messages)
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æç¤º
            messages.append({"role": "user", "content": prompt})
            
            # å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼ˆé€‚é…vLLMçš„generateæ–¹æ³•ï¼‰
            # å¯¹äºå¯¹è¯æ¨¡å‹ï¼Œéœ€è¦å°†æ¶ˆæ¯æ ¼å¼åŒ–ä¸ºæ¨¡å‹èƒ½ç†è§£çš„æ ¼å¼
            full_prompt = ""
            for msg in messages:
                role = msg.get('role', 'user')
                content = msg.get('content', '')
                if role == 'system':
                    full_prompt += f"System: {content}\n\n"
                elif role == 'user':
                    full_prompt += f"User: {content}\n\n"
                elif role == 'assistant':
                    full_prompt += f"Assistant: {content}\n\n"
            
            # æ·»åŠ æç¤ºè®©æ¨¡å‹ç»§ç»­
            full_prompt += "Assistant: "
            
            # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤ vLLM è°ƒç”¨ï¼Œé¿å…ä¸å†…éƒ¨çº¿ç¨‹å†²çª
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥çš„vLLMè°ƒç”¨ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            # æ·»åŠ é‡è¯•æœºåˆ¶å¤„ç†è¶…æ—¶å’Œå…¶ä»–ä¸´æ—¶é”™è¯¯
            max_retries = 3
            retry_delay = 2.0  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            
            def _generate_with_lock():
                with vllm_lock:
                    return llm.generate([full_prompt], sampling_params)
            
            # åœ¨å¼‚æ­¥å‡½æ•°ä¸­ï¼Œä½¿ç”¨ get_running_loop() è·å–å½“å‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯
            loop = asyncio.get_running_loop()
            
            # é‡è¯•æœºåˆ¶
            last_exception = None
            for attempt in range(max_retries):
                try:
                    outputs = await loop.run_in_executor(None, _generate_with_lock)
                    
                    if not outputs or not outputs[0].outputs:
                        return ""
                    
                    return outputs[0].outputs[0].text
                    
                except Exception as e:
                    last_exception = e
                    error_msg = str(e)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¶…æ—¶é”™è¯¯æˆ–å¼•æ“é”™è¯¯
                    is_timeout_error = "timeout" in error_msg.lower() or "TimeoutError" in str(type(e))
                    is_engine_error = "EngineCore" in error_msg or "EngineDeadError" in str(type(e))
                    
                    if (is_timeout_error or is_engine_error) and attempt < max_retries - 1:
                        wait_time = retry_delay * (attempt + 1)  # æŒ‡æ•°é€€é¿
                        print(f"âš ï¸  vLLM è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_msg[:100]}")
                        print(f"   ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        # æœ€åä¸€æ¬¡å°è¯•æˆ–éè¶…æ—¶é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                        raise
            
            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
            if last_exception:
                raise last_exception
    
    # è·å– tokenizerï¼ˆç”¨äºæ–‡æœ¬æˆªæ–­ï¼‰
    tokenizer = llm.get_tokenizer()
    
    return async_generate_text, tokenizer


def get_chunk_files(input_path):
    """
    è·å–è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    
    å‚æ•°:
        input_path: æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¤¹è·¯å¾„
    
    è¿”å›:
        æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    input_path = Path(input_path).resolve()
    
    if input_path.is_file():
        # å¦‚æœæ˜¯å•ä¸ªæ–‡ä»¶ï¼Œç›´æ¥è¿”å›
        return [str(input_path)]
    elif input_path.is_dir():
        # å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼ŒæŸ¥æ‰¾æ‰€æœ‰ JSON æ–‡ä»¶
        json_files = list(input_path.glob("*.json"))
        if not json_files:
            raise ValueError(f"æ–‡ä»¶å¤¹ {input_path} ä¸­æ²¡æœ‰æ‰¾åˆ° JSON æ–‡ä»¶")
        return sorted([str(f) for f in json_files])
    else:
        raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {input_path}")


def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ GraphRAG æ–¹æ³•æå–å®ä½“å’Œå…³ç³»"""
    
    print("="*70)
    print(" æ³•å¾‹çŸ¥è¯†å›¾è°±å®ä½“æå– - GraphRAG æ–¹æ³•")
    print("="*70)
    
    # ============================================
    # é…ç½®å‚æ•° - æ ¹æ®ä½ çš„ç¯å¢ƒä¿®æ”¹è¿™é‡Œ
    # ============================================
    
    # è¾“å…¥é…ç½® - æ”¯æŒå•ä¸ªæ–‡ä»¶æˆ–æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ï¼‰
    # æ–¹å¼1: æŒ‡å®šå•ä¸ªæ–‡ä»¶
    #   chunk_input = "datasets/law_test_chunk_v2.json"
    #   æˆ–
    #   chunk_input = "/newdataf/SJ/LeanRAG/datasets/law_test_chunk_v2.json"
    # 
    # æ–¹å¼2: æŒ‡å®šæ–‡ä»¶å¤¹ï¼ˆä¼šå¤„ç†æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰ JSON æ–‡ä»¶ï¼Œæ¨èï¼‰
    #   chunk_input = "datasets/chunks"
    #   æˆ–
    #   chunk_input = "/newdataf/SJ/LeanRAG/datasets/chunks"
    
    if current_dir.name == "GraphExtraction":
        # åœ¨ GraphExtraction ç›®å½•å†…ï¼Œè·¯å¾„éœ€è¦å›åˆ°ä¸Šçº§
        chunk_input = "../datasets/chunks"  # ä¿®æ”¹ä¸ºä½ çš„ chunks æ–‡ä»¶å¤¹è·¯å¾„
        output_dir = "../law_kg_output_v2"
    else:
        # åœ¨é¡¹ç›®æ ¹ç›®å½•
        chunk_input = "datasets/chunks"  # ä¿®æ”¹ä¸ºä½ çš„ chunks æ–‡ä»¶å¤¹è·¯å¾„
        output_dir = "law_kg_output_v2"
    
    # ä¹Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šè·¯å¾„ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼Œæ¨èç”¨äºç»å¯¹è·¯å¾„ï¼‰
    # ä¾‹å¦‚: export CHUNK_INPUT_PATH="/newdataf/SJ/LeanRAG/datasets/chunks"
    chunk_input = os.environ.get('CHUNK_INPUT_PATH', chunk_input)
    output_dir = os.environ.get('OUTPUT_DIR', output_dir)
    
    # å¦‚æœä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œå¯ä»¥ç›´æ¥åœ¨è¿™é‡Œè®¾ç½®ï¼ˆå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šå¹¶ä¿®æ”¹è·¯å¾„ï¼‰
    # æ³¨æ„ï¼šè¿™ä¼šè¦†ç›–ç¯å¢ƒå˜é‡çš„è®¾ç½®
    # chunk_input = "/newdataf/SJ/LeanRAG/datasets/chunks"
    output_dir = "/newdataf/SJ/LeanRAG/basicLaw_doc_output"
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•æƒé™ï¼Œå¦‚æœæ— å†™æƒé™åˆ™ä½¿ç”¨å¤‡ç”¨ç›®å½•
    def check_and_get_output_dir(desired_dir):
        """æ£€æŸ¥ç›®å½•æƒé™ï¼Œå¦‚æœæ— å†™æƒé™åˆ™ä½¿ç”¨å¤‡ç”¨ç›®å½•ï¼Œå¹¶ç¡®ä¿ç›®å½•å±äºå½“å‰ç”¨æˆ·"""
        import tempfile
        from pathlib import Path
        import getpass
        import stat
        
        current_user = getpass.getuser()
        desired_path = Path(desired_dir)
        
        try:
            # å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ‰€æœ‰è€…å’Œæƒé™
            if desired_path.exists():
                try:
                    # è·å–ç›®å½•çš„ stat ä¿¡æ¯
                    dir_stat = desired_path.stat()
                    # æ£€æŸ¥ç›®å½•æ‰€æœ‰è€…ï¼ˆåœ¨ Unix ç³»ç»Ÿä¸Šï¼‰
                    if hasattr(dir_stat, 'st_uid'):
                        import pwd
                        try:
                            dir_owner = pwd.getpwuid(dir_stat.st_uid).pw_name
                            if dir_owner != current_user:
                                print(f"âš ï¸  ç›®å½• {desired_dir} çš„æ‰€æœ‰è€…æ˜¯ {dir_owner}ï¼Œå½“å‰ç”¨æˆ·æ˜¯ {current_user}")
                                print(f"   è¿™é€šå¸¸æ˜¯å› ä¸ºä¹‹å‰ä½¿ç”¨ sudo è¿è¡Œè¿‡è„šæœ¬")
                                print(f"   å»ºè®®ä¿®å¤æƒé™ï¼šsudo chown -R {current_user}:{current_user} {desired_dir}")
                        except (KeyError, AttributeError):
                            # åœ¨æŸäº›ç³»ç»Ÿä¸Šå¯èƒ½æ— æ³•è·å–ç”¨æˆ·åï¼Œè·³è¿‡
                            pass
                except Exception as e:
                    # æƒé™æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­å°è¯•
                    pass
            
            # å°è¯•åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰æˆ–æ£€æŸ¥æƒé™
            desired_path.mkdir(parents=True, exist_ok=True)
            
            # å°è¯•åˆ›å»ºä¸€ä¸ªæµ‹è¯•æ–‡ä»¶
            test_file = desired_path / ".write_test"
            try:
                test_file.write_text("test")
                test_file.unlink()
                print(f"âœ… è¾“å‡ºç›®å½•æƒé™æ£€æŸ¥é€šè¿‡: {desired_dir}")
                return str(desired_path)
            except PermissionError:
                print(f"âš ï¸  ç›®å½• {desired_dir} æ²¡æœ‰å†™æƒé™")
                # ä½¿ç”¨å¤‡ç”¨ç›®å½•ï¼ˆå½“å‰ç”¨æˆ·ä¸»ç›®å½•ï¼Œä¿æŒä¸åŸç›®å½•ç›¸åŒçš„åç§°ï¼‰
                fallback_dir = Path.home() / "basicLaw_doc_output"
                fallback_dir.mkdir(parents=True, exist_ok=True)
                print(f"   å°†ä½¿ç”¨å¤‡ç”¨ç›®å½•: {fallback_dir}")
                print(f"   æç¤ºï¼šå¦‚éœ€ä½¿ç”¨åŸç›®å½•ï¼Œè¯·æ‰§è¡Œï¼š")
                print(f"   sudo chown -R {current_user}:{current_user} {desired_dir}  # ä¿®å¤æ‰€æœ‰è€…")
                print(f"   sudo chmod 755 {desired_dir}  # ä¿®å¤æƒé™")
                return str(fallback_dir)
        except PermissionError as e:
            print(f"âš ï¸  æ— æ³•åˆ›å»ºæˆ–è®¿é—®ç›®å½• {desired_dir}: {e}")
            # ä½¿ç”¨å¤‡ç”¨ç›®å½•ï¼ˆå½“å‰ç”¨æˆ·ä¸»ç›®å½•ï¼Œä¿æŒä¸åŸç›®å½•ç›¸åŒçš„åç§°ï¼‰
            fallback_dir = Path.home() / "basicLaw_doc_output"
            fallback_dir.mkdir(parents=True, exist_ok=True)
            print(f"   å°†ä½¿ç”¨å¤‡ç”¨ç›®å½•: {fallback_dir}")
            print(f"   æç¤ºï¼šå¦‚éœ€ä½¿ç”¨åŸç›®å½•ï¼Œè¯·æ‰§è¡Œï¼š")
            print(f"   sudo chown -R {current_user}:{current_user} {desired_dir}  # ä¿®å¤æ‰€æœ‰è€…")
            return str(fallback_dir)
        except Exception as e:
            print(f"âš ï¸  æ£€æŸ¥ç›®å½•æ—¶å‡ºé”™: {e}")
            # ä½¿ç”¨å¤‡ç”¨ç›®å½•ï¼ˆå½“å‰ç”¨æˆ·ä¸»ç›®å½•ï¼Œä¿æŒä¸åŸç›®å½•ç›¸åŒçš„åç§°ï¼‰
            fallback_dir = Path.home() / "basicLaw_doc_output"
            fallback_dir.mkdir(parents=True, exist_ok=True)
            print(f"   å°†ä½¿ç”¨å¤‡ç”¨ç›®å½•: {fallback_dir}")
            return str(fallback_dir)
    
    output_dir = check_and_get_output_dir(output_dir)
    
    # LLM é…ç½®ï¼ˆç›´æ¥ä½¿ç”¨vLLMï¼Œä¸éœ€è¦å¯åŠ¨APIæœåŠ¡ï¼‰
    # ä¼˜åŠ¿ï¼š
    # 1. æ— éœ€å¯åŠ¨APIæœåŠ¡ï¼Œç›´æ¥åœ¨ä»£ç ä¸­ä½¿ç”¨
    # 2. æ›´é«˜æ•ˆï¼Œå‡å°‘HTTPå¼€é”€
    # 3. æ›´ç®€å•ï¼Œé€‚åˆè„šæœ¬å’Œæ‰¹å¤„ç†ä»»åŠ¡
    # 4. vLLMä¼šè‡ªåŠ¨ç¼“å­˜å·²åŠ è½½çš„æ¨¡å‹
    # 
    # é…ç½®è¯´æ˜ï¼š
    # - tensor_parallel_size: å¼ é‡å¹¶è¡ŒGPUæ•°é‡ï¼ˆ1, 2, 4, 8ç­‰ï¼‰
    #   ä½¿ç”¨å¤šGPUå¯ä»¥æ˜¾è‘—é™ä½æ¯å¼ å¡çš„å†…å­˜å‹åŠ›ï¼Œè§£å†³OOMé—®é¢˜
    # - gpu_ids: æŒ‡å®šä½¿ç”¨çš„GPUç¼–å·ï¼ˆé»˜è®¤"2,3"ä½¿ç”¨åä¸¤å¼ å¡ï¼‰
    #   å¯é€šè¿‡ç¯å¢ƒå˜é‡ VLLM_GPU_IDS ä¿®æ”¹ï¼Œä¾‹å¦‚: export VLLM_GPU_IDS="2,3"
    #   å¦‚æœå·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œå°†ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡çš„è®¾ç½®
    # - gpu_memory_utilization: æ¯å¼ GPUçš„å†…å­˜åˆ©ç”¨ç‡ï¼ˆ0.0-1.0ï¼‰
    #   é»˜è®¤0.70ï¼ˆå¦‚æœGPUæœ‰å…¶ä»–è¿›ç¨‹å ç”¨ï¼Œéœ€è¦æé«˜ä»¥ç»™KV cacheè¶³å¤Ÿç©ºé—´ï¼‰
    #   å¦‚æœé‡åˆ°"No available memory for cache blocks"é”™è¯¯ï¼Œéœ€è¦æé«˜åˆ°0.70-0.75
    #   å¦‚æœé‡åˆ°OOMé”™è¯¯ï¼Œå¯ä»¥é™ä½åˆ°0.65
    # - max_model_len: æœ€å¤§åºåˆ—é•¿åº¦
    #   é»˜è®¤3072ï¼ˆéœ€è¦è‡³å°‘å®¹çº³è¾“å…¥æ–‡æœ¬+æç¤ºè¯ï¼Œå¦‚æœé‡åˆ°"decoder prompt is longer"é”™è¯¯ï¼Œéœ€è¦æé«˜ï¼‰
    #   å¦‚æœå†…å­˜å……è¶³ä¸”éœ€è¦å¤„ç†æ›´é•¿æ–‡æœ¬ï¼Œå¯ä»¥æé«˜åˆ°4096æˆ–6144
    #   å¦‚æœé‡åˆ°OOMé”™è¯¯ï¼Œå¯ä»¥é™ä½åˆ°2048ï¼Œä½†å¯èƒ½æ— æ³•å¤„ç†è¾ƒé•¿çš„æ–‡æœ¬å—
    #
    # è‡ªå®šä¹‰æ¨¡å‹ç¼“å­˜ä½ç½®ï¼š
    # æ–¹æ³•1: é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼ˆæ¨èï¼‰
    #   export HF_HOME=/path/to/your/cache
    #   æˆ–
    #   export TRANSFORMERS_CACHE=/path/to/your/cache
    # 
    # æ–¹æ³•2: åœ¨é…ç½®ä¸­ç›´æ¥æŒ‡å®š
    #   åœ¨ llm_config ä¸­è®¾ç½® 'cache_dir': '/path/to/your/cache'
    
    # ä½¿ç”¨æœ¬åœ° Qwen2-7B-Instruct æ¨¡å‹
    # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹ï¼ˆå¦‚æœè®¾ç½®äº†çš„è¯ï¼‰
    # åŸé…ç½®è®°å½•ï¼š
    # - DeepSeek-V2-Lite-Chat (MoE æ¨¡å‹ï¼Œéœ€è¦ 23GB+ æ˜¾å­˜ï¼Œå•å¡æ— æ³•åŠ è½½)
    # - Qwen/Qwen2.5-7B-Instruct (ä» HuggingFace ä¸‹è½½)
    if os.environ.get('VLLM_MODEL_PATH'):
        model_path = os.environ.get('VLLM_MODEL_PATH')
        print(f"ğŸ“ ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹è·¯å¾„: {model_path}")
    elif os.environ.get('VLLM_MODEL_NAME'):
        # å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°ï¼Œä½¿ç”¨æ¨¡å‹åç§°ï¼ˆä¼šè‡ªåŠ¨ä»ç¼“å­˜æˆ–ä¸‹è½½ï¼‰
        model_path = os.environ.get('VLLM_MODEL_NAME')
        print(f"ğŸ“ ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹åç§°: {model_path}")
    else:
        # é»˜è®¤ä½¿ç”¨æœ¬åœ° Qwen2-1.5B-Instruct æ¨¡å‹ï¼ˆçº¦ 3-4GB æ˜¾å­˜ï¼Œå•å¼  24GB æ˜¾å¡å®Œå…¨å¤Ÿç”¨ï¼‰
        # æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼š/newdatad/WHH/MyEmoHH/models/Qwen2-1.5B-Instruct/
        model_path = '/newdatad/WHH/MyEmoHH/models/Qwen2-1.5B-Instruct/'
        print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_path}")
        print(f"ğŸ’¡ æç¤º: ä½¿ç”¨æœåŠ¡å™¨æœ¬åœ°æ¨¡å‹ï¼Œæ— éœ€ä» HuggingFace ä¸‹è½½")
        print(f"   æ³¨æ„: æ”¹ç”¨ Qwen2-1.5B-Instruct (3-4GB) æ›¿ä»£ Qwen2-7B (14-16GB) ä»¥é¿å…å†…å­˜ä¸è¶³")
    
    # Qwen2-1.5B-Instruct æ¨¡å‹é…ç½®
    # è¯¥æ¨¡å‹çº¦éœ€ 3-4GB æ˜¾å­˜ï¼Œå•å¼  24GB æ˜¾å¡æœ‰å……è¶³ç©ºé—´ç”¨äºæ¨¡å‹å’Œ KV cache
    # ç›¸æ¯” Qwen2-7B (14-16GB) æ˜¾è‘—é™ä½æ˜¾å­˜éœ€æ±‚
    default_mem_util = '0.85'  # 1.5B æ¨¡å‹å¯ä»¥ä½¿ç”¨è¾ƒé«˜çš„å†…å­˜åˆ©ç”¨ç‡
    default_max_len = '4096'  # 1.5B æ¨¡å‹å¯ä»¥ä½¿ç”¨è¾ƒé•¿çš„åºåˆ—é•¿åº¦
    print(f"ğŸ“Œ ä½¿ç”¨ Qwen2-1.5B-Instruct é…ç½®ï¼ˆmem_util=0.85, max_len=4096ï¼‰")
    print(f"   æ¨¡å‹å¤§å°: çº¦ 3-4GBï¼Œå•å¼  24GB æ˜¾å¡æœ‰å……è¶³ç©ºé—´")
    
    llm_config = {
        'model': model_path,
        'tensor_parallel_size': int(os.environ.get('VLLM_TENSOR_PARALLEL_SIZE', '1')),  # é»˜è®¤ä½¿ç”¨1å¼ GPUï¼ˆå•å¡æ¨¡å¼ï¼ŒGPU 3ï¼‰
        'gpu_ids': os.environ.get('VLLM_GPU_IDS', '3'),  # é»˜è®¤ä½¿ç”¨ GPU 3ï¼ˆç¬¬å››å¼ å¡ï¼‰ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡ VLLM_GPU_IDS ä¿®æ”¹
        'gpu_memory_utilization': float(os.environ.get('VLLM_GPU_MEM_UTIL', default_mem_util)),  # æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨è°ƒæ•´
        'max_model_len': int(os.environ.get('VLLM_MAX_MODEL_LEN', default_max_len)),  # æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨è°ƒæ•´
        'temperature': float(os.environ.get('VLLM_TEMPERATURE', '0.2')),  # é»˜è®¤0.2
        'top_p': float(os.environ.get('VLLM_TOP_P', '0.9')),  # é»˜è®¤0.9
        'max_tokens': int(os.environ.get('VLLM_MAX_TOKENS', '1024')),  # é»˜è®¤1024
        'dtype': os.environ.get('VLLM_DTYPE', 'auto'),  # é»˜è®¤auto
        'cache_dir': os.environ.get('HF_HOME') or os.environ.get('TRANSFORMERS_CACHE') or os.environ.get('VLLM_CACHE_DIR'),  # è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
    }
    
    # ============================================
    # æ­¥éª¤1: æ£€æŸ¥è¾“å…¥æ–‡ä»¶/æ–‡ä»¶å¤¹
    # ============================================
    print(f"\n{'='*60}")
    print("æ­¥éª¤1: æ£€æŸ¥è¾“å…¥æ–‡ä»¶/æ–‡ä»¶å¤¹")
    print(f"{'='*60}")
    
    try:
        chunk_files = get_chunk_files(chunk_input)
        print(f"âœ… æ‰¾åˆ° {len(chunk_files)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†:")
        total_size = 0
        for i, file_path in enumerate(chunk_files, 1):
            file_size = os.path.getsize(file_path) / 1024  # KB
            total_size += file_size
            file_name = os.path.basename(file_path)
            print(f"   [{i}] {file_name} ({file_size:.2f} KB)")
        print(f"\n   æ€»å¤§å°: {total_size:.2f} KB ({total_size/1024:.2f} MB)")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        print(f"è¾“å…¥è·¯å¾„: {chunk_input}")
        return
    
    # ============================================
    # æ­¥éª¤2: åŠ è½½æ‰€æœ‰åˆ†å—æ•°æ®
    # ============================================
    print(f"\n{'='*60}")
    print("æ­¥éª¤2: åŠ è½½åˆ†å—æ•°æ®")
    print(f"{'='*60}")
    
    try:
        all_chunks = {}
        total_chunks = 0
        
        for i, chunk_file in enumerate(chunk_files, 1):
            file_name = os.path.basename(chunk_file)
            print(f"\næ­£åœ¨åŠ è½½æ–‡ä»¶ [{i}/{len(chunk_files)}]: {file_name}")
            
            file_chunks = get_chunk(chunk_file)
            print(f"  âœ… åŠ è½½äº† {len(file_chunks)} ä¸ªæ–‡æœ¬å—")
            
            # åˆå¹¶åˆ°æ€»å­—å…¸ä¸­ï¼ˆå¦‚æœ hash_code é‡å¤ï¼Œåé¢çš„ä¼šè¦†ç›–å‰é¢çš„ï¼‰
            # ç”±äº hash_code æ˜¯åŸºäºå†…å®¹ç”Ÿæˆçš„ï¼Œé‡å¤çš„å¯èƒ½æ€§å¾ˆå°
            before_count = len(all_chunks)
            all_chunks.update(file_chunks)
            after_count = len(all_chunks)
            
            if after_count - before_count < len(file_chunks):
                duplicate_count = len(file_chunks) - (after_count - before_count)
                print(f"  âš ï¸  å‘ç° {duplicate_count} ä¸ªé‡å¤çš„ hash_codeï¼ˆå·²å»é‡ï¼‰")
            
            total_chunks += len(file_chunks)
        
        print(f"\nâœ… æ€»å…±åŠ è½½ {len(all_chunks)} ä¸ªå”¯ä¸€æ–‡æœ¬å—ï¼ˆåŸå§‹æ€»æ•°: {total_chunks}ï¼‰")
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå—çš„ä¿¡æ¯
        if all_chunks:
            first_key = list(all_chunks.keys())[0]
            first_text = all_chunks[first_key]
            print(f"\nç¬¬ä¸€ä¸ªå—ä¿¡æ¯:")
            print(f"  Hash ID: {first_key[:32]}...")
            print(f"  æ–‡æœ¬é•¿åº¦: {len(first_text)} å­—ç¬¦")
            print(f"  æ–‡æœ¬é¢„è§ˆ: {first_text[:100]}...")
        
        chunks = all_chunks
        
    except Exception as e:
        print(f"âŒ åŠ è½½åˆ†å—æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ============================================
    # æ­¥éª¤3: åŠ è½½ vLLM æ¨¡å‹ï¼ˆç›´æ¥æ¨¡å¼ï¼‰
    # ============================================
    print(f"\n{'='*60}")
    print("æ­¥éª¤3: åŠ è½½ vLLM æ¨¡å‹")
    print(f"{'='*60}")
    
    try:
        use_llm_func, tokenizer = setup_vllm_direct(llm_config)
        print("âœ… vLLM æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"âŒ vLLM æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ============================================
    # æ­¥éª¤4: æå–å®ä½“å’Œå…³ç³»
    # ============================================
    print(f"\n{'='*60}")
    print("æ­¥éª¤4: æå–å®ä½“å’Œå…³ç³» (ä½¿ç”¨ GraphRAG)")
    print(f"{'='*60}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print("\nå¼€å§‹æå–... (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)")
    print("-" * 60)
    
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # è¿è¡Œå¼‚æ­¥æå–
        # main() æ˜¯åŒæ­¥å‡½æ•°ï¼Œåº”è¯¥ä½¿ç”¨ asyncio.run() æ¥è¿è¡Œå¼‚æ­¥å‡½æ•°
        # asyncio.run() ä¼šè‡ªåŠ¨åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯å¹¶è¿è¡Œï¼Œå®Œæˆåæ¸…ç†
        
        # è¿½åŠ æ¨¡å¼é…ç½®ï¼ˆé»˜è®¤Trueï¼Œæ”¯æŒè¿½åŠ æ–°æ•°æ®å¹¶å»é‡ï¼‰
        # è®¾ç½®ä¸ºFalseå°†è¦†ç›–ç°æœ‰æ–‡ä»¶
        append_mode = os.environ.get('APPEND_MODE', 'true').lower() == 'true'
        if append_mode:
            print(f"ğŸ“ æ¨¡å¼: è¿½åŠ æ¨¡å¼ï¼ˆå°†å»é‡å¹¶è¿½åŠ æ–°æ•°æ®ï¼‰")
        else:
            print(f"ğŸ“ æ¨¡å¼: è¦†ç›–æ¨¡å¼ï¼ˆå°†è¦†ç›–ç°æœ‰æ–‡ä»¶ï¼‰")
        
        # è·å– max_model_len é…ç½®ï¼Œä¼ é€’ç»™ triple_extraction ç”¨äºæ–‡æœ¬æˆªæ–­
        max_model_len = llm_config.get('max_model_len', 3072)
        
        asyncio.run(
            triple_extraction(chunks, use_llm_func, output_dir, append_mode=append_mode, max_model_len=max_model_len, tokenizer=tokenizer)
        )
        
        print("-" * 60)
        print("âœ… å®ä½“å’Œå…³ç³»æå–å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æå–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ============================================
    # æ­¥éª¤5: æ£€æŸ¥è¾“å‡ºç»“æœ
    # ============================================
    print(f"\n{'='*60}")
    print("æ­¥éª¤5: è¾“å‡ºç»“æœç»Ÿè®¡")
    print(f"{'='*60}")
    
    entity_file = f"{output_dir}/entity.jsonl"
    relation_file = f"{output_dir}/relation.jsonl"
    
    # ç»Ÿè®¡å®ä½“
    if os.path.exists(entity_file):
        with open(entity_file, 'r', encoding='utf-8') as f:
            entities = [json.loads(line) for line in f if line.strip()]
        print(f"âœ… å®ä½“æ–‡ä»¶: {entity_file}")
        print(f"   å®ä½“æ•°é‡: {len(entities)}")
        
        if entities:
            print(f"\n   å®ä½“ç¤ºä¾‹:")
            for i, entity in enumerate(entities[:3], 1):
                print(f"     [{i}] {entity.get('entity_name', 'N/A')}")
                print(f"         ç±»å‹: {entity.get('entity_type', 'N/A')}")
                print(f"         æè¿°: {entity.get('description', 'N/A')[:60]}...")
    else:
        print(f"âš ï¸  å®ä½“æ–‡ä»¶ä¸å­˜åœ¨: {entity_file}")
    
    # ç»Ÿè®¡å…³ç³»
    if os.path.exists(relation_file):
        with open(relation_file, 'r', encoding='utf-8') as f:
            raw_relations = [json.loads(line) for line in f if line.strip()]
        # å¤„ç†å¯èƒ½å­˜åœ¨çš„åˆ—è¡¨æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
        relations = []
        for rel in raw_relations:
            if isinstance(rel, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå±•å¹³å®ƒ
                relations.extend(rel)
            else:
                # å¦‚æœæ˜¯å­—å…¸ï¼Œç›´æ¥æ·»åŠ 
                relations.append(rel)
        print(f"\nâœ… å…³ç³»æ–‡ä»¶: {relation_file}")
        print(f"   å…³ç³»æ•°é‡: {len(relations)}")
        
        if relations:
            print(f"\n   å…³ç³»ç¤ºä¾‹:")
            for i, rel in enumerate(relations[:3], 1):
                src = rel.get('src_id', 'N/A')
                tgt = rel.get('tgt_id', 'N/A')
                desc = rel.get('description', 'N/A')
                print(f"     [{i}] {src} -> {tgt}")
                print(f"         æè¿°: {desc[:60]}...")
    else:
        print(f"âš ï¸  å…³ç³»æ–‡ä»¶ä¸å­˜åœ¨: {relation_file}")
    
    # ============================================
    # æ­¥éª¤6: å»é‡å¤„ç†
    # ============================================
    print(f"\n{'='*60}")
    print("æ­¥éª¤6: å»é‡å’Œåå¤„ç†")
    print(f"{'='*60}")
    print("æ¥ä¸‹æ¥éœ€è¦è¿è¡Œå»é‡è„šæœ¬:")
    print(f"  python GraphExtraction/deal_triple.py")
    print("\næˆ–è€…ä¿®æ”¹ deal_triple.py ä¸­çš„è·¯å¾„åè¿è¡Œ:")
    print(f"  working_dir='{output_dir}'")
    print(f"  output_path='{output_dir}_processed'")
    
    # ============================================
    # å®Œæˆ
    # ============================================
    print(f"\n{'='*70}")
    print(" å®ä½“æå–å®Œæˆ!")
    print(f"{'='*70}")
    print(f"\nè¾“å‡ºæ–‡ä»¶ä½ç½®:")
    print(f"  - å®ä½“: {entity_file}")
    print(f"  - å…³ç³»: {relation_file}")
    print(f"\nåç»­æ­¥éª¤:")
    print(f"  1. è¿è¡Œå»é‡è„šæœ¬: python law_deal_triple.py")
    print(f"  2. æ„å»ºçŸ¥è¯†å›¾è°±: python build_graph.py")
    print(f"  3. æŸ¥è¯¢æµ‹è¯•: python query_graph.py")


if __name__ == "__main__":
    main()

