#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ³•å¾‹å®ä½“å»é‡å’Œåå¤„ç†è„šæœ¬
åŸºäº GraphExtraction/deal_triple.py ä¿®æ”¹
ä½¿ç”¨æœ¬åœ°Qwen2-7B-Instructæ¨¡å‹ï¼ŒåŒå¡å¹¶è¡Œ
"""

import json
import os
import sys
import threading
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import tiktoken

# âš ï¸ é‡è¦ï¼šå¿…é¡»åœ¨å¯¼å…¥ä»»ä½• CUDA ç›¸å…³åº“ï¼ˆå¦‚ vLLMã€Rayã€torchï¼‰ä¹‹å‰è®¾ç½® CUDA_VISIBLE_DEVICES
# å¦åˆ™ç¯å¢ƒå˜é‡è®¾ç½®æ— æ•ˆï¼Œä¼šä½¿ç”¨é»˜è®¤ GPUï¼ˆé€šå¸¸æ˜¯ GPU 0ï¼‰
# é»˜è®¤é…ç½®ï¼šä½¿ç”¨ GPU 3ï¼ˆç¬¬å››å¼ å¡ï¼‰ï¼Œå•å¡è¿è¡Œ
# æ³¨æ„ï¼šæ ¹æ®å®é™… GPU ä½¿ç”¨æƒ…å†µé€‰æ‹©ç©ºé—²çš„ GPUï¼ˆé€šè¿‡ nvidia-smi æˆ– nvitop æŸ¥çœ‹ï¼‰
# å¦‚æœç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼Œä¼šå¼ºåˆ¶è¦†ç›–ä¸º GPU 3
default_gpu_ids = os.environ.get('VLLM_GPU_IDS', '3')

# å¼ºåˆ¶è®¾ç½® CUDA_VISIBLE_DEVICESï¼ˆè¦†ç›–ä»»ä½•å·²æœ‰è®¾ç½®ï¼‰
if 'CUDA_VISIBLE_DEVICES' in os.environ:
    old_value = os.environ['CUDA_VISIBLE_DEVICES']
    os.environ['CUDA_VISIBLE_DEVICES'] = default_gpu_ids
    print(f"ğŸ”§ è¦†ç›– CUDA_VISIBLE_DEVICES: {old_value} -> {default_gpu_ids}")
else:
    os.environ['CUDA_VISIBLE_DEVICES'] = default_gpu_ids
    print(f"ğŸ”§ åœ¨å¯¼å…¥ vLLM ä¹‹å‰è®¾ç½® CUDA_VISIBLE_DEVICES={default_gpu_ids}")

print(f"âœ… å½“å‰ CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}ï¼Œå°†ä½¿ç”¨ GPU 3ï¼ˆç¬¬å››å¼ å¡ï¼‰")

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„ï¼ˆä¸Šä¸€çº§ç›®å½•ï¼‰åˆ° sys.pathï¼Œç¡®ä¿å¯å¯¼å…¥æ ¹ç›®å½•ä¸‹æ¨¡å—
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from tools.utils import read_jsonl, write_jsonl, create_if_not_exist, InstanceManager
from prompt import PROMPTS

# å¯¼å…¥vLLMï¼ˆç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦APIæœåŠ¡ï¼‰
try:
    from vllm import LLM, SamplingParams
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: vLLM æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æœ¬åœ°æ¨¡å‹")


# æè¿°æ‘˜è¦çš„tokené˜ˆå€¼
THRESHOLD = 50


def setup_vllm_sync(config):
    """
    ç›´æ¥åŠ è½½vLLMæ¨¡å‹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œç”¨äºæ‘˜è¦ä»»åŠ¡ï¼‰
    
    å‚æ•°:
        config: LLMé…ç½®å­—å…¸
            - model: æ¨¡å‹è·¯å¾„æˆ–åç§°
            - tensor_parallel_size: å¼ é‡å¹¶è¡ŒGPUæ•°é‡ï¼ˆé»˜è®¤1ï¼Œå•å¡æ¨¡å¼ï¼‰
            - gpu_memory_utilization: æ¯å¼ GPUçš„å†…å­˜åˆ©ç”¨ç‡ï¼ˆé»˜è®¤0.80ï¼‰
            - max_model_len: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤4096ï¼‰
            - temperature: é‡‡æ ·æ¸©åº¦ï¼ˆé»˜è®¤0.2ï¼‰
            - top_p: top_pé‡‡æ ·é˜ˆå€¼ï¼ˆé»˜è®¤0.9ï¼‰
            - max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆé»˜è®¤1024ï¼‰
    
    è¿”å›:
        åŒæ­¥LLMç”Ÿæˆå‡½æ•°
    """
    if not VLLM_AVAILABLE:
        raise ImportError("vLLM æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install vllm")
    
    print(f"\n{'='*60}")
    print("åŠ è½½ vLLM æ¨¡å‹ï¼ˆåŒæ­¥æ¨¡å¼ï¼Œç”¨äºæ‘˜è¦ï¼‰")
    print(f"{'='*60}")
    print(f"æ¨¡å‹è·¯å¾„: {config['model']}")
    print(f"å¼ é‡å¹¶è¡ŒGPUæ•°é‡: {config.get('tensor_parallel_size', 1)}")
    print(f"æ¯å¼ GPUå†…å­˜åˆ©ç”¨ç‡: {config.get('gpu_memory_utilization', 0.80)}")
    print(f"æœ€å¤§åºåˆ—é•¿åº¦: {config.get('max_model_len', 4096)}")
    
    print("=" * 60)
    print("\næ­£åœ¨åŠ è½½æ¨¡å‹...ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰")
    
    # è®¾ç½®PyTorchå†…å­˜åˆ†é…é…ç½®ï¼ˆé¿å…å†…å­˜ç¢ç‰‡ï¼‰
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    
    # è·å– tensor_parallel_size
    tensor_parallel_size = config.get('tensor_parallel_size', 1)
    
    # åœ¨ä½¿ç”¨å¤š GPU æ—¶ï¼ŒvLLM ä¼šä½¿ç”¨ Ray æ¥åè°ƒï¼Œéœ€è¦æ­£ç¡®åˆå§‹åŒ– Ray
    if tensor_parallel_size > 1:
        try:
            import ray
            # æ£€æŸ¥ Ray æ˜¯å¦å·²ç»åˆå§‹åŒ–
            if not ray.is_initialized():
                print("ğŸ”§ åˆå§‹åŒ– Ray é›†ç¾¤ï¼ˆç”¨äºå¤š GPU å¼ é‡å¹¶è¡Œï¼‰...")
                # æ˜¾å¼åˆå§‹åŒ– Rayï¼ŒæŒ‡å®š GPU æ•°é‡
                # æ³¨æ„ï¼šRay ä¼šä½¿ç”¨ CUDA_VISIBLE_DEVICES ä¸­æŒ‡å®šçš„ GPU
                ray.init(
                    ignore_reinit_error=True,
                    num_gpus=tensor_parallel_size,
                    num_cpus=tensor_parallel_size,  # æ¯ä¸ª GPU åˆ†é… 1 ä¸ª CPU
                    object_store_memory=2 * 10**9,  # 2GB å¯¹è±¡å­˜å‚¨
                    _temp_dir="/tmp/ray",  # æŒ‡å®šä¸´æ—¶ç›®å½•
                )
                print("âœ… Ray é›†ç¾¤åˆå§‹åŒ–å®Œæˆ")
            else:
                print("âœ… Ray é›†ç¾¤å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âš ï¸  Ray åˆå§‹åŒ–è­¦å‘Š: {e}")
            print("   å°†å°è¯•ç»§ç»­è¿è¡Œï¼ŒvLLM å¯èƒ½ä¼šè‡ªåŠ¨åˆå§‹åŒ– Ray")
    
    # åŠ è½½vLLMæ¨¡å‹
    llm_kwargs = {
        'model': config['model'],
        'tensor_parallel_size': tensor_parallel_size,
        'gpu_memory_utilization': config.get('gpu_memory_utilization', 0.80),
        'max_model_len': config.get('max_model_len', 4096),
        'trust_remote_code': True,
        'dtype': config.get('dtype', 'auto'),
    }
    
    llm = LLM(**llm_kwargs)
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=config.get('temperature', 0.2),
        top_p=config.get('top_p', 0.9),
        max_tokens=config.get('max_tokens', 1024),  # ä¸æå–è„šæœ¬ä¸€è‡´
    )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("=" * 60)
    print("")
    
    # åˆ›å»ºçº¿ç¨‹é”ï¼Œç¡®ä¿ vLLM è°ƒç”¨çš„çº¿ç¨‹å®‰å…¨
    vllm_lock = threading.Lock()
    
    # åˆ›å»ºåŒæ­¥ç”Ÿæˆå‡½æ•°
    def generate_text_sync(prompt, system_prompt="", **kwargs):
        """
        åŒæ­¥ç”Ÿæˆæ–‡æœ¬å‡½æ•°ï¼ˆåŒ…è£…vLLMçš„åŒæ­¥è°ƒç”¨ï¼‰
        
        å‚æ•°:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆæš‚æ—¶å¿½ç•¥ï¼Œä½¿ç”¨é»˜è®¤sampling_paramsï¼‰
        
        è¿”å›:
            ç”Ÿæˆçš„æ–‡æœ¬å­—ç¬¦ä¸²
        """
        # æ„å»ºå®Œæ•´çš„æç¤º
        full_prompt = ""
        if system_prompt:
            full_prompt += f"System: {system_prompt}\n\n"
        full_prompt += f"User: {prompt}\n\nAssistant: "
        
        # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤ vLLM è°ƒç”¨
        with vllm_lock:
            outputs = llm.generate([full_prompt], sampling_params)
        
        if not outputs or not outputs[0].outputs:
            return ""
        
        return outputs[0].outputs[0].text.strip()
    
    return generate_text_sync


def summarize_entity(entity_name, description, summary_prompt, threshold, tokenizer, use_llm):
    """
    å¦‚æœå®ä½“æè¿°è¶…è¿‡é˜ˆå€¼ï¼Œä½¿ç”¨LLMè¿›è¡Œæ‘˜è¦
    
    å‚æ•°:
        entity_name: å®ä½“åç§°
        description: å®ä½“æè¿°
        summary_prompt: æ‘˜è¦æç¤ºè¯æ¨¡æ¿
        threshold: tokené˜ˆå€¼
        tokenizer: tiktoken tokenizer
        use_llm: LLMå‡½æ•°
    """
    tokens = len(tokenizer.encode(description))
    if tokens > threshold:
        exact_prompt = summary_prompt.format(entity_name=entity_name, description=description)
        response = use_llm(exact_prompt)
        return entity_name, response
    return entity_name, description  # ä¸éœ€è¦æ‘˜è¦åˆ™è¿”å›åŸå§‹æè¿°


def deal_duplicate_entity(working_dir, output_path, use_llm=None):
    """
    å¤„ç†é‡å¤å®ä½“ï¼Œåˆå¹¶æè¿°ï¼Œå¹¶è¿›è¡Œæ‘˜è¦
    
    å‚æ•°:
        working_dir: è¾“å…¥ç›®å½•ï¼ˆåŒ…å« entity.jsonl å’Œ relation.jsonlï¼‰
        output_path: è¾“å‡ºç›®å½•
        use_llm: LLMå‡½æ•°ï¼ˆå¯é€‰ï¼Œç”¨äºæ‘˜è¦ï¼‰
    """
    print(f"\n{'='*60}")
    print("å»é‡å’Œåå¤„ç†")
    print(f"{'='*60}")
    
    relation_path = f"{working_dir}/relation.jsonl"
    relation_output_path = f"{output_path}/relation.jsonl"
    entity_path = f"{working_dir}/entity.jsonl"
    entity_output_path = f"{output_path}/entity.jsonl"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(entity_path):
        print(f"âŒ é”™è¯¯: å®ä½“æ–‡ä»¶ä¸å­˜åœ¨: {entity_path}")
        return False
    
    if not os.path.exists(relation_path):
        print(f"âŒ é”™è¯¯: å…³ç³»æ–‡ä»¶ä¸å­˜åœ¨: {relation_path}")
        return False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    create_if_not_exist(output_path)
    
    # ============================================
    # å¤„ç†å®ä½“
    # ============================================
    print(f"\nå¤„ç†å®ä½“æ–‡ä»¶: {entity_path}")
    
    all_entities = []
    e_dic = {}
    # ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡æ‘˜è¦æç¤ºè¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬
    summary_prompt = PROMPTS.get('summary_entities_zh', PROMPTS.get('summary_entities', ''))
    
    # è¯»å–å¹¶åˆå¹¶é‡å¤å®ä½“
    with open(entity_path, "r", encoding='utf-8') as f:
        for line_num, xline in enumerate(f, 1):
            try:
                line = json.loads(xline)
                entity_name = str(line['entity_name']).replace('"', '')
                entity_type = line.get('entity_type', '').replace('"', '')
                description = line['description'].replace('"', '')
                source_id = line['source_id']
                
                if entity_name not in e_dic:
                    e_dic[entity_name] = dict(
                        entity_name=str(entity_name),
                        entity_type=entity_type,
                        description=description,
                        source_id=source_id,
                        degree=0,
                    )
                else:
                    # åˆå¹¶æè¿°
                    e_dic[entity_name]['description'] += " | " + description
                    # åˆå¹¶æ¥æºID
                    if e_dic[entity_name]['source_id'] != source_id:
                        e_dic[entity_name]['source_id'] += "|" + source_id
            
            except Exception as e:
                print(f"âš ï¸  è­¦å‘Š: ç¬¬ {line_num} è¡Œè§£æå¤±è´¥: {e}")
                continue
    
    print(f"   åŸå§‹å®ä½“æ•°: {line_num}")
    print(f"   å»é‡åå®ä½“æ•°: {len(e_dic)}")
    
    # å»é‡æ¥æºID
    for k, v in e_dic.items():
        v['source_id'] = "|".join(set(v['source_id'].split("|")))
    
    # ============================================
    # æ‘˜è¦é•¿æè¿°
    # ============================================
    tokenizer = tiktoken.get_encoding("cl100k_base")
    to_summarize = []
    
    for k, v in e_dic.items():
        description = v['description']
        tokens = len(tokenizer.encode(description))
        if tokens > THRESHOLD:
            to_summarize.append((k, description))
        else:
            all_entities.append(v)
    
    print(f"   éœ€è¦æ‘˜è¦çš„å®ä½“æ•°: {len(to_summarize)}")
    
    if to_summarize and use_llm:
        print("   å¼€å§‹æ‘˜è¦å¤„ç†...")
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {
                executor.submit(summarize_entity, k, desc, summary_prompt, THRESHOLD, tokenizer, use_llm): k
                for k, desc in to_summarize
            }
            for future in tqdm(as_completed(futures), total=len(futures), desc="   æ‘˜è¦è¿›åº¦"):
                k, summarized_desc = future.result()
                e_dic[k]['description'] = summarized_desc
                all_entities.append(e_dic[k])
    elif to_summarize:
        print("   âš ï¸  æœªæä¾›LLMå‡½æ•°ï¼Œè·³è¿‡æ‘˜è¦ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æè¿°")
        for k, _ in to_summarize:
            all_entities.append(e_dic[k])
    
    # ä¿å­˜å®ä½“
    write_jsonl(all_entities, entity_output_path)
    print(f"âœ… å®ä½“å·²ä¿å­˜: {entity_output_path}")
    print(f"   æœ€ç»ˆå®ä½“æ•°: {len(all_entities)}")
    
    # ============================================
    # å¤„ç†å…³ç³»
    # ============================================
    print(f"\nå¤„ç†å…³ç³»æ–‡ä»¶: {relation_path}")
    
    all_relations = []
    r_dic = {}  # ç”¨äºå…³ç³»å»é‡: key = (src_tgt, tgt_src)
    raw_relation_count = 0
    
    with open(relation_path, "r", encoding='utf-8') as f:
        for line_num, xline in enumerate(f, 1):
            try:
                data = json.loads(xline)
                raw_relation_count += 1
                
                # å¤„ç†ä¸¤ç§æ•°æ®æ ¼å¼ï¼šåˆ—è¡¨æˆ–å­—å…¸
                if isinstance(data, list):
                    line = data[0]
                else:
                    line = data
                
                # æ ¹æ®æ•°æ®æ ¼å¼é€‰æ‹©æ­£ç¡®çš„å­—æ®µå
                if 'src_id' in line:
                    src_tgt = str(line['src_id']).replace('"', '')
                    tgt_src = str(line['tgt_id']).replace('"', '')
                else:
                    src_tgt = str(line.get('src_tgt', '')).replace('"', '')
                    tgt_src = str(line.get('tgt_src', '')).replace('"', '')
                
                description = line['description'].replace('"', '')
                # å¦‚æœåŸæ•°æ®æœ‰weightå­—æ®µï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™é»˜è®¤ä¸º1
                weight = line.get('weight', 1)
                if isinstance(weight, (int, float)):
                    weight = float(weight)
                else:
                    weight = 1.0
                source_id = line['source_id']
                
                # å…³ç³»å»é‡ï¼šä½¿ç”¨ (src_tgt, tgt_src) ä½œä¸ºå”¯ä¸€é”®
                relation_key = (src_tgt, tgt_src)
                
                if relation_key not in r_dic:
                    # æ–°å…³ç³»
                    r_dic[relation_key] = dict(
                        src_tgt=src_tgt,
                        tgt_src=tgt_src,
                        description=description,
                        weight=weight,
                        source_id=source_id
                    )
                else:
                    # é‡å¤å…³ç³»ï¼šåˆå¹¶æè¿°ã€ç´¯åŠ æƒé‡ã€åˆå¹¶æ¥æºID
                    r_dic[relation_key]['description'] += " | " + description
                    r_dic[relation_key]['weight'] += weight  # ç´¯åŠ æƒé‡
                    if r_dic[relation_key]['source_id'] != source_id:
                        r_dic[relation_key]['source_id'] += "|" + source_id
            
            except Exception as e:
                print(f"âš ï¸  è­¦å‘Š: ç¬¬ {line_num} è¡Œè§£æå¤±è´¥: {e}")
                continue
    
    # å»é‡æ¥æºID
    for k, v in r_dic.items():
        v['source_id'] = "|".join(set(v['source_id'].split("|")))
        all_relations.append(v)
    
    print(f"   åŸå§‹å…³ç³»æ•°: {raw_relation_count}")
    print(f"   å»é‡åå…³ç³»æ•°: {len(all_relations)}")
    
    # ä¿å­˜å…³ç³»
    write_jsonl(all_relations, relation_output_path)
    print(f"âœ… å…³ç³»å·²ä¿å­˜: {relation_output_path}")
    print(f"   æœ€ç»ˆå…³ç³»æ•°: {len(all_relations)}")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    
    print("="*70)
    print(" æ³•å¾‹å®ä½“å»é‡å’Œåå¤„ç†")
    print("="*70)
    
    # ============================================
    # é…ç½®å‚æ•° - æ ¹æ®ä½ çš„ç¯å¢ƒä¿®æ”¹è¿™é‡Œ
    # ============================================
    
    # è¾“å…¥è¾“å‡ºç›®å½•
    # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥æŒ‡å‘ law_extract_graphrag_parllar2.py çš„è¾“å‡ºç›®å½•
    working_dir = "/newdataf/SJ/LeanRAG/output/social_law_7B"            # GraphRAG è¾“å‡ºçš„åŸå§‹ç›®å½•
    output_path = "/newdataf/SJ/LeanRAG/output/social_law_7B_processed"  # å¤„ç†åçš„è¾“å‡ºç›®å½•
    
    # æ˜¯å¦ä½¿ç”¨LLMè¿›è¡Œæ‘˜è¦ï¼ˆå¯é€‰ï¼‰
    use_llm_for_summary = True  # æ”¹ä¸º Trueï¼Œå¯ç”¨æ‘˜è¦åŠŸèƒ½å¹¶ä½¿ç”¨æœ¬åœ° vLLM æ¨¡å‹
    
    # LLM é…ç½®ï¼ˆæœ¬åœ° vLLMï¼Œä¸æå–è„šæœ¬ law_extract_graphrag_parllar2_QWen7B.py ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ï¼‰
    # ä½¿ç”¨æœ¬åœ° Qwen2-7B-Instruct æ¨¡å‹ï¼ˆä¸æå–è„šæœ¬ä¸€è‡´ï¼‰
    if os.environ.get('VLLM_MODEL_PATH'):
        model_path = os.environ.get('VLLM_MODEL_PATH')
        print(f"ğŸ“ ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹è·¯å¾„: {model_path}")
    elif os.environ.get('VLLM_MODEL_NAME'):
        model_path = os.environ.get('VLLM_MODEL_NAME')
        print(f"ğŸ“ ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹åç§°: {model_path}")
    else:
        # é»˜è®¤ä½¿ç”¨æœ¬åœ° Qwen2-7B-Instruct æ¨¡å‹ï¼ˆä¸æå–è„šæœ¬ä¸€è‡´ï¼‰
        # æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼š/newdatad/WHH/MyEmoHH/models/Qwen2-7B-Instruct
        model_path = '/newdatad/WHH/MyEmoHH/models/Qwen2-7B-Instruct'
        print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_path}")
        print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ä¸æå–è„šæœ¬ç›¸åŒçš„æ¨¡å‹ï¼Œä¿è¯ä¸€è‡´æ€§")
        print(f"   é…ç½®: ä½¿ç”¨ tensor_parallel_size=1 å•å¡æ¨¡å¼ï¼Œéœ€è¦çº¦ 14-16GB æ˜¾å­˜")
    
    # Qwen2-7B-Instruct æ¨¡å‹é…ç½®ï¼ˆå•å¡æ¨¡å¼ï¼‰
    # è¯¥æ¨¡å‹çº¦éœ€ 14-16GB æ˜¾å­˜ï¼Œä½¿ç”¨å•å¡æ—¶éœ€è¦è¾ƒé«˜çš„å†…å­˜åˆ©ç”¨ç‡
    default_mem_util = '0.85'  # å•å¡æ¨¡å¼ä½¿ç”¨è¾ƒé«˜çš„å†…å­˜åˆ©ç”¨ç‡
    default_max_len = '3072'  # å•å¡æ¨¡å¼ä½¿ç”¨é€‚ä¸­çš„åºåˆ—é•¿åº¦
    print(f"ğŸ“Œ ä½¿ç”¨ Qwen2-7B-Instruct é…ç½®ï¼ˆå•å¡æ¨¡å¼ï¼šmem_util=0.85, max_len=3072ï¼‰")
    print(f"   æ¨¡å‹å¤§å°: çº¦ 14-16GBï¼Œå•å¡è¿è¡Œéœ€è¦24GBæ˜¾å­˜")
    
    llm_config = {
        'model': model_path,
        'tensor_parallel_size': int(os.environ.get('VLLM_TENSOR_PARALLEL_SIZE', '1')),  # é»˜è®¤ä½¿ç”¨1å¼ GPUï¼ˆå•å¡æ¨¡å¼ï¼ŒGPU 3ï¼‰
        'gpu_ids': os.environ.get('VLLM_GPU_IDS', '3'),  # é»˜è®¤ä½¿ç”¨ GPU 3ï¼ˆç¬¬å››å¼ å¡ï¼‰ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡ VLLM_GPU_IDS ä¿®æ”¹
        'gpu_memory_utilization': float(os.environ.get('VLLM_GPU_MEM_UTIL', default_mem_util)),  # ä¸æå–è„šæœ¬ä¸€è‡´
        'max_model_len': int(os.environ.get('VLLM_MAX_MODEL_LEN', default_max_len)),  # ä¸æå–è„šæœ¬ä¸€è‡´
        'temperature': float(os.environ.get('VLLM_TEMPERATURE', '0.2')),  # é»˜è®¤0.2
        'top_p': float(os.environ.get('VLLM_TOP_P', '0.9')),  # é»˜è®¤0.9
        'max_tokens': int(os.environ.get('VLLM_MAX_TOKENS', '1024')),  # é»˜è®¤512ï¼ˆæ‘˜è¦ä»»åŠ¡ä½¿ç”¨è¾ƒçŸ­è¾“å‡ºï¼‰
        'dtype': os.environ.get('VLLM_DTYPE', 'auto'),  # é»˜è®¤auto
        'cache_dir': os.environ.get('HF_HOME') or os.environ.get('TRANSFORMERS_CACHE') or os.environ.get('VLLM_CACHE_DIR'),  # è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
    }
    
    # ============================================
    # è®¾ç½® LLMï¼ˆå¦‚æœéœ€è¦ï¼‰
    # ============================================
    use_llm = None
    
    if use_llm_for_summary:
        print(f"\n{'='*60}")
        print("é…ç½® LLM ç”¨äºæè¿°æ‘˜è¦ï¼ˆä½¿ç”¨æœ¬åœ° vLLMï¼‰")
        print(f"{'='*60}")
        
        try:
            use_llm = setup_vllm_sync(llm_config)
            print("âœ… LLM é…ç½®å®Œæˆ")
        except Exception as e:
            print(f"âŒ LLM é…ç½®å¤±è´¥: {e}")
            print("âš ï¸  å°†è·³è¿‡æ‘˜è¦åŠŸèƒ½ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æè¿°")
            use_llm = None
    else:
        print(f"\nâš ï¸  æœªå¯ç”¨LLMæ‘˜è¦åŠŸèƒ½ï¼ˆuse_llm_for_summary=Falseï¼‰")
    
    # ============================================
    # æ‰§è¡Œå»é‡å¤„ç†
    # ============================================
    success = deal_duplicate_entity(working_dir, output_path, use_llm)
    
    if success:
        print(f"\n{'='*70}")
        print(" å¤„ç†å®Œæˆ!")
        print(f"{'='*70}")
        print(f"\nè¾“å‡ºæ–‡ä»¶:")
        print(f"  - å®ä½“: {output_path}/entity.jsonl")
        print(f"  - å…³ç³»: {output_path}/relation.jsonl")
        print(f"\nåç»­æ­¥éª¤:")
        print(f"  1. å°†å¤„ç†åçš„æ–‡ä»¶ç”¨äºæ„å»ºçŸ¥è¯†å›¾è°±")
        print(f"  2. è¿è¡Œ: python build_graph.py")
    else:
        print(f"\nâŒ å¤„ç†å¤±è´¥")


if __name__ == "__main__":
    main()


