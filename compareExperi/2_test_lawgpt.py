#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LawGPT_zh æ‰¹é‡æµ‹è¯• - åŸºäº ChatGLM-6B çš„æ³•å¾‹æ¨¡å‹
"""

import sys
import os
import json
import argparse
from typing import List, Dict
from tqdm import tqdm
import time

# è®¾ç½®GPU
os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'

import torch
from transformers import AutoModel, AutoTokenizer

print("="*70)
print("LawGPT_zh æ‰¹é‡æµ‹è¯• (ChatGLM-6B æ³•å¾‹æ¨¡å‹)")
print("="*70)
print(f"ä½¿ç”¨GPU: {os.environ['CUDA_VISIBLE_DEVICES']}")
print(f"å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
print("="*70)


def load_model(model_path: str):
    """åŠ è½½ LawGPT æ¨¡å‹"""
    print(f"\nğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    
    # å…ˆåŠ è½½æ¨¡å‹
    model = AutoModel.from_pretrained(
        model_path,
        trust_remote_code=True,
        revision='main'  # æŒ‡å®šç‰ˆæœ¬ä»¥æ¶ˆé™¤è­¦å‘Š
    ).half().cuda()
    
    # å†åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        revision='main'  # æŒ‡å®šç‰ˆæœ¬ä»¥æ¶ˆé™¤è­¦å‘Š
    )
    
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print(f"   æ¨¡å‹ç±»å‹: ChatGLM-6B (æ³•å¾‹å¾®è°ƒç‰ˆ)")
    print(f"   å‚æ•°é‡: 6B")
    
    return model, tokenizer


def generate_answer(
    model,
    tokenizer,
    question: str,
    max_length: int = 2048,
    temperature: float = 0.7,
    top_p: float = 0.9
) -> str:
    """ç”Ÿæˆç­”æ¡ˆ - ä½¿ç”¨ ChatGLM çš„ chat æ–¹æ³•"""
    
    try:
        response, history = model.chat(
            tokenizer,
            question,
            history=[],
            max_length=max_length,
            temperature=temperature,
            top_p=top_p
        )
        return response.strip()
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆå¤±è´¥: {e}")
        return f"[é”™è¯¯: {str(e)}]"


def main():
    parser = argparse.ArgumentParser(description="LawGPT_zh æ‰¹é‡æµ‹è¯•")
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥JSONæ–‡ä»¶")
    parser.add_argument("--output", type=str, required=True, help="è¾“å‡ºJSONæ–‡ä»¶")
    parser.add_argument(
        "--model-path",
        type=str,
        default="/newdatae/model/LawGPT_zh",
        help="LawGPTæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument("--max-length", type=int, default=2048, help="æœ€å¤§é•¿åº¦")
    parser.add_argument("--temperature", type=float, default=0.7, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top-p", type=float, default=0.9, help="top_pé‡‡æ ·")
    
    args = parser.parse_args()
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(args.model_path)
    
    # åŠ è½½é—®é¢˜
    print(f"\nğŸ”„ æ­£åœ¨åŠ è½½é—®é¢˜: {args.input}")
    with open(args.input, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    total_questions = len(data)
    print(f"âœ… åŠ è½½äº† {total_questions} ä¸ªé—®é¢˜\n")
    
    # å¤„ç†æ¯ä¸ªé—®é¢˜
    results = []
    start_time = time.time()
    question_times = []
    
    print("="*70)
    print("å¼€å§‹å¤„ç†é—®é¢˜")
    print("="*70)
    print(f"å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}")
    print("="*70 + "\n")
    
    for i, item in enumerate(tqdm(data, desc="å¤„ç†é—®é¢˜"), 1):
        question = item.get('question', '')
        if not question:
            continue
        
        question_start = time.time()
        try:
            answer = generate_answer(
                model,
                tokenizer,
                question,
                max_length=args.max_length,
                temperature=args.temperature,
                top_p=args.top_p
            )
            question_time = time.time() - question_start
            question_times.append(question_time)
            
            result_item = item.copy()
            result_item['prediction'] = answer
            result_item['method'] = 'lawgpt_zh'
            result_item['processing_time'] = question_time
            results.append(result_item)
            
        except Exception as e:
            question_time = time.time() - question_start
            question_times.append(question_time)
            print(f"\nâŒ [{i}/{total_questions}] å¤„ç†å¤±è´¥: {e}")
            
            result_item = item.copy()
            result_item['prediction'] = f"[é”™è¯¯: {str(e)}]"
            result_item['method'] = 'lawgpt_zh'
            result_item['processing_time'] = question_time
            results.append(result_item)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    end_time = time.time()
    total_time = end_time - start_time
    avg_time = sum(question_times) / len(question_times) if question_times else 0
    min_time = min(question_times) if question_times else 0
    max_time = max(question_times) if question_times else 0
    
    # ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ: {args.output}")
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*70)
    print("âœ… å®éªŒå®Œæˆç»Ÿè®¡")
    print("="*70)
    print(f"æ€»é—®é¢˜æ•°: {total_questions}")
    print(f"æˆåŠŸå¤„ç†: {len(results)}")
    print(f"å¤±è´¥æ•°é‡: {total_questions - len(results)}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    print("="*70)
    
    # æ‰“å°è¯¦ç»†æ—¶é—´ç»Ÿè®¡
    print("\n" + "="*70)
    print("â±ï¸  è¯¦ç»†æ—¶é—´ç»Ÿè®¡")
    print("="*70)
    print(f"å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}")
    print(f"ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}")
    print(f"-" * 70)
    print(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ / {total_time/3600:.2f} å°æ—¶)")
    print(f"é—®é¢˜æ€»æ•°: {len(results)}")
    print(f"-" * 70)
    print(f"å¹³å‡æ¯é¢˜è€—æ—¶: {avg_time:.2f} ç§’")
    print(f"æœ€å¿«é—®é¢˜è€—æ—¶: {min_time:.2f} ç§’")
    print(f"æœ€æ…¢é—®é¢˜è€—æ—¶: {max_time:.2f} ç§’")
    print(f"-" * 70)
    print(f"ååé‡: {len(results)/total_time:.2f} é—®é¢˜/ç§’")
    print(f"é¢„è®¡1000é¢˜è€—æ—¶: {(avg_time * 1000)/60:.2f} åˆ†é’Ÿ")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
