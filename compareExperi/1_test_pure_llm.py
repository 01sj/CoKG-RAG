#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¯¹æ¯”å®éªŒ1ï¼šæµ‹è¯•çº¯LLMï¼ˆæ— RAGï¼‰

ç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹å›ç­”é—®é¢˜ï¼Œä¸ä½¿ç”¨ä»»ä½•æ£€ç´¢å¢å¼ºã€‚
è¿™æ˜¯æœ€åŸºç¡€çš„åŸºçº¿æ–¹æ³•ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
python compareExperi/1_test_pure_llm.py \
    --input datasets/query_social.json \
    --output compareExperi/results/pure_llm_pred.json
"""

import sys
import os

# âš ï¸ é‡è¦ï¼šå¿…é¡»åœ¨å¯¼å…¥ä»»ä½•CUDAç›¸å…³åº“ä¹‹å‰è®¾ç½®GPU
# ä½¿ç”¨ç¬¬3å¼ GPUå¡ï¼ˆç´¢å¼•2ï¼‰
os.environ['CUDA_VISIBLE_DEVICES'] = '2'
print(f"ğŸ”§ è®¾ç½®ä½¿ç”¨GPUå¡: CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}")

import json
import argparse
from typing import List, Dict
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é»˜è®¤é…ç½®
DEFAULT_MODEL_PATH = "/newdatae/model/Qwen-7B-Chat"

# ç³»ç»Ÿæç¤ºè¯ï¼ˆç”¨äºçº¯LLMåŸºçº¿æµ‹è¯•ï¼‰
# æ³¨æ„ï¼šè¿™é‡Œä¸è¦æ±‚å¼•ç”¨å…·ä½“æ³•æ¡ï¼Œå› ä¸ºæ¨¡å‹æ²¡æœ‰æ£€ç´¢èƒ½åŠ›ï¼Œé¿å…äº§ç”Ÿå¹»è§‰
DEFAULT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹åŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”ç¤¾ä¼šæ³•ç›¸å…³çš„é—®é¢˜ã€‚
è¯·æ ¹æ®ä½ å·²æœ‰çš„æ³•å¾‹çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. å¦‚æœä½ çŸ¥é“ç›¸å…³æ³•å¾‹è§„å®šï¼Œè¯·è¯´æ˜
2. å¦‚æœä¸ç¡®å®šï¼Œè¯·æ˜ç¡®è¡¨ç¤ºä¸ç¡®å®šï¼Œä¸è¦ç¼–é€ 
3. é€»è¾‘æ¸…æ™°ï¼Œæ¡ç†åˆ†æ˜
4. è¯­è¨€ä¸“ä¸šä½†æ˜“æ‡‚"""


def load_questions(input_file: str) -> List[Dict]:
    """åŠ è½½é—®é¢˜æ•°æ®"""
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def test_pure_llm(
    input_file: str,
    output_file: str,
    model_path: str,
    system_prompt: str,
    temperature: float = 0.3,
    top_p: float = 0.9,
    max_tokens: int = 1024,
    gpu_mem_util: float = 0.75,
    max_model_len: int = 8192,
):
    """ä½¿ç”¨vLLMæµ‹è¯•çº¯LLM"""
    import time
    from vllm import LLM, SamplingParams
    
    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = time.time()
    
    print("\nğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   GPUå†…å­˜ä½¿ç”¨ç‡: {gpu_mem_util}")
    print(f"   æœ€å¤§æ¨¡å‹é•¿åº¦: {max_model_len}")
    
    # åŠ è½½vLLMæ¨¡å‹ï¼ˆQwenæ¨¡å‹éœ€è¦trust_remote_code=Trueï¼‰
    try:
        llm = LLM(
            model=model_path,
            trust_remote_code=True,
            gpu_memory_utilization=gpu_mem_util,
            max_model_len=max_model_len,
        )
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    except ValueError as e:
        if "KV cache" in str(e):
            print(f"\nâŒ GPUå†…å­˜ä¸è¶³ï¼")
            print(f"é”™è¯¯ä¿¡æ¯: {e}")
            print(f"\nğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆï¼š")
            print(f"1. é™ä½ max_model_len: --max-model-len 3600")
            print(f"2. æé«˜ gpu_mem_util: --gpu-mem-util 0.9")
            print(f"3. ä¸¤è€…ç»“åˆä½¿ç”¨")
            raise
        else:
            raise
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
    )
    
    # åŠ è½½é—®é¢˜
    print(f"\nğŸ”„ æ­£åœ¨åŠ è½½é—®é¢˜: {input_file}")
    questions = load_questions(input_file)
    print(f"âœ… åŠ è½½äº† {len(questions)} ä¸ªé—®é¢˜")
    
    # å‡†å¤‡prompts
    prompts = []
    for item in questions:
        question = item.get('question', '')
        # æ„å»ºå®Œæ•´çš„promptï¼ˆQwen ChatMLæ ¼å¼ï¼‰
        # Qwen-7B-Chat ä½¿ç”¨ ChatML æ ¼å¼ï¼š<|im_start|>role\ncontent<|im_end|>
        prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
"""
        prompts.append(prompt)
    
    # æ‰¹é‡ç”Ÿæˆ
    print("\nğŸ”„ æ­£åœ¨ç”Ÿæˆå›ç­”...")
    generation_start_time = time.time()
    outputs = llm.generate(prompts, sampling_params)
    generation_end_time = time.time()
    
    # æ•´ç†ç»“æœ
    results = []
    for i, output in enumerate(tqdm(outputs, desc="å¤„ç†ç»“æœ")):
        item = questions[i].copy()
        generated_text = output.outputs[0].text.strip()
        item['prediction'] = generated_text  # ä¿å­˜åœ¨ prediction å­—æ®µ
        results.append(item)
    
    # ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # è®¡ç®—æ—¶é—´ç»Ÿè®¡
    total_end_time = time.time()
    total_time = total_end_time - total_start_time
    generation_time = generation_end_time - generation_start_time
    avg_time_per_question = generation_time / len(results) if results else 0
    
    # æ‰“å°æ—¶é—´ç»Ÿè®¡
    print("\n" + "="*60)
    print("â±ï¸  æ—¶é—´ç»Ÿè®¡")
    print("="*60)
    print(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
    print(f"ç”Ÿæˆå›ç­”æ—¶é—´: {generation_time:.2f}ç§’ ({generation_time/60:.2f}åˆ†é’Ÿ)")
    print(f"é—®é¢˜æ€»æ•°: {len(results)}")
    print(f"å¹³å‡æ¯é¢˜è€—æ—¶: {avg_time_per_question:.2f}ç§’")
    print(f"ååé‡: {len(results)/generation_time:.2f} é—®é¢˜/ç§’")
    print("="*60)
    
    print(f"\nâœ… æˆåŠŸå¤„ç† {len(results)} ä¸ªé—®é¢˜")


def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•çº¯LLMï¼ˆæ— RAGï¼‰")
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥JSONæ–‡ä»¶")
    parser.add_argument("--output", type=str, required=True, help="è¾“å‡ºJSONæ–‡ä»¶")
    parser.add_argument("--model-path", type=str, default=DEFAULT_MODEL_PATH,
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--system-prompt", type=str, default=DEFAULT_SYSTEM_PROMPT,
                       help="ç³»ç»Ÿæç¤ºè¯")
    parser.add_argument("--temperature", type=float, default=0.3,
                       help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top-p", type=float, default=0.9,
                       help="top_pé‡‡æ ·")
    parser.add_argument("--max-tokens", type=int, default=1024,
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--gpu-mem-util", type=float, default=0.9,
                       help="GPUå†…å­˜ä½¿ç”¨ç‡ï¼ˆé»˜è®¤0.9ï¼Œå¦‚æœOOMå¯é™ä½ï¼‰")
    parser.add_argument("--max-model-len", type=int, default=3600,
                       help="æ¨¡å‹æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤3600ï¼Œæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼‰")
    
    args = parser.parse_args()
    
    print("="*60)
    print("å¯¹æ¯”å®éªŒ1ï¼šæµ‹è¯•çº¯LLMï¼ˆæ— RAGï¼‰")
    print("="*60)
    print(f"æ¨¡å‹: {args.model_path}")
    print(f"è¾“å…¥: {args.input}")
    print(f"è¾“å‡º: {args.output}")
    print("="*60)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(args.output)
    if output_dir:  # å¦‚æœæœ‰ç›®å½•è·¯å¾„
        os.makedirs(output_dir, exist_ok=True)
    
    # è°ƒç”¨æµ‹è¯•å‡½æ•°
    test_pure_llm(
        input_file=args.input,
        output_file=args.output,
        model_path=args.model_path,
        system_prompt=args.system_prompt,
        temperature=args.temperature,
        top_p=args.top_p,
        max_tokens=args.max_tokens,
        gpu_mem_util=args.gpu_mem_util,
        max_model_len=args.max_model_len,
    )
    
    print("\nâœ… çº¯LLMæµ‹è¯•å®Œæˆï¼")
    print(f"ç»“æœå·²ä¿å­˜è‡³: {args.output}")


if __name__ == "__main__":
    main()
