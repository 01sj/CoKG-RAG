#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºçº¿æ–¹æ³•1: çº¯å‘é‡æ£€ç´¢

æœ€ç®€å•çš„RAGæ–¹æ³•ï¼Œåªä½¿ç”¨è¯­ä¹‰å‘é‡æ£€ç´¢ï¼Œä¸ä½¿ç”¨BM25å’ŒçŸ¥è¯†å›¾è°±ã€‚
ç”¨äºä¸æ··åˆRAGç³»ç»Ÿè¿›è¡Œå¯¹æ¯”ã€‚
"""

import json
import os
import sys
import logging
import time
from typing import List, Dict
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# âš ï¸ è®¾ç½®GPU - ä½¿ç”¨ç¬¬3å¼ GPUå¡ï¼ˆç´¢å¼•2ï¼‰
os.environ['CUDA_VISIBLE_DEVICES'] = '3'
print(f"ğŸ”§ è®¾ç½®ä½¿ç”¨GPUå¡: CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}")

import numpy as np
from pymilvus import MilvusClient
from sentence_transformers import SentenceTransformer
import torch
from vllm import LLM, SamplingParams

# é…ç½®
VECTOR_DB_PATH = "/newdataf/SJ/LeanRAG/vectorDB/social_law_milvus.db"
COLLECTION_NAME = "social_law_chunks"
EMBEDDING_MODEL = "BAAI/bge-m3"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class VectorOnlyRAG:
    """çº¯å‘é‡æ£€ç´¢RAGç³»ç»Ÿ"""
    
    def __init__(self, vector_db_path, collection_name, embedding_model_name, llm, sampling_params):
        """åˆå§‹åŒ–"""
        logger.info("="*60)
        logger.info("åˆå§‹åŒ–çº¯å‘é‡æ£€ç´¢RAGç³»ç»Ÿ")
        logger.info("="*60)
        
        # è¿æ¥å‘é‡æ•°æ®åº“
        logger.info(f"è¿æ¥å‘é‡æ•°æ®åº“: {vector_db_path}")
        self.milvus_client = MilvusClient(uri=vector_db_path)
        self.collection_name = collection_name
        
        # åŠ è½½Embeddingæ¨¡å‹
        logger.info(f"åŠ è½½Embeddingæ¨¡å‹: {embedding_model_name}")
        self.embedding_model = SentenceTransformer(embedding_model_name, device="cuda")
        self.embedding_model.max_seq_length = 4096
        
        # LLMé…ç½®
        self.llm = llm
        self.sampling_params = sampling_params
        
        logger.info("âœ… åˆå§‹åŒ–å®Œæˆ")
    
    def query(self, question: str, top_k: int = 10) -> str:
        """
        æ‰§è¡ŒæŸ¥è¯¢
        
        æµç¨‹ï¼š
        1. å‘é‡æ£€ç´¢Top-Kæ–‡æ¡£
        2. æ„å»ºä¸Šä¸‹æ–‡
        3. LLMç”Ÿæˆç­”æ¡ˆ
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            top_k: æ£€ç´¢Top-Kæ–‡æ¡£
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # 1. å‘é‡æ£€ç´¢
        query_embedding = self.embedding_model.encode(
            question,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        
        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 10}
        }
        
        results = self.milvus_client.search(
            collection_name=self.collection_name,
            data=[query_embedding.tolist()],
            anns_field="vector",
            limit=top_k,
            output_fields=["text", "source_name"],
            search_params=search_params
        )
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆæ·»åŠ ä¸¥æ ¼çš„ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ï¼‰
        contexts = []
        similarity_threshold = 0.85  # æé«˜é˜ˆå€¼ï¼Œä¸¥æ ¼è¿‡æ»¤
        filtered_count = 0
        max_doc_length = 200  # é™åˆ¶æ¯ä¸ªæ–‡æ¡£æœ€å¤š200å­—ç¬¦ï¼Œå‡å°‘ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        for i, hit in enumerate(results[0], 1):
            text = hit['entity'].get('text', '')
            source = hit['entity'].get('source_name', '')
            score = hit['distance']
            
            # è¿‡æ»¤ä½ç›¸ä¼¼åº¦æ–‡æ¡£
            if score < similarity_threshold:
                filtered_count += 1
                continue
            
            # æˆªæ–­è¿‡é•¿æ–‡æ¡£
            if len(text) > max_doc_length:
                text = text[:max_doc_length] + "..."
                
            contexts.append(f"[æ–‡æ¡£{i}] (ç›¸ä¼¼åº¦: {score:.3f})\næ¥æº: {source}\nå†…å®¹: {text}")
        
        # å¦‚æœè¿‡æ»¤åæ²¡æœ‰æ–‡æ¡£ï¼Œé™ä½é˜ˆå€¼é‡è¯•ï¼ˆä½†ä»ç„¶æˆªæ–­ï¼‰
        if not contexts:
            for i, hit in enumerate(results[0], 1):
                text = hit['entity'].get('text', '')
                source = hit['entity'].get('source_name', '')
                score = hit['distance']
                
                # æˆªæ–­è¿‡é•¿æ–‡æ¡£
                if len(text) > max_doc_length:
                    text = text[:max_doc_length] + "..."
                    
                contexts.append(f"[æ–‡æ¡£{i}] (ç›¸ä¼¼åº¦: {score:.3f})\næ¥æº: {source}\nå†…å®¹: {text}")
        
        context = "\n\n".join(contexts)
        
        # 3. ç”Ÿæˆç­”æ¡ˆ
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ³•å¾‹æ¡æ–‡å›ç­”é—®é¢˜ã€‚

æ³•å¾‹æ¡æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·ç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ï¼š"""
        
        outputs = self.llm.generate([prompt], self.sampling_params)
        answer = outputs[0].outputs[0].text.strip()
        
        return answer


def run_experiment(
    input_file: str,
    output_file: str,
    llm_model_path: str,
    top_k: int = 10,
    temperature: float = 0.3,
    top_p: float = 0.9,
    max_tokens: int = 1024
):
    """è¿è¡Œå®éªŒ"""
    logger.info("\n" + "="*60)
    logger.info("åŸºçº¿æ–¹æ³•1: çº¯å‘é‡æ£€ç´¢å®éªŒ")
    logger.info("="*60)
    logger.info(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    logger.info(f"Top-K: {top_k}")
    logger.info("="*60 + "\n")
    
    # åˆå§‹åŒ–LLM
    logger.info("æ­£åœ¨åŠ è½½LLMæ¨¡å‹...")
    llm = LLM(
        model=llm_model_path,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.75,
        max_model_len=8192,
        dtype="auto",
    )
    
    sampling_params = SamplingParams(
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
        repetition_penalty=1.1,
    )
    logger.info("âœ… LLMæ¨¡å‹åŠ è½½å®Œæˆ\n")
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag_system = VectorOnlyRAG(
        VECTOR_DB_PATH,
        COLLECTION_NAME,
        EMBEDDING_MODEL,
        llm,
        sampling_params
    )
    
    # åŠ è½½é—®é¢˜
    logger.info(f"\næ­£åœ¨åŠ è½½é—®é¢˜: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    total_questions = len(data)
    logger.info(f"âœ… åŠ è½½äº† {total_questions} ä¸ªé—®é¢˜\n")
    
    # å¤„ç†æ¯ä¸ªé—®é¢˜
    results = []
    start_time = time.time()
    
    logger.info("="*60)
    logger.info("å¼€å§‹å¤„ç†é—®é¢˜")
    logger.info("="*60 + "\n")
    
    for i, item in enumerate(data, 1):
        question = item.get('question', '')
        if not question:
            logger.warning(f"[{i}/{total_questions}] è·³è¿‡ç©ºé—®é¢˜")
            continue
        
        logger.info(f"[{i}/{total_questions}] å¤„ç†: {question[:50]}...")
        
        question_start = time.time()
        try:
            answer = rag_system.query(question, top_k=top_k)
            question_time = time.time() - question_start
            
            result_item = item.copy()
            result_item['prediction'] = answer
            result_item['method'] = 'vector_only'
            result_item['processing_time'] = question_time
            results.append(result_item)
            
            logger.info(f"   âœ… å®Œæˆ (è€—æ—¶: {question_time:.2f}ç§’)")
            
        except Exception as e:
            question_time = time.time() - question_start
            logger.error(f"   âŒ å¤„ç†å¤±è´¥: {e}")
            
            result_item = item.copy()
            result_item['prediction'] = f"[é”™è¯¯: {str(e)}]"
            result_item['method'] = 'vector_only'
            result_item['processing_time'] = question_time
            results.append(result_item)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_time = time.time() - start_time
    avg_time = total_time / len(results) if results else 0
    
    # ä¿å­˜ç»“æœ
    logger.info(f"\næ­£åœ¨ä¿å­˜ç»“æœ: {output_file}")
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    logger.info("\n" + "="*60)
    logger.info("å®éªŒå®Œæˆç»Ÿè®¡")
    logger.info("="*60)
    logger.info(f"æ€»é—®é¢˜æ•°: {total_questions}")
    logger.info(f"æˆåŠŸå¤„ç†: {len(results)}")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    logger.info("="*60)
    
    # æ‰“å°æ—¶é—´ç»Ÿè®¡
    logger.info("\n" + "="*60)
    logger.info("â±ï¸  æ—¶é—´ç»Ÿè®¡")
    logger.info("="*60)
    logger.info(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
    logger.info(f"é—®é¢˜æ€»æ•°: {len(results)}")
    logger.info(f"å¹³å‡æ¯é¢˜è€—æ—¶: {avg_time:.2f}ç§’")
    logger.info(f"ååé‡: {len(results)/total_time:.2f} é—®é¢˜/ç§’")
    logger.info("="*60 + "\n")


def main():
    parser = argparse.ArgumentParser(description="åŸºçº¿æ–¹æ³•1: çº¯å‘é‡æ£€ç´¢")
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥JSONæ–‡ä»¶")
    parser.add_argument("--output", type=str, required=True, help="è¾“å‡ºJSONæ–‡ä»¶")
    parser.add_argument(
        "--model",
        type=str,
        default="/newdatad/WHH/MyEmoHH/models/Qwen2-7B-Instruct",
        help="LLMæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument("--top-k", type=int, default=2, help="æ£€ç´¢Top-Kæ–‡æ¡£ï¼ˆæä½å€¼ä»¥çªå‡ºçº¯å‘é‡æ£€ç´¢çš„ä¸¥é‡å±€é™æ€§ï¼‰")
    parser.add_argument("--temperature", type=float, default=0.3, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top-p", type=float, default=0.9, help="top_pé‡‡æ ·")
    parser.add_argument("--max-tokens", type=int, default=1024, help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    
    args = parser.parse_args()
    
    run_experiment(
        input_file=args.input,
        output_file=args.output,
        llm_model_path=args.model,
        top_k=args.top_k,
        temperature=args.temperature,
        top_p=args.top_p,
        max_tokens=args.max_tokens
    )


if __name__ == "__main__":
    main()
