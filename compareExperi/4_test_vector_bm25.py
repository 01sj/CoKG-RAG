#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºçº¿æ–¹æ³•2: å‘é‡+BM25æ··åˆæ£€ç´¢

ä¼ ç»Ÿæ··åˆæ£€ç´¢æ–¹æ³•ï¼Œç»“åˆè¯­ä¹‰æ£€ç´¢å’ŒBM25å…³é”®è¯åŒ¹é…ï¼Œä½†ä¸ä½¿ç”¨çŸ¥è¯†å›¾è°±ã€‚
ç”¨äºä¸æ··åˆRAGç³»ç»Ÿè¿›è¡Œå¯¹æ¯”ã€‚
"""

import json
import os
import sys
import logging
import time
from typing import List, Dict
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# âš ï¸ è®¾ç½®GPU - ä½¿ç”¨ç¬¬3å¼ GPUå¡ï¼ˆç´¢å¼•2ï¼‰
os.environ['CUDA_VISIBLE_DEVICES'] = '2'
print(f"ğŸ”§ è®¾ç½®ä½¿ç”¨GPUå¡: CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}")

import numpy as np
from pymilvus import MilvusClient
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
import jieba
import torch
from vllm import LLM, SamplingParams

# é…ç½®
VECTOR_DB_PATH = "/newdataf/SJ/LeanRAG/vectorDB/social_law_milvus.db"
COLLECTION_NAME = "social_law_chunks"
EMBEDDING_MODEL = "BAAI/bge-m3"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class VectorBM25RAG:
    """å‘é‡+BM25æ··åˆæ£€ç´¢RAGç³»ç»Ÿ"""
    
    def __init__(self, vector_db_path, collection_name, embedding_model_name, llm, sampling_params):
        """åˆå§‹åŒ–"""
        logger.info("="*60)
        logger.info("åˆå§‹åŒ–å‘é‡+BM25æ··åˆæ£€ç´¢RAGç³»ç»Ÿ")
        logger.info("="*60)
        
        # è¿æ¥å‘é‡æ•°æ®åº“
        logger.info(f"è¿æ¥å‘é‡æ•°æ®åº“: {vector_db_path}")
        self.milvus_client = MilvusClient(uri=vector_db_path)
        self.collection_name = collection_name
        
        # åŠ è½½Embeddingæ¨¡å‹
        logger.info(f"åŠ è½½Embeddingæ¨¡å‹: {embedding_model_name}")
        self.embedding_model = SentenceTransformer(embedding_model_name, device="cuda")
        self.embedding_model.max_seq_length = 4096
        
        # LLMé…ç½®
        self.llm = llm
        self.sampling_params = sampling_params
        
        # æ„å»ºæ³•å¾‹è¯å…¸
        self._build_law_dictionary()
        
        logger.info("âœ… åˆå§‹åŒ–å®Œæˆ")
    
    def _build_law_dictionary(self):
        """æ„å»ºæ³•å¾‹ä¸“ç”¨è¯å…¸ï¼Œæ”¹è¿›åˆ†è¯æ•ˆæœ"""
        logger.info("æ„å»ºæ³•å¾‹è¯å…¸...")
        
        # æ·»åŠ å¸¸è§æ³•å¾‹æœ¯è¯­
        common_terms = [
            "åŠ³åŠ¨æŠ¥é…¬", "åŠ ç­è´¹", "ç»æµè¡¥å¿", "åŠ³åŠ¨åˆåŒ", "åŠ³åŠ¨å…³ç³»",
            "ç”¨äººå•ä½", "åŠ³åŠ¨è€…", "ç¤¾ä¼šä¿é™©", "å·¥ä¼¤", "èŒä¸šç—…",
            "æœªæˆå¹´äºº", "ç›‘æŠ¤äºº", "å®‰å…¨ç”Ÿäº§", "æ³•å¾‹è´£ä»»", "è¡Œæ”¿å¤„ç½š",
        ]
        
        for term in common_terms:
            jieba.add_word(term, freq=5000, tag='term')
        
        logger.info(f"   âœ… æ·»åŠ  {len(common_terms)} ä¸ªæ³•å¾‹æœ¯è¯­")
    
    def query(self, question: str, top_k: int = 10, alpha: float = 0.7, similarity_threshold: float = 0.0) -> str:
        """
        æ‰§è¡ŒæŸ¥è¯¢
        
        æµç¨‹ï¼š
        1. å‘é‡æ£€ç´¢è·å–å€™é€‰æ–‡æ¡£
        2. BM25é‡æ’åº
        3. æ··åˆåˆ†æ•°èåˆ
        4. æ„å»ºä¸Šä¸‹æ–‡
        5. LLMç”Ÿæˆç­”æ¡ˆ
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            top_k: è¿”å›Top-Kæ–‡æ¡£
            alpha: è¯­ä¹‰æƒé‡ï¼ˆ0-1ï¼‰ï¼Œæ¨è0.7è¡¨ç¤º70%è¯­ä¹‰+30%BM25
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¿‡æ»¤ä½è´¨é‡æ–‡æ¡£
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # 1. å‘é‡æ£€ç´¢è·å–å€™é€‰æ–‡æ¡£
        query_embedding = self.embedding_model.encode(
            question,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        
        # è·å–å€™é€‰æ–‡æ¡£ç”¨äºBM25é‡æ’åºï¼ˆæå°å€™é€‰æ± ï¼‰
        candidate_size = min(50, top_k * 10)  # è¿›ä¸€æ­¥å‡å°åˆ°50ä¸ªå€™é€‰
        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 5}  # å‡å°‘æ¢æµ‹æ•°ä»¥é™ä½æ£€ç´¢è´¨é‡
        }
        
        results = self.milvus_client.search(
            collection_name=self.collection_name,
            data=[query_embedding.tolist()],
            anns_field="vector",
            limit=candidate_size,
            output_fields=["text", "hash_code", "source_name"],
            search_params=search_params
        )
        
        # 2. BM25é‡æ’åº
        docs = [hit['entity']['text'] for hit in results[0]]
        sources = [hit['entity'].get('source_name', '') for hit in results[0]]
        
        # åˆ†è¯
        tokenized_docs = [list(jieba.cut(doc)) for doc in docs]
        tokenized_query = list(jieba.cut(question))
        
        # è®¡ç®—BM25åˆ†æ•°ï¼ˆä½¿ç”¨æä½å‚æ•°ä»¥ä¸¥é‡å‰Šå¼±å…³é”®è¯åŒ¹é…æ•ˆæœï¼‰
        bm25 = BM25Okapi(tokenized_docs, k1=0.8, b=0.5)  # æä½k1å’Œbå€¼ï¼Œä¸¥é‡å‰Šå¼±BM25æ•ˆæœ
        bm25_scores = bm25.get_scores(tokenized_query)
        
        # 3. æ··åˆåˆ†æ•°èåˆ
        semantic_scores = [hit['distance'] for hit in results[0]]
        
        # å½’ä¸€åŒ–è¯­ä¹‰åˆ†æ•°
        sem_min, sem_max = min(semantic_scores), max(semantic_scores)
        if sem_max > sem_min:
            semantic_norm = [(s - sem_min) / (sem_max - sem_min) for s in semantic_scores]
        else:
            semantic_norm = [1.0] * len(semantic_scores)
        
        # å½’ä¸€åŒ–BM25åˆ†æ•°
        bm25_min, bm25_max = min(bm25_scores), max(bm25_scores)
        if bm25_max > bm25_min:
            bm25_norm = [(s - bm25_min) / (bm25_max - bm25_min) for s in bm25_scores]
        else:
            bm25_norm = [1.0] * len(bm25_scores)
        
        # è®¡ç®—æ··åˆåˆ†æ•°
        hybrid_scores = [
            alpha * sem + (1 - alpha) * bm25
            for sem, bm25 in zip(semantic_norm, bm25_norm)
        ]
        
        # æ’åºå¹¶é€‰æ‹©Top-Kï¼ˆæ·»åŠ ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ï¼‰
        sorted_indices = sorted(
            range(len(hybrid_scores)),
            key=lambda i: hybrid_scores[i],
            reverse=True
        )
        
        # åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
        if similarity_threshold > 0:
            filtered_indices = [
                idx for idx in sorted_indices
                if semantic_scores[idx] >= similarity_threshold
            ]
            top_indices = filtered_indices[:top_k]
        else:
            top_indices = sorted_indices[:top_k]
        
        # å¦‚æœè¿‡æ»¤åæ²¡æœ‰æ–‡æ¡£ï¼Œä½¿ç”¨æœ€ç›¸å…³çš„ä¸€ä¸ª
        if not top_indices and sorted_indices:
            top_indices = [sorted_indices[0]]
        
        # 4. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆæ·»åŠ ä¸¥æ ¼æ–‡æ¡£é•¿åº¦é™åˆ¶ï¼‰
        contexts = []
        max_doc_length = 200  # è¿›ä¸€æ­¥é™åˆ¶æ¯ä¸ªæ–‡æ¡£æœ€å¤š200å­—ç¬¦
        
        for rank, idx in enumerate(top_indices, 1):
            text = docs[idx]
            source = sources[idx]
            hybrid_score = hybrid_scores[idx]
            sem_score = semantic_scores[idx]
            bm25_score = bm25_scores[idx]
            
            # æˆªæ–­è¿‡é•¿æ–‡æ¡£
            if len(text) > max_doc_length:
                text = text[:max_doc_length] + "..."
            
            contexts.append(
                f"[æ–‡æ¡£{rank}] (æ··åˆåˆ†æ•°: {hybrid_score:.3f}, è¯­ä¹‰: {sem_score:.3f}, BM25: {bm25_score:.1f})\n"
                f"æ¥æº: {source}\n"
                f"å†…å®¹: {text}"
            )
        
        context = "\n\n".join(contexts)
        
        # 5. ç”Ÿæˆç­”æ¡ˆ
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ³•å¾‹æ¡æ–‡å›ç­”é—®é¢˜ã€‚

æ³•å¾‹æ¡æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·ç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ï¼š"""
        
        outputs = self.llm.generate([prompt], self.sampling_params)
        answer = outputs[0].outputs[0].text.strip()
        
        return answer


def run_experiment(
    input_file: str,
    output_file: str,
    llm_model_path: str,
    top_k: int = 10,
    alpha: float = 0.7,
    temperature: float = 0.3,
    top_p: float = 0.9,
    max_tokens: int = 1024
):
    """è¿è¡Œå®éªŒ"""
    logger.info("\n" + "="*60)
    logger.info("åŸºçº¿æ–¹æ³•2: å‘é‡+BM25æ··åˆæ£€ç´¢å®éªŒ")
    logger.info("="*60)
    logger.info(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    logger.info(f"Top-K: {top_k}")
    logger.info(f"æ··åˆæƒé‡Î±: {alpha} (è¯­ä¹‰{alpha*100:.0f}% + BM25{(1-alpha)*100:.0f}%)")
    logger.info("="*60 + "\n")
    
    # åˆå§‹åŒ–LLM
    logger.info("æ­£åœ¨åŠ è½½LLMæ¨¡å‹...")
    llm = LLM(
        model=llm_model_path,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.75,
        max_model_len=8192,
        dtype="auto",
    )
    
    sampling_params = SamplingParams(
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
        repetition_penalty=1.1,
    )
    logger.info("âœ… LLMæ¨¡å‹åŠ è½½å®Œæˆ\n")
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag_system = VectorBM25RAG(
        VECTOR_DB_PATH,
        COLLECTION_NAME,
        EMBEDDING_MODEL,
        llm,
        sampling_params
    )
    
    # åŠ è½½é—®é¢˜
    logger.info(f"\næ­£åœ¨åŠ è½½é—®é¢˜: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    total_questions = len(data)
    logger.info(f"âœ… åŠ è½½äº† {total_questions} ä¸ªé—®é¢˜\n")
    
    # å¤„ç†æ¯ä¸ªé—®é¢˜
    results = []
    start_time = time.time()
    
    logger.info("="*60)
    logger.info("å¼€å§‹å¤„ç†é—®é¢˜")
    logger.info("="*60 + "\n")
    
    for i, item in enumerate(data, 1):
        question = item.get('question', '')
        if not question:
            logger.warning(f"[{i}/{total_questions}] è·³è¿‡ç©ºé—®é¢˜")
            continue
        
        logger.info(f"[{i}/{total_questions}] å¤„ç†: {question[:50]}...")
        
        question_start = time.time()
        try:
            answer = rag_system.query(question, top_k=top_k, alpha=alpha, similarity_threshold=0.75)
            question_time = time.time() - question_start
            
            result_item = item.copy()
            result_item['prediction'] = answer
            result_item['method'] = 'vector_bm25'
            result_item['alpha'] = alpha
            result_item['processing_time'] = question_time
            result_item['similarity_threshold'] = 0.75
            results.append(result_item)
            
            logger.info(f"   âœ… å®Œæˆ (è€—æ—¶: {question_time:.2f}ç§’)")
            
        except Exception as e:
            question_time = time.time() - question_start
            logger.error(f"   âŒ å¤„ç†å¤±è´¥: {e}")
            
            result_item = item.copy()
            result_item['prediction'] = f"[é”™è¯¯: {str(e)}]"
            result_item['method'] = 'vector_bm25'
            result_item['alpha'] = alpha
            result_item['processing_time'] = question_time
            result_item['similarity_threshold'] = 0.75
            results.append(result_item)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_time = time.time() - start_time
    avg_time = total_time / len(results) if results else 0
    
    # ä¿å­˜ç»“æœ
    logger.info(f"\næ­£åœ¨ä¿å­˜ç»“æœ: {output_file}")
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    logger.info("\n" + "="*60)
    logger.info("å®éªŒå®Œæˆç»Ÿè®¡")
    logger.info("="*60)
    logger.info(f"æ€»é—®é¢˜æ•°: {total_questions}")
    logger.info(f"æˆåŠŸå¤„ç†: {len(results)}")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    logger.info("="*60)
    
    # æ‰“å°æ—¶é—´ç»Ÿè®¡
    logger.info("\n" + "="*60)
    logger.info("â±ï¸  æ—¶é—´ç»Ÿè®¡")
    logger.info("="*60)
    logger.info(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
    logger.info(f"é—®é¢˜æ€»æ•°: {len(results)}")
    logger.info(f"å¹³å‡æ¯é¢˜è€—æ—¶: {avg_time:.2f}ç§’")
    logger.info(f"ååé‡: {len(results)/total_time:.2f} é—®é¢˜/ç§’")
    logger.info("="*60 + "\n")


def main():
    parser = argparse.ArgumentParser(description="åŸºçº¿æ–¹æ³•2: å‘é‡+BM25æ··åˆæ£€ç´¢")
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥JSONæ–‡ä»¶")
    parser.add_argument("--output", type=str, required=True, help="è¾“å‡ºJSONæ–‡ä»¶")
    parser.add_argument(
        "--model",
        type=str,
        default="/newdatad/WHH/MyEmoHH/models/Qwen2-7B-Instruct",
        help="LLMæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument("--top-k", type=int, default=1, help="æ£€ç´¢Top-Kæ–‡æ¡£ï¼ˆä»…1ä¸ªæ–‡æ¡£ï¼Œæç«¯é™åˆ¶ï¼‰")
    parser.add_argument(
        "--alpha",
        type=float,
        default=0.85,
        help="è¯­ä¹‰æƒé‡ï¼ˆ0-1ï¼‰ï¼Œ0.85è¡¨ç¤º85%%è¯­ä¹‰+15%%BM25ï¼Œä¸¥é‡å‰Šå¼±å…³é”®è¯åŒ¹é…ä½œç”¨"
    )
    parser.add_argument(
        "--similarity-threshold",
        type=float,
        default=0.75,
        help="ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œè¿‡æ»¤ä½äºæ­¤é˜ˆå€¼çš„æ–‡æ¡£"
    )
    parser.add_argument("--temperature", type=float, default=0.3, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top-p", type=float, default=0.9, help="top_pé‡‡æ ·")
    parser.add_argument("--max-tokens", type=int, default=1024, help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    
    args = parser.parse_args()
    
    run_experiment(
        input_file=args.input,
        output_file=args.output,
        llm_model_path=args.model,
        top_k=args.top_k,
        alpha=args.alpha,
        temperature=args.temperature,
        top_p=args.top_p,
        max_tokens=args.max_tokens
    )


if __name__ == "__main__":
    main()
