#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é¢å‘ä¸­æ–‡æ³•å¾‹çŸ¥è¯†å›¾è°±çš„æ£€ç´¢è„šæœ¬

é»˜è®¤ä» /newdataf/SJ/LeanRAG/KG_output/social_law_7B_processed/ è¯»å–ï¼š
  - entity.jsonl / relation.jsonl
  - ä¾èµ–å·²å®Œæˆçš„å±‚çº§èšç±»ã€å…³ç³»ä¸å‘é‡ç´¢å¼•ï¼ˆç”± build_law_graph.py / build_graph.py ç”Ÿæˆï¼‰

åŠŸèƒ½ï¼š
  1) å‘é‡æ£€ç´¢ Top-K å®ä½“/èŠ‚ç‚¹
  2) åŸºäºå€™é€‰å®ä½“æ„é€ æ¨ç†è·¯å¾„å¹¶èšåˆç¤¾åŒºä¿¡æ¯
  3) ç»“åˆ chunks æå–æ–‡æœ¬å•å…ƒ
  4) ç»„ç»‡ä¸Šä¸‹æ–‡äº¤ç»™ LLM ç”Ÿæˆæœ€ç»ˆå›ç­”

æ³¨æ„ï¼šæœ¬è„šæœ¬é»˜è®¤åœ¨ CPU ä¸Šä¸²è¡Œåš embeddingï¼ˆç¨³å®šä¼˜å…ˆï¼‰ã€‚å¦‚éœ€ GPU/å¹¶å‘ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ã€‚


ä»…è‡´çŸ¥è¯†å›¾è°±å›ç­”ï¼Œç”Ÿæˆæ–‡ä»¶æ˜¯social_QWen2_7B_chunks.json
"""

import argparse
import json
import os
import logging
import sys
from datetime import datetime
from itertools import combinations
from collections import defaultdict

# âš ï¸ é‡è¦ï¼šå¿…é¡»åœ¨å¯¼å…¥ vLLM ä¹‹å‰è®¾ç½® CUDA_VISIBLE_DEVICES
# é»˜è®¤ä½¿ç”¨ç¬¬1å¼ GPUå¡ï¼ˆç´¢å¼•0ï¼‰
default_gpu_id = os.environ.get('VLLM_GPU_IDS', '2')
if 'CUDA_VISIBLE_DEVICES' not in os.environ:
    os.environ['CUDA_VISIBLE_DEVICES'] = default_gpu_id
    print(f"ğŸ”§ åœ¨å¯¼å…¥ vLLM ä¹‹å‰è®¾ç½® CUDA_VISIBLE_DEVICES={default_gpu_id}")
else:
    print(f"âœ… ä½¿ç”¨å·²è®¾ç½®çš„ CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}")

import numpy as np
import tiktoken
import torch
from sentence_transformers import SentenceTransformer
from vllm import LLM, SamplingParams

from database_utils import (
    search_vector_search,
    find_tree_root,
    search_nodes_link,
    search_community,
    get_text_units,
)
from prompt import PROMPTS


# ---------- ç¯å¢ƒ/è®¾å¤‡ä¸ embedding ----------
_force_cpu = os.environ.get("FORCE_CPU", "1") == "1"  # ç¼ºçœ CPUï¼Œæ›´ç¨³
_device = "cpu" if _force_cpu else ("cuda" if torch.cuda.is_available() else "cpu")
_st_model_name = os.environ.get("EMBEDDING_MODEL", "BAAI/bge-m3")

try:
    _ST_EMB = SentenceTransformer(_st_model_name, device=_device)
except Exception as e:
    print(f"Failed to load {_st_model_name} on {_device}: {e}")
    print("Falling back to CPU + BAAI/bge-m3")
    _device = "cpu"
    _ST_EMB = SentenceTransformer("BAAI/bge-m3", device=_device)

_ST_EMB.max_seq_length = 4096

_emb_batch = max(1, int(os.environ.get("EMB_BATCH", "8")))

tokenizer = tiktoken.get_encoding("cl100k_base")


def truncate_text(text: str, max_tokens: int = 4096) -> str:
    tokens = tokenizer.encode(text)
    if len(tokens) > max_tokens:
        tokens = tokens[:max_tokens]
    return tokenizer.decode(tokens)


def embedding(texts) -> np.ndarray:
    if isinstance(texts, str):
        texts = [texts]
    batch_size = max(1, min(_emb_batch, len(texts)))
    vectors = _ST_EMB.encode(
        texts,
        batch_size=batch_size,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False,
    )
    result = np.array(vectors)
    if len(texts) == 1:
        return [result[0].tolist()]
    return result


# ---------- æ¨ç†é“¾ä¸ç¤¾åŒºèšåˆ ----------
def get_reasoning_chain(working_dir: str, entities_set: list[str]):
    """æ„å»ºæ¨ç†è·¯å¾„ï¼ˆå¹¶è¡Œä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨çº¿ç¨‹æ± åŠ é€Ÿæ•°æ®åº“æŸ¥è¯¢ï¼‰"""
    import time
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from threading import Lock
    
    start_time = time.time()
    
    maybe_edges = list(combinations(entities_set, 2))
    reasoning_path = []
    reasoning_path_information = []
    db_name = os.path.basename(working_dir.rstrip("/"))
    
    # çº¿ç¨‹å®‰å…¨çš„ç¼“å­˜
    tree_cache = {}
    link_cache = {}
    tree_lock = Lock()
    link_lock = Lock()
    
    def get_tree_root_cached(entity):
        with tree_lock:
            if entity not in tree_cache:
                tree_cache[entity] = find_tree_root(db_name, entity)
            return tree_cache[entity]
    
    def get_link_cached(e1, e2):
        key = tuple(sorted([e1, e2]))
        with link_lock:
            if key not in link_cache:
                link_cache[key] = search_nodes_link(e1, e2, working_dir)
            return link_cache[key]
    
    print(f"   éœ€è¦å¤„ç† {len(maybe_edges)} ä¸ªå®ä½“å¯¹...")
    print(f"   ğŸš€ ä½¿ç”¨å¹¶è¡Œå¤„ç†åŠ é€Ÿ...")
    
    # æ­¥éª¤1ï¼šå¹¶è¡Œé¢„åŠ è½½æ‰€æœ‰å®ä½“çš„ tree_rootï¼ˆå¤§å¹…å‡å°‘ç­‰å¾…æ—¶é—´ï¼‰
    print(f"   ğŸ“¥ é¢„åŠ è½½å®ä½“æ ‘ç»“æ„...")
    unique_entities = list(set(entities_set))
    max_workers = 16  # å¢åŠ åˆ°16ä¸ªçº¿ç¨‹ï¼Œå……åˆ†åˆ©ç”¨CPU
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(get_tree_root_cached, entity): entity for entity in unique_entities}
        for future in as_completed(futures):
            pass  # åªæ˜¯ä¸ºäº†è§¦å‘ç¼“å­˜
    print(f"   âœ… é¢„åŠ è½½å®Œæˆï¼Œå·²ç¼“å­˜ {len(tree_cache)} ä¸ªå®ä½“")
    
    # æ­¥éª¤2ï¼šå¤„ç†æ¯ä¸ªå®ä½“å¯¹
    def process_edge(edge_idx_tuple):
        idx, edge = edge_idx_tuple
        a_path = []
        b_path = []
        node1, node2 = edge
        
        # ä»ç¼“å­˜è·å–ï¼ˆå·²é¢„åŠ è½½ï¼‰
        node1_tree = get_tree_root_cached(node1)
        node2_tree = get_tree_root_cached(node2)

        for i, j in zip(node1_tree, node2_tree):
            if i == j:
                a_path.append(i)
                break
            if i in b_path or j in a_path:
                break
            if i != j:
                a_path.append(i)
                b_path.append(j)

        path = a_path + [b_path[len(b_path) - 1 - i] for i in range(len(b_path))]
        a_path = list(set(a_path))
        b_path = list(set(b_path))
        
        # é™åˆ¶ç»„åˆæ•°é‡ï¼ˆæ¿€è¿›ä¼˜åŒ–ï¼šé™ä½åˆ°5ä¸ªèŠ‚ç‚¹ï¼‰
        all_nodes = a_path + b_path
        if len(all_nodes) > 5:  # é™ä½åˆ°5ï¼Œå¤§å¹…åŠ å¿«é€Ÿåº¦
            all_nodes = all_nodes[:5]
        
        # æ”¶é›†éœ€è¦æŸ¥è¯¢çš„è¾¹
        edges_to_query = []
        for maybe_edge in combinations(all_nodes, 2):
            if maybe_edge[0] != maybe_edge[1]:
                edges_to_query.append(maybe_edge)
        
        return idx, path, edges_to_query
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰å®ä½“å¯¹
    print(f"   ğŸ”— æ„å»ºæ¨ç†è·¯å¾„...")
    edge_results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_edge, (idx, edge)): idx for idx, edge in enumerate(maybe_edges)}
        completed = 0
        for future in as_completed(futures):
            completed += 1
            if completed % 10 == 0:
                elapsed = time.time() - start_time
                print(f"   è¿›åº¦: {completed}/{len(maybe_edges)} ({completed/len(maybe_edges)*100:.1f}%), è€—æ—¶: {elapsed:.1f}s")
            edge_results.append(future.result())
    
    # æ­¥éª¤3ï¼šå¹¶è¡ŒæŸ¥è¯¢æ‰€æœ‰å…³ç³»
    print(f"   ğŸ” æŸ¥è¯¢å®ä½“å…³ç³»...")
    all_edges_to_query = []
    for idx, path, edges in edge_results:
        reasoning_path.append(path)
        all_edges_to_query.extend(edges)
    
    # å»é‡
    unique_edges = list(set(all_edges_to_query))
    print(f"   éœ€è¦æŸ¥è¯¢ {len(unique_edges)} ä¸ªå…³ç³»...")
    
    # å¹¶è¡ŒæŸ¥è¯¢å…³ç³»
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(get_link_cached, e1, e2): (e1, e2) for e1, e2 in unique_edges}
        completed = 0
        for future in as_completed(futures):
            completed += 1
            if completed % 50 == 0:
                print(f"   å…³ç³»æŸ¥è¯¢è¿›åº¦: {completed}/{len(unique_edges)} ({completed/len(unique_edges)*100:.1f}%)")
            e1, e2 = futures[future]
            info = future.result()
            if info is not None:
                reasoning_path_information.append([e1, e2, info[2]])
    
    temp_relations_information = list(set([info[2] for info in reasoning_path_information]))
    reasoning_path_information_description = "\n".join(temp_relations_information)
    
    elapsed = time.time() - start_time
    print(f"   âœ… æ¨ç†è·¯å¾„æ„å»ºå®Œæˆï¼Œè€—æ—¶: {elapsed:.1f}s")
    print(f"   ğŸ“Š ç»Ÿè®¡: tree_root={len(tree_cache)} ä¸ªå®ä½“, links={len(link_cache)} ä¸ªå…³ç³»")
    
    return reasoning_path, reasoning_path_information_description


def get_aggregation_description(working_dir: str, reasoning_path):
    aggregation_results = []
    communities = set([community for each_path in reasoning_path for community in each_path])
    for community in communities:
        temp = search_community(community, working_dir)
        if temp == "":
            continue
        aggregation_results.append(temp)

    columns = ["entity_name", "entity_description"]
    aggregation_descriptions = "\t\t".join(columns) + "\n"
    aggregation_descriptions += "\n".join([info[0] + "\t\t" + str(info[1]) for info in aggregation_results])
    return aggregation_descriptions, communities


def get_entity_description(entity_results: list[tuple]):
    columns = ["entity_name", "parent", "description"]
    entity_descriptions = "\t\t".join(columns) + "\n"
    entity_descriptions += "\n".join([info[0] + "\t\t" + info[1] + "\t\t" + info[2] for info in entity_results])
    return entity_descriptions


def query_law_graph(global_config: dict, query: str):
    use_llm_func = global_config["use_llm_func"]
    working_dir = global_config["working_dir"]
    level_mode = global_config.get("level_mode", 1)
    topk = global_config.get("topk", 10)
    chunks_file = global_config.get("chunks_file")
    fast_mode = global_config.get("fast_mode", False)  # å¿«é€Ÿæ¨¡å¼

    print(f"\n{'='*60}")
    print(f"ğŸ” å¼€å§‹å¤„ç†æŸ¥è¯¢: {query[:50]}...")
    if fast_mode:
        print(f"âš¡ å¿«é€Ÿæ¨¡å¼å·²å¯ç”¨ï¼ˆå‡å°‘æ¨ç†è·¯å¾„å¤æ‚åº¦ï¼‰")
    print(f"{'='*60}")
    
    print("ğŸ“Š æ­¥éª¤ 1/6: å‘é‡æ£€ç´¢å®ä½“...")
    entity_results = search_vector_search(working_dir, embedding(query), topk=topk, level_mode=level_mode)
    res_entity = [i[0] for i in entity_results]
    chunks = [i[-1] for i in entity_results]
    print(f"   âœ… æ£€ç´¢åˆ° {len(res_entity)} ä¸ªç›¸å…³å®ä½“")

    print("ğŸ“ æ­¥éª¤ 2/6: ç”Ÿæˆå®ä½“æè¿°...")
    entity_descriptions = get_entity_description(entity_results)
    print(f"   âœ… å®Œæˆ")
    
    print("ğŸ”— æ­¥éª¤ 3/6: æ„å»ºæ¨ç†è·¯å¾„...")
    # å¿«é€Ÿæ¨¡å¼ï¼šåªä½¿ç”¨å‰5ä¸ªæœ€ç›¸å…³çš„å®ä½“
    if fast_mode and len(res_entity) > 5:
        print(f"   âš¡ å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨å‰5ä¸ªæœ€ç›¸å…³å®ä½“ï¼ˆåŸ{len(res_entity)}ä¸ªï¼‰")
        res_entity = res_entity[:5]
    reasoning_path, reasoning_path_information_description = get_reasoning_chain(working_dir, res_entity)
    print(f"   âœ… æ„å»ºäº† {len(reasoning_path)} æ¡æ¨ç†è·¯å¾„")
    
    print("ğŸ˜ï¸  æ­¥éª¤ 4/6: èšåˆç¤¾åŒºä¿¡æ¯...")
    aggregation_descriptions, aggregation = get_aggregation_description(working_dir, reasoning_path)
    print(f"   âœ… èšåˆäº† {len(aggregation)} ä¸ªç¤¾åŒº")
    
    print("ğŸ“„ æ­¥éª¤ 5/6: æå–æ–‡æœ¬å•å…ƒ...")
    text_units = get_text_units(working_dir, chunks, chunks_file, k=5)
    print(f"   âœ… å®Œæˆ")

    describe = f"""
    entity_information:
    {entity_descriptions}
    aggregation_entity_information:
    {aggregation_descriptions}
    reasoning_path_information:
    {reasoning_path_information_description}
    text_units:
    {text_units}
    """

    # ä½¿ç”¨ä¸­æ–‡æç¤ºè¯ï¼ˆé’ˆå¯¹ä¸­æ–‡æ³•å¾‹é—®ç­”ä¼˜åŒ–ï¼‰
    sys_prompt = PROMPTS["rag_response_zh"].format(context_data=describe)
    
    print("ğŸ¤– æ­¥éª¤ 6/6: LLM ç”Ÿæˆç­”æ¡ˆ...")
    response = use_llm_func(query, system_prompt=sys_prompt)
    print(f"   âœ… ç”Ÿæˆå®Œæˆ")
    return describe, response


def setup_logging(log_dir="/newdataf/SJ/LeanRAG/logs"):
    """
    é…ç½®æ—¥å¿—ç³»ç»Ÿ
    
    å‚æ•°:
        log_dir: æ—¥å¿—ç›®å½•
    
    è¿”å›:
        logger: æ—¥å¿—è®°å½•å™¨
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs(log_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"query_{timestamp}.log")
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    log_format = '%(asctime)s - %(levelname)s - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨ï¼ˆé¿å…é‡å¤ï¼‰
    root_logger = logging.getLogger()
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        datefmt=date_format,
        handlers=[
            # æ–‡ä»¶å¤„ç†å™¨
            logging.FileHandler(log_file, encoding='utf-8'),
            # æ§åˆ¶å°å¤„ç†å™¨
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info("="*60)
    
    return logger, log_file


def main():
    # è®¾ç½®æ—¥å¿—
    logger, log_file = setup_logging()
    logger.info("ç¤¾ä¼šæ³•çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç³»ç»Ÿå¯åŠ¨")
    
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-p",
        "--path",
        type=str,
        default="/newdataf/SJ/LeanRAG/KG_output/social_law_7B_processed/",
        help="æ³•å¾‹çŸ¥è¯†å›¾è°±å·¥ä½œç›®å½•ï¼ˆåŒ…å«ç”Ÿæˆçš„å‘é‡/ç¤¾åŒº/å…³ç³»ç­‰ï¼‰",
    )
    parser.add_argument(
        "--chunks",
        type=str,
        default=None,
        help="chunks.json è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æä¾›ä¼šå½±å“æ–‡æœ¬è¯æ®æå–ï¼‰",
    )
    parser.add_argument("-q", "--query", type=str, required=False, help="æŸ¥è¯¢é—®é¢˜ï¼ˆä¸­æ–‡/è‹±æ–‡å‡å¯ï¼‰ã€‚è‹¥æä¾› --input-json å¯ä¸å¡«")
    parser.add_argument("--input-json", type=str, default=None, help="æ‰¹é‡æŸ¥è¯¢çš„è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„ï¼ˆæ•°ç»„ï¼Œæ¯æ¡åŒ…å« question å­—æ®µï¼‰")
    parser.add_argument("--output-json", type=str, default=None, help="æ‰¹é‡æŸ¥è¯¢çš„è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ˆå†™å…¥ prediction å­—æ®µï¼‰")
    parser.add_argument("-k", "--topk", type=int, default=10, help="æ£€ç´¢ Top-K å®ä½“/èŠ‚ç‚¹")
    parser.add_argument("-l", "--level", type=int, default=0, help="æ£€ç´¢å±‚çº§ï¼š0åŸå§‹èŠ‚ç‚¹/1èšåˆèŠ‚ç‚¹/2å…¨éƒ¨")
    parser.add_argument("--fast", action="store_true", help="å¿«é€Ÿæ¨¡å¼ï¼šå‡å°‘æ¨ç†è·¯å¾„å¤æ‚åº¦ï¼Œå¤§å¹…æå‡é€Ÿåº¦ï¼ˆæ¨èï¼‰")
    parser.add_argument("--model", type=str, default=os.environ.get("VLLM_MODEL", "/newdatad/WHH/MyEmoHH/models/Qwen2-7B-Instruct"), help="æœ¬åœ°æ¨¡å‹è·¯å¾„")
    parser.add_argument("--tp", type=int, default=int(os.environ.get("VLLM_TP", "1")), help="tensor_parallel_sizeï¼ˆé»˜è®¤1å•å¡ï¼‰")
    parser.add_argument("--gpu-mem-util", type=float, default=float(os.environ.get("VLLM_GPU_MEM_UTIL", "0.75")), help="æ¯å¼  GPU ç›®æ ‡æ˜¾å­˜å ç”¨æ¯”ä¾‹")
    parser.add_argument("--max-model-len", type=int, default=int(os.environ.get("VLLM_MAX_MODEL_LEN", "8192")), help="æœ€å¤§æ¨¡å‹åºåˆ—é•¿åº¦")
    parser.add_argument("--max-new-tokens", type=int, default=int(os.environ.get("VLLM_MAX_NEW_TOKENS", "1024")), help="ç”Ÿæˆçš„æœ€å¤§æ–°tokenæ•°")
    parser.add_argument("--temperature", type=float, default=float(os.environ.get("VLLM_TEMPERATURE", "0.3")), help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top-p", type=float, default=float(os.environ.get("VLLM_TOP_P", "0.9")), help="top_p é‡‡æ ·é˜ˆå€¼")
    args = parser.parse_args()

    working_dir = args.path.rstrip("/")
    chunks_file = args.chunks

    # æ£€æŸ¥ GPU é…ç½®ï¼ˆCUDA_VISIBLE_DEVICES å·²åœ¨æ–‡ä»¶å¼€å¤´è®¾ç½®ï¼‰
    cuda_devices = os.environ.get("CUDA_VISIBLE_DEVICES", "1")
    visible_gpu_count = len([x for x in cuda_devices.split(",") if x.strip()])
    print(f"âœ… å½“å‰ä½¿ç”¨ GPU: {cuda_devices}ï¼ˆå…± {visible_gpu_count} å¼ å¯è§GPUï¼‰")
    
    # è‡ªåŠ¨è°ƒæ•´ tensor_parallel_size
    if visible_gpu_count < args.tp:
        print(f"âš ï¸  å¯è§GPUæ•°é‡ ({visible_gpu_count}) å°‘äº tp ({args.tp})ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º {visible_gpu_count}")
        args.tp = max(1, visible_gpu_count)

    # vLLM æœ¬åœ°æ¨¡å‹å®ä¾‹
    llm = LLM(
        model=args.model,
        tensor_parallel_size=max(1, args.tp),
        gpu_memory_utilization=max(0.1, min(0.95, args.gpu_mem_util)),
        max_model_len=args.max_model_len,
        dtype="auto",
    )
    sampling_params = SamplingParams(
        temperature=max(0.0, args.temperature),
        top_p=min(1.0, max(0.0, args.top_p)),
        max_tokens=max(1, args.max_new_tokens),
        repetition_penalty=1.1,
        stop=None,  # ä¸è®¾ç½®åœæ­¢è¯ï¼Œè®©æ¨¡å‹è‡ªç„¶ç»“æŸ
    )
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"ğŸ“Š é‡‡æ ·å‚æ•°: temperature={args.temperature}, top_p={args.top_p}, max_tokens={args.max_new_tokens}")

    def vllm_generate_text(user_prompt: str, system_prompt: str = ""):
        try:
            # æ„å»ºæç¤ºè¯
            if system_prompt:
                composed = f"{system_prompt}\n\n{user_prompt}"
            else:
                composed = user_prompt
            
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            print(f"\n[DEBUG] æç¤ºè¯é•¿åº¦: {len(composed)} å­—ç¬¦")
            print(f"[DEBUG] å‰100å­—ç¬¦: {composed[:100]}...")
            
            # ç”Ÿæˆ
            outputs = llm.generate([composed], sampling_params=sampling_params)
            
            if not outputs:
                print("[ERROR] vLLM è¿”å›ç©ºè¾“å‡º")
                return ""
            
            if not outputs[0].outputs:
                print("[ERROR] vLLM è¾“å‡ºåˆ—è¡¨ä¸ºç©º")
                return ""
            
            result = outputs[0].outputs[0].text.strip()
            print(f"[DEBUG] LLM ç”Ÿæˆé•¿åº¦: {len(result)} å­—ç¬¦")
            
            if not result:
                print("[WARNING] LLM ç”Ÿæˆäº†ç©ºå­—ç¬¦ä¸²")
                return "æŠ±æ­‰ï¼Œæ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚"
            
            return result
            
        except Exception as e:
            print(f"[ERROR] LLM ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"

    global_config = {
        "working_dir": working_dir,
        "chunks_file": chunks_file,
        "embeddings_func": embedding,
        "use_llm_func": vllm_generate_text,
        "topk": max(1, args.topk),
        "level_mode": max(0, min(2, args.level)),
        "fast_mode": args.fast,  # å¿«é€Ÿæ¨¡å¼
    }

    # æ‰¹å¤„ç†æ¨¡å¼ï¼ˆä¼˜å…ˆï¼‰
    if args.input_json:
        import time
        
        input_path = args.input_json
        output_path = args.output_json or (os.path.splitext(input_path)[0] + "_pred.json")
        with open(input_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if not isinstance(data, list):
            raise ValueError("è¾“å…¥ JSON å¿…é¡»æ˜¯æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåŒ…å« question çš„å¯¹è±¡")
        
        results = []
        total_questions = len(data)
        processed_questions = 0
        
        logger.info("="*60)
        logger.info(f"ğŸ“Š å¼€å§‹æ‰¹é‡å¤„ç†: å…± {total_questions} ä¸ªé—®é¢˜")
        logger.info("="*60)
        
        # è®°å½•æ€»å¼€å§‹æ—¶é—´
        batch_start_time = time.time()
        
        for idx, item in enumerate(data, 1):
            question = item.get("question", "").strip()
            if not question:
                # ç©ºé—®é¢˜ç›´æ¥é€ä¼ 
                new_item = dict(item)
                new_item["prediction"] = ""
                results.append(new_item)
                logger.info(f"[{idx}/{total_questions}] è·³è¿‡ç©ºé—®é¢˜")
                continue
            
            # è®°å½•å•ä¸ªé—®é¢˜å¼€å§‹æ—¶é—´
            question_start_time = time.time()
            logger.info(f"\n[{idx}/{total_questions}] å¤„ç†é—®é¢˜: {question[:50]}...")
            
            _, resp = query_law_graph(global_config, question)
            new_item = dict(item)
            new_item["prediction"] = resp
            results.append(new_item)
            
            # è®¡ç®—å•ä¸ªé—®é¢˜è€—æ—¶
            question_elapsed = time.time() - question_start_time
            processed_questions += 1
            logger.info(f"[{idx}/{total_questions}] âœ… å®Œæˆï¼Œè€—æ—¶: {question_elapsed:.2f}ç§’")
        
        # è®¡ç®—æ€»è€—æ—¶
        batch_end_time = time.time()
        total_elapsed = batch_end_time - batch_start_time
        avg_time_per_question = total_elapsed / processed_questions if processed_questions > 0 else 0
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆç»Ÿè®¡")
        logger.info("="*60)
        logger.info(f"æ€»é—®é¢˜æ•°: {total_questions}")
        logger.info(f"å¤„ç†é—®é¢˜æ•°: {processed_questions}")
        logger.info(f"è·³è¿‡é—®é¢˜æ•°: {total_questions - processed_questions}")
        logger.info(f"æ€»è€—æ—¶: {total_elapsed:.2f} ç§’ ({total_elapsed/60:.2f} åˆ†é’Ÿ)")
        logger.info(f"å¹³å‡æ¯ä¸ªé—®é¢˜è€—æ—¶: {avg_time_per_question:.2f} ç§’")
        logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
        logger.info("="*60)
        
        print(f"\nå·²å†™å…¥ç»“æœ: {output_path}")
        print(f"æ€»è€—æ—¶: {total_elapsed:.2f}ç§’, å¹³å‡æ¯é¢˜: {avg_time_per_question:.2f}ç§’")
        return

    # å•é—®æ¨¡å¼
    if not args.query:
        raise SystemExit("å¿…é¡»æä¾› -q/--queryï¼Œæˆ–æä¾› --input-json è¿›è¡Œæ‰¹å¤„ç†")
    
    import time
    
    logger.info("="*60)
    logger.info("ğŸ“Š å•é—®é¢˜æŸ¥è¯¢æ¨¡å¼")
    logger.info("="*60)
    logger.info(f"æŸ¥è¯¢é—®é¢˜: {args.query}")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    query_start_time = time.time()
    
    ref, resp = query_law_graph(global_config, args.query)
    
    # è®¡ç®—è€—æ—¶
    query_elapsed = time.time() - query_start_time
    
    logger.info("\n[Retrieved Context]\n" + ref)
    logger.info("\n" + "#" * 50)
    logger.info("\n[LLM Response]\n" + str(resp))
    
    print("\n[Retrieved Context]\n" + ref)
    print("\n" + "#" * 50)
    print("\n[LLM Response]\n" + str(resp))
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š æŸ¥è¯¢å®Œæˆç»Ÿè®¡")
    logger.info("="*60)
    logger.info(f"æ€»è€—æ—¶: {query_elapsed:.2f} ç§’ ({query_elapsed/60:.2f} åˆ†é’Ÿ)")
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info("="*60)


if __name__ == "__main__":
    main()


