#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é¢å‘ä¸­æ–‡æ³•å¾‹çŸ¥è¯†å›¾è°±çš„æ£€ç´¢è„šæœ¬

é»˜è®¤ä» /newdataf/SJ/LeanRAG/law_kg_output_processed/ è¯»å–ï¼š
  - entity.jsonl / relation.jsonl
  - ä¾èµ–å·²å®Œæˆçš„å±‚çº§èšç±»ã€å…³ç³»ä¸å‘é‡ç´¢å¼•ï¼ˆç”± build_law_graph.py / build_graph.py ç”Ÿæˆï¼‰

åŠŸèƒ½ï¼š
  1) å‘é‡æ£€ç´¢ Top-K å®ä½“/èŠ‚ç‚¹
  2) åŸºäºå€™é€‰å®ä½“æ„é€ æ¨ç†è·¯å¾„å¹¶èšåˆç¤¾åŒºä¿¡æ¯
  3) ç»“åˆ chunks æå–æ–‡æœ¬å•å…ƒ
  4) ç»„ç»‡ä¸Šä¸‹æ–‡äº¤ç»™ LLM ç”Ÿæˆæœ€ç»ˆå›ç­”

æ³¨æ„ï¼šæœ¬è„šæœ¬é»˜è®¤åœ¨ CPU ä¸Šä¸²è¡Œåš embeddingï¼ˆç¨³å®šä¼˜å…ˆï¼‰ã€‚å¦‚éœ€ GPU/å¹¶å‘ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ã€‚
"""

import argparse
import json
import os
from itertools import combinations
from collections import defaultdict

import numpy as np
import tiktoken
import torch
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

from vllm import LLM, SamplingParams
from database_utils import (
    search_vector_search,
    find_tree_root,
    search_nodes_link,
    search_community,
    get_text_units,
)
from prompt import PROMPTS


# ---------- ç¯å¢ƒ/è®¾å¤‡ä¸ embedding ----------
_force_cpu = os.environ.get("FORCE_CPU", "1") == "1"  # ç¼ºçœ CPUï¼Œæ›´ç¨³
_device = "cpu" if _force_cpu else ("cuda" if torch.cuda.is_available() else "cpu")
_st_model_name = os.environ.get("EMBEDDING_MODEL", "BAAI/bge-m3")

try:
    _ST_EMB = SentenceTransformer(_st_model_name, device=_device)
except Exception as e:
    print(f"Failed to load {_st_model_name} on {_device}: {e}")
    print("Falling back to CPU + BAAI/bge-m3")
    _device = "cpu"
    _ST_EMB = SentenceTransformer("BAAI/bge-m3", device=_device)

_ST_EMB.max_seq_length = 4096

_emb_batch = max(1, int(os.environ.get("EMB_BATCH", "8")))

tokenizer = tiktoken.get_encoding("cl100k_base")


def truncate_text(text: str, max_tokens: int = 4096) -> str:
    tokens = tokenizer.encode(text)
    if len(tokens) > max_tokens:
        tokens = tokens[:max_tokens]
    return tokenizer.decode(tokens)


def embedding(texts) -> np.ndarray:
    if isinstance(texts, str):
        texts = [texts]
    batch_size = max(1, min(_emb_batch, len(texts)))
    vectors = _ST_EMB.encode(
        texts,
        batch_size=batch_size,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False,
    )
    result = np.array(vectors)
    if len(texts) == 1:
        return [result[0].tolist()]
    return result


# ---------- æ¨ç†é“¾ä¸ç¤¾åŒºèšåˆ ----------
def get_reasoning_chain(working_dir: str, entities_set: list[str]):
    maybe_edges = list(combinations(entities_set, 2))
    reasoning_path = []
    reasoning_path_information = []
    db_name = os.path.basename(working_dir.rstrip("/"))

    for edge in maybe_edges:
        a_path = []
        b_path = []
        node1 = edge[0]
        node2 = edge[1]
        node1_tree = find_tree_root(db_name, node1)
        node2_tree = find_tree_root(db_name, node2)

        for i, j in zip(node1_tree, node2_tree):
            if i == j:
                a_path.append(i)
                break
            if i in b_path or j in a_path:
                break
            if i != j:
                a_path.append(i)
                b_path.append(j)

        reasoning_path.append(a_path + [b_path[len(b_path) - 1 - i] for i in range(len(b_path))])
        a_path = list(set(a_path))
        b_path = list(set(b_path))
        for maybe_edge in list(combinations(a_path + b_path, 2)):
            if maybe_edge[0] == maybe_edge[1]:
                continue
            information = search_nodes_link(maybe_edge[0], maybe_edge[1], working_dir)
            if information is None:
                continue
            reasoning_path_information.append([maybe_edge[0], maybe_edge[1], information[2]])

    temp_relations_information = list(set([info[2] for info in reasoning_path_information]))
    reasoning_path_information_description = "\n".join(temp_relations_information)
    return reasoning_path, reasoning_path_information_description


def get_aggregation_description(working_dir: str, reasoning_path):
    aggregation_results = []
    communities = set([community for each_path in reasoning_path for community in each_path])
    for community in communities:
        temp = search_community(community, working_dir)
        if temp == "":
            continue
        aggregation_results.append(temp)

    columns = ["entity_name", "entity_description"]
    aggregation_descriptions = "\t\t".join(columns) + "\n"
    aggregation_descriptions += "\n".join([info[0] + "\t\t" + str(info[1]) for info in aggregation_results])
    return aggregation_descriptions, communities


def search_article_in_chunks(chunks_file: str, article_pattern: str, max_results: int = 5) -> list[dict]:
    """
    åœ¨æ–‡æœ¬å—ä¸­ç›´æ¥æœç´¢åŒ…å«æŒ‡å®šæ³•æ¡çš„å†…å®¹
    
    å‚æ•°:
        chunks_file: æ–‡æœ¬å—æ–‡ä»¶è·¯å¾„
        article_pattern: æ³•æ¡æ¨¡å¼ï¼ˆå¦‚"ç¬¬ä¸€æ¡"ã€"ç¬¬äºŒåä¸‰æ¡"ï¼‰
        max_results: æœ€å¤§è¿”å›ç»“æœæ•°
    
    è¿”å›:
        åŒ…å«æ³•æ¡çš„æ–‡æœ¬å—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« hash_code å’Œ text
    """
    import json
    import re
    
    results = []
    
    if not os.path.exists(chunks_file):
        return results
    
    try:
        with open(chunks_file, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
        if isinstance(chunks_data, list):
            chunks_list = chunks_data
        elif isinstance(chunks_data, dict):
            chunks_list = [{"hash_code": k, "text": v} for k, v in chunks_data.items()]
        else:
            return results
        
        # åœ¨æ–‡æœ¬å—ä¸­æœç´¢åŒ…å«æ³•æ¡çš„å†…å®¹
        for chunk in chunks_list:
            if not isinstance(chunk, dict):
                continue
            
            text = chunk.get("text", "")
            hash_code = chunk.get("hash_code", "")
            
            # æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«æ³•æ¡æ¨¡å¼
            # æ”¯æŒå¤šç§æ ¼å¼ï¼šç¬¬ä¸€æ¡ã€ç¬¬1æ¡ã€ç¬¬ä¸€æ¡ï¼ˆæ ‡é¢˜ï¼‰ç­‰
            if article_pattern in text:
                # æå–åŒ…å«æ³•æ¡çš„å®Œæ•´æ®µè½
                # å°è¯•æå–æ³•æ¡åŠå…¶åç»­å†…å®¹ï¼ˆç›´åˆ°ä¸‹ä¸€ä¸ªæ³•æ¡æˆ–æ®µè½ç»“æŸï¼‰
                # ä½¿ç”¨ä¼ å…¥çš„ article_pattern ä½œä¸ºèµ·ç‚¹ï¼ŒåŒ¹é…åˆ°ä¸‹ä¸€ä¸ªæ³•æ¡æˆ–æ–‡æœ¬ç»“æŸ
                pattern = re.escape(article_pattern) + r'[^ç¬¬]*?(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶\d]+æ¡|$)'
                match = re.search(pattern, text, re.DOTALL)
                
                if match:
                    article_text = match.group(0).strip()
                    results.append({
                        "hash_code": hash_code,
                        "text": article_text,
                        "match_type": "direct_search"
                    })
                else:
                    # å¦‚æœæ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œè‡³å°‘åŒ…å«æ³•æ¡æ¨¡å¼çš„æ–‡æœ¬å—
                    results.append({
                        "hash_code": hash_code,
                        "text": text,
                        "match_type": "contains_pattern"
                    })
                
                if len(results) >= max_results:
                    break
        
        # æŒ‰ç›¸å…³æ€§æ’åºï¼ˆåŒ…å«å®Œæ•´æ³•æ¡å†…å®¹çš„ä¼˜å…ˆï¼‰
        results.sort(key=lambda x: 1 if x["match_type"] == "direct_search" else 2)
        
    except Exception as e:
        print(f"Warning: æœç´¢æ³•æ¡æ—¶å‡ºé”™: {e}")
    
    return results


def get_entity_description(entity_results: list[tuple]):
    columns = ["entity_name", "parent", "description"]
    entity_descriptions = "\t\t".join(columns) + "\n"
    entity_descriptions += "\n".join([info[0] + "\t\t" + info[1] + "\t\t" + info[2] for info in entity_results])
    return entity_descriptions


def query_law_graph(global_config: dict, query: str, return_structured: bool = False):
    """
    æŸ¥è¯¢æ³•å¾‹çŸ¥è¯†å›¾è°±å¹¶ç”Ÿæˆç­”æ¡ˆ
    
    å‚æ•°:
        global_config: å…¨å±€é…ç½®å­—å…¸
        query: æŸ¥è¯¢é—®é¢˜
        return_structured: æ˜¯å¦è¿”å›ç»“æ„åŒ–ç»“æœï¼ˆåŒ…å«æ–‡æœ¬å—ç­‰è¯¦ç»†ä¿¡æ¯ï¼‰
    
    è¿”å›:
        å¦‚æœ return_structured=False: (describe, response) - æè¿°æ–‡æœ¬å’ŒLLMå›ç­”
        å¦‚æœ return_structured=True: dict - åŒ…å«æ‰€æœ‰æ£€ç´¢ä¿¡æ¯å’Œæ–‡æœ¬å—çš„å­—å…¸
    """
    use_llm_func = global_config["use_llm_func"]
    working_dir = global_config["working_dir"]
    level_mode = global_config.get("level_mode", 1)
    topk = global_config.get("topk", 10)
    chunks_file = global_config.get("chunks_file")

    # æ ‡å‡†åŒ–è·¯å¾„å¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯
    working_dir = os.path.normpath(working_dir.rstrip("/"))
    print(f"[DEBUG] Query working directory: {working_dir}")

    # 1. å‘é‡æ£€ç´¢ç›¸å…³å®ä½“
    print(f"[æ­¥éª¤ 1/9] ç”ŸæˆæŸ¥è¯¢å‘é‡...")
    query_embedding = embedding(query)
    print(f"[æ­¥éª¤ 2/9] å‘é‡æ£€ç´¢ Top-{topk} å®ä½“...")
    entity_results = search_vector_search(working_dir, query_embedding, topk=topk, level_mode=level_mode)
    print(f"âœ… æ£€ç´¢åˆ° {len(entity_results)} ä¸ªç›¸å…³å®ä½“")
    res_entity = [i[0] for i in entity_results]
    chunks = [i[-1] for i in entity_results]  # è·å–å®ä½“çš„ source_idï¼ˆå¯¹åº”æ–‡æœ¬å—çš„ hash_codeï¼‰

    # 2. è·å–å®ä½“æè¿°
    print(f"[æ­¥éª¤ 3/9] è·å–å®ä½“æè¿°...")
    entity_descriptions = get_entity_description(entity_results)
    
    # 3. æ„å»ºæ¨ç†è·¯å¾„
    print(f"[æ­¥éª¤ 4/9] æ„å»ºæ¨ç†è·¯å¾„...")
    reasoning_path, reasoning_path_information_description = get_reasoning_chain(working_dir, res_entity)
    print(f"âœ… æ‰¾åˆ° {len(reasoning_path)} æ¡æ¨ç†è·¯å¾„")
    
    # 4. è·å–èšåˆå®ä½“æè¿°
    print(f"[æ­¥éª¤ 5/9] è·å–èšåˆå®ä½“æè¿°...")
    aggregation_descriptions, aggregation = get_aggregation_description(working_dir, reasoning_path)
    
    # 5. æ£€æµ‹æ˜¯å¦ä¸ºæ³•æ¡æŸ¥è¯¢
    is_article_query = any(keyword in query for keyword in ["æ³•æ¡", "ç¬¬", "æ¡", "æ¡æ¬¾", "æ¡æ–‡", "è§„å®š", "å†…å®¹"])
    
    # 5.1 å¦‚æœæ˜¯æ³•æ¡æŸ¥è¯¢ï¼Œå°è¯•ä»æ–‡æœ¬å—ä¸­ç›´æ¥æœç´¢æ³•æ¡
    direct_article_chunks = []
    article_pattern = None
    if is_article_query and chunks_file and os.path.exists(chunks_file):
        # æå–æ³•æ¡ç¼–å·ï¼ˆå¦‚"ç¬¬ä¸€æ¡"ã€"ç¬¬äºŒåä¸‰æ¡"ã€"ç¬¬1æ¡"ã€"ç¬¬23æ¡"ç­‰ï¼‰
        import re
        # åŒ¹é…ä¸­æ–‡æ•°å­—+æ¡ï¼Œå¦‚ï¼šç¬¬ä¸€æ¡ã€ç¬¬äºŒåä¸‰æ¡ã€ç¬¬ä¸€ç™¾æ¡ç­‰
        # ä¹ŸåŒ¹é…é˜¿æ‹‰ä¼¯æ•°å­—+æ¡ï¼Œå¦‚ï¼šç¬¬1æ¡ã€ç¬¬23æ¡ç­‰
        article_match = re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶\d]+æ¡', query)
        if article_match:
            article_pattern = article_match.group(0)
            print(f"[DEBUG] æ£€æµ‹åˆ°æ³•æ¡æŸ¥è¯¢: {article_pattern}")
            # åœ¨æ–‡æœ¬å—ä¸­ç›´æ¥æœç´¢åŒ…å«è¯¥æ³•æ¡çš„å†…å®¹
            direct_article_chunks = search_article_in_chunks(chunks_file, article_pattern)
            if direct_article_chunks:
                print(f"[DEBUG] åœ¨æ–‡æœ¬å—ä¸­æ‰¾åˆ° {len(direct_article_chunks)} ä¸ªåŒ…å«è¯¥æ³•æ¡çš„æ–‡æœ¬å—")
    
    # 6. è·å–æ–‡æœ¬å—ï¼ˆå®ä½“åŸæœ¬çš„æ–‡æœ¬å—ï¼‰
    print(f"[æ­¥éª¤ 6/9] è·å–æ–‡æœ¬å—...")
    # å¦‚æœæ˜¯æ³•æ¡æŸ¥è¯¢ï¼Œå¢åŠ æ–‡æœ¬å—æ•°é‡ä»¥ç¡®ä¿åŒ…å«å®Œæ•´æ³•æ¡å†…å®¹
    text_units_k = global_config.get("text_units_k")
    if text_units_k is None:
        text_units_k = 10 if is_article_query else 5
    text_units_from_entities = get_text_units(working_dir, chunks, chunks_file, k=text_units_k)
    print(f"âœ… è·å–åˆ°æ–‡æœ¬å— (é•¿åº¦: {len(text_units_from_entities)} å­—ç¬¦)")
    
    # 6.1 å¦‚æœé€šè¿‡ç›´æ¥æœç´¢æ‰¾åˆ°äº†æ³•æ¡æ–‡æœ¬å—ï¼Œä¼˜å…ˆä½¿ç”¨è¿™äº›æ–‡æœ¬å—
    if direct_article_chunks and article_pattern:
        # åªä½¿ç”¨ç›´æ¥æœç´¢åˆ°çš„æ³•æ¡æ–‡æœ¬å—ï¼ˆè¿™äº›å·²ç»è¿‡æ»¤è¿‡ï¼ŒåªåŒ…å«æŸ¥è¯¢çš„æ³•æ¡ï¼‰
        direct_text_units = "\n".join([chunk["text"] for chunk in direct_article_chunks])
        print(f"[DEBUG] ä½¿ç”¨ç›´æ¥æœç´¢åˆ°çš„æ³•æ¡æ–‡æœ¬å—ï¼ˆåªåŒ…å«æŸ¥è¯¢çš„æ³•æ¡ï¼‰")
        # å¯¹äºæ³•æ¡æŸ¥è¯¢ï¼Œä¼˜å…ˆä½¿ç”¨ç›´æ¥æœç´¢çš„ç»“æœï¼Œé¿å…åŒ…å«å…¶ä»–æ³•æ¡
        text_units = direct_text_units
        # å¦‚æœç›´æ¥æœç´¢çš„ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°å®Œæ•´çš„æ³•æ¡å†…å®¹ï¼Œå†å°è¯•ä»å®ä½“æ£€ç´¢çš„ç»“æœä¸­æŸ¥æ‰¾
        if article_pattern not in direct_text_units:
            print(f"[DEBUG] ç›´æ¥æœç´¢æœªæ‰¾åˆ°å®Œæ•´æ³•æ¡ï¼Œå°è¯•ä»å®ä½“æ£€ç´¢ç»“æœä¸­æŸ¥æ‰¾")
            # ä»å®ä½“æ£€ç´¢çš„ text_units ä¸­æå–åŒ…å«æŸ¥è¯¢æ³•æ¡çš„éƒ¨åˆ†
            import re
            pattern = re.escape(article_pattern) + r'[^ç¬¬]*?(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶\d]+æ¡|$)'
            match = re.search(pattern, text_units_from_entities, re.DOTALL)
            if match:
                extracted_article = match.group(0).strip()
                text_units = extracted_article
                print(f"[DEBUG] ä»å®ä½“æ£€ç´¢ç»“æœä¸­æå–åˆ°æ³•æ¡å†…å®¹")
            else:
                # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç›´æ¥æœç´¢çš„ç»“æœï¼ˆè‡³å°‘åŒ…å«æ³•æ¡æ¨¡å¼ï¼‰
                text_units = direct_text_units
    else:
        # å¦‚æœä¸æ˜¯æ³•æ¡æŸ¥è¯¢æˆ–æ²¡æœ‰æ‰¾åˆ°ç›´æ¥æœç´¢çš„ç»“æœï¼Œä½¿ç”¨å®ä½“æ£€ç´¢çš„ç»“æœ
        text_units = text_units_from_entities
    
    # 7. è·å–è¯¦ç»†çš„æ–‡æœ¬å—ä¿¡æ¯ï¼ˆç”¨äºç»“æ„åŒ–è¿”å›ï¼‰
    text_chunks_detail = []
    if chunks_file and os.path.exists(chunks_file):
        try:
            with open(chunks_file, 'r', encoding='utf-8') as f:
                chunks_data = json.load(f)
            chunks_dict = {item["hash_code"]: item["text"] for item in chunks_data}
            
            # ç»Ÿè®¡æ¯ä¸ª chunk çš„å‡ºç°æ¬¡æ•°
            from collections import Counter
            chunks_list = []
            for chunk_id in chunks:
                if "|" in chunk_id:
                    chunks_list.extend(chunk_id.split("|"))
                else:
                    chunks_list.append(chunk_id)
            counter = Counter(chunks_list)
            
            # è·å–æœ€ç›¸å…³çš„æ–‡æœ¬å—ï¼ˆæŒ‰å‡ºç°æ¬¡æ•°æ’åºï¼‰
            sorted_chunks = sorted(counter.items(), key=lambda x: x[1], reverse=True)
            for chunk_id, count in sorted_chunks[:topk]:
                if chunk_id in chunks_dict:
                    text_chunks_detail.append({
                        "hash_code": chunk_id,
                        "text": chunks_dict[chunk_id],
                        "relevance_count": count  # è¯¥æ–‡æœ¬å—è¢«å¤šå°‘ä¸ªç›¸å…³å®ä½“å¼•ç”¨
                    })
        except Exception as e:
            print(f"Warning: Failed to load chunks file for detailed info: {e}")

    # 8. ç»„ç»‡ä¸Šä¸‹æ–‡æè¿°ï¼ˆå¦‚æœæ˜¯æ³•æ¡æŸ¥è¯¢ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼‰
    print(f"[æ­¥éª¤ 7/9] ç»„ç»‡ä¸Šä¸‹æ–‡...")
    # å¦‚æœæ˜¯æ³•æ¡æŸ¥è¯¢ï¼Œåœ¨ describe ä¸­æ˜ç¡®æ ‡æ³¨ç”¨æˆ·æŸ¥è¯¢çš„æ³•æ¡
    if is_article_query and article_pattern:
        describe = f"""
    ç”¨æˆ·æŸ¥è¯¢çš„æ³•æ¡: {article_pattern}
    
    entity_information:
    {entity_descriptions}
    aggregation_entity_information:
    {aggregation_descriptions}
    reasoning_path_information:
    {reasoning_path_information_description}
    text_units:
    {text_units}
    
    é‡è¦æç¤º: ç”¨æˆ·è¯¢é—®çš„æ˜¯"{article_pattern}"ï¼Œè¯·åªè¿”å›è¯¥æ³•æ¡çš„å†…å®¹ï¼Œä¸è¦è¿”å› text_units ä¸­çš„å…¶ä»–æ³•æ¡ã€‚
    """
    else:
        describe = f"""
    entity_information:
    {entity_descriptions}
    aggregation_entity_information:
    {aggregation_descriptions}
    reasoning_path_information:
    {reasoning_path_information_description}
    text_units:
    {text_units}
    """

    # 8.1 æ£€æŸ¥å¹¶æˆªæ–­è¿‡é•¿çš„ä¸Šä¸‹æ–‡ï¼Œé¿å…è¶…è¿‡ max_model_len
    # é¢„ç•™ç©ºé—´ç»™ prompt æ¨¡æ¿å’Œç”Ÿæˆï¼ˆçº¦ 2000 tokensï¼‰
    no_truncate = global_config.get("no_truncate", False)
    max_context_tokens = global_config.get("max_model_len", 32768) - 2000
    describe_tokens = len(tokenizer.encode(describe))
    
    if no_truncate:
        print(f"â„¹ï¸  ç¦ç”¨æˆªæ–­æ¨¡å¼ï¼šä¸Šä¸‹æ–‡é•¿åº¦ {describe_tokens} tokensï¼Œæœ€å¤§æ”¯æŒ {max_context_tokens + 2000} tokens")
        if describe_tokens > max_context_tokens + 2000:
            print(f"âš ï¸  è­¦å‘Š: ä¸Šä¸‹æ–‡é•¿åº¦ ({describe_tokens} tokens) è¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦ ({max_context_tokens + 2000} tokens)ï¼Œå¯èƒ½å¯¼è‡´é”™è¯¯")
    elif describe_tokens > max_context_tokens:
        print(f"âš ï¸  è­¦å‘Š: ä¸Šä¸‹æ–‡é•¿åº¦ ({describe_tokens} tokens) è¶…è¿‡é™åˆ¶ ({max_context_tokens} tokens)ï¼Œè¿›è¡Œæˆªæ–­")
        # æŒ‰ä¼˜å…ˆçº§æˆªæ–­ï¼šä¼˜å…ˆä¿ç•™ text_units å’Œ entity_information
        # 1. å…ˆæˆªæ–­ reasoning_path_informationï¼ˆé€šå¸¸è¾ƒé•¿ä¸”é‡è¦æ€§è¾ƒä½ï¼‰
        if len(reasoning_path_information_description) > 0:
            reasoning_tokens = len(tokenizer.encode(reasoning_path_information_description))
            if reasoning_tokens > max_context_tokens * 0.3:  # å¦‚æœæ¨ç†è·¯å¾„ä¿¡æ¯è¶…è¿‡30%ï¼Œè¿›è¡Œæˆªæ–­
                # åªä¿ç•™å‰ä¸€éƒ¨åˆ†
                reasoning_lines = reasoning_path_information_description.split('\n')
                truncated_reasoning = []
                current_tokens = 0
                max_reasoning_tokens = int(max_context_tokens * 0.2)  # æœ€å¤šä¿ç•™20%çš„tokensç»™æ¨ç†è·¯å¾„
                for line in reasoning_lines:
                    line_tokens = len(tokenizer.encode(line))
                    if current_tokens + line_tokens <= max_reasoning_tokens:
                        truncated_reasoning.append(line)
                        current_tokens += line_tokens
                    else:
                        break
                reasoning_path_information_description = '\n'.join(truncated_reasoning)
                print(f"   æˆªæ–­æ¨ç†è·¯å¾„ä¿¡æ¯ï¼Œä¿ç•™ {len(truncated_reasoning)}/{len(reasoning_lines)} è¡Œ")
        
        # 2. é‡æ–°ç»„è£… describe å¹¶æ£€æŸ¥æ˜¯å¦è¿˜éœ€è¦è¿›ä¸€æ­¥æˆªæ–­
        if is_article_query and article_pattern:
            describe_temp = f"""
    ç”¨æˆ·æŸ¥è¯¢çš„æ³•æ¡: {article_pattern}
    
    entity_information:
    {entity_descriptions}
    aggregation_entity_information:
    {aggregation_descriptions}
    reasoning_path_information:
    {reasoning_path_information_description}
    text_units:
    {text_units}
    
    é‡è¦æç¤º: ç”¨æˆ·è¯¢é—®çš„æ˜¯"{article_pattern}"ï¼Œè¯·åªè¿”å›è¯¥æ³•æ¡çš„å†…å®¹ï¼Œä¸è¦è¿”å› text_units ä¸­çš„å…¶ä»–æ³•æ¡ã€‚
    """
        else:
            describe_temp = f"""
    entity_information:
    {entity_descriptions}
    aggregation_entity_information:
    {aggregation_descriptions}
    reasoning_path_information:
    {reasoning_path_information_description}
    text_units:
    {text_units}
    """
        
        describe_tokens = len(tokenizer.encode(describe_temp))
        
        # 3. å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œæˆªæ–­ aggregation_entity_information
        if describe_tokens > max_context_tokens:
            if len(aggregation_descriptions) > 0:
                # åªä¿ç•™å‰å‡ ä¸ªèšåˆå®ä½“
                agg_lines = aggregation_descriptions.split('\n')
                truncated_agg = agg_lines[:min(10, len(agg_lines))]  # æœ€å¤šä¿ç•™10è¡Œ
                aggregation_descriptions = '\n'.join(truncated_agg)
                print(f"   æˆªæ–­èšåˆå®ä½“ä¿¡æ¯ï¼Œä¿ç•™ {len(truncated_agg)}/{len(agg_lines)} è¡Œ")
                # é‡æ–°ç»„è£…
                if is_article_query and article_pattern:
                    describe_temp = f"""
    ç”¨æˆ·æŸ¥è¯¢çš„æ³•æ¡: {article_pattern}
    
    entity_information:
    {entity_descriptions}
    aggregation_entity_information:
    {aggregation_descriptions}
    reasoning_path_information:
    {reasoning_path_information_description}
    text_units:
    {text_units}
    
    é‡è¦æç¤º: ç”¨æˆ·è¯¢é—®çš„æ˜¯"{article_pattern}"ï¼Œè¯·åªè¿”å›è¯¥æ³•æ¡çš„å†…å®¹ï¼Œä¸è¦è¿”å› text_units ä¸­çš„å…¶ä»–æ³•æ¡ã€‚
    """
                else:
                    describe_temp = f"""
    entity_information:
    {entity_descriptions}
    aggregation_entity_information:
    {aggregation_descriptions}
    reasoning_path_information:
    {reasoning_path_information_description}
    text_units:
    {text_units}
    """
                describe_tokens = len(tokenizer.encode(describe_temp))
        
        # 4. å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œæˆªæ–­ text_unitsï¼ˆä¿ç•™æœ€é‡è¦çš„éƒ¨åˆ†ï¼‰
        if describe_tokens > max_context_tokens:
            if len(text_units) > 0:
                # æˆªæ–­ text_unitsï¼Œä¿ç•™å‰é¢çš„éƒ¨åˆ†
                text_units_tokens = len(tokenizer.encode(text_units))
                max_text_units_tokens = int(max_context_tokens * 0.4)  # æœ€å¤šä¿ç•™40%çš„tokensç»™æ–‡æœ¬å—
                if text_units_tokens > max_text_units_tokens:
                    # ä¿ç•™å‰ä¸€éƒ¨åˆ†çš„æ–‡æœ¬å—
                    text_units_lines = text_units.split('\n')
                    truncated_text_units = []
                    current_tokens = 0
                    for line in text_units_lines:
                        line_tokens = len(tokenizer.encode(line))
                        if current_tokens + line_tokens <= max_text_units_tokens:
                            truncated_text_units.append(line)
                            current_tokens += line_tokens
                        else:
                            break
                    text_units = '\n'.join(truncated_text_units)
                    print(f"   æˆªæ–­æ–‡æœ¬å—ï¼Œä¿ç•™ {len(truncated_text_units)}/{len(text_units_lines)} è¡Œ")
                    # é‡æ–°ç»„è£…
                    if is_article_query and article_pattern:
                        describe_temp = f"""
    ç”¨æˆ·æŸ¥è¯¢çš„æ³•æ¡: {article_pattern}
    
    entity_information:
    {entity_descriptions}
    aggregation_entity_information:
    {aggregation_descriptions}
    reasoning_path_information:
    {reasoning_path_information_description}
    text_units:
    {text_units}
    
    é‡è¦æç¤º: ç”¨æˆ·è¯¢é—®çš„æ˜¯"{article_pattern}"ï¼Œè¯·åªè¿”å›è¯¥æ³•æ¡çš„å†…å®¹ï¼Œä¸è¦è¿”å› text_units ä¸­çš„å…¶ä»–æ³•æ¡ã€‚
    """
                    else:
                        describe_temp = f"""
    entity_information:
    {entity_descriptions}
    aggregation_entity_information:
    {aggregation_descriptions}
    reasoning_path_information:
    {reasoning_path_information_description}
    text_units:
    {text_units}
    """
                    describe_tokens = len(tokenizer.encode(describe_temp))
        
        # ä½¿ç”¨æœ€ç»ˆçš„ describe
        describe = describe_temp
        final_tokens = len(tokenizer.encode(describe))
        print(f"   æˆªæ–­åä¸Šä¸‹æ–‡é•¿åº¦: {final_tokens} tokens")
    else:
        print(f"[æ­¥éª¤ 8/9] ä¸Šä¸‹æ–‡é•¿åº¦æ£€æŸ¥é€šè¿‡ ({describe_tokens} tokens)")

    # 9. ç”ŸæˆLLMå›ç­”
    print(f"[æ­¥éª¤ 9/9] è°ƒç”¨ LLM ç”Ÿæˆå›ç­”...")
    # ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡ç‰ˆæœ¬çš„ promptï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬
    # å¦‚æœæ˜¯æ³•æ¡æŸ¥è¯¢ï¼Œä½¿ç”¨ä¸“é—¨çš„æ³•æ¡æŸ¥è¯¢ prompt
    if is_article_query:
        rag_prompt = PROMPTS.get("rag_response_article_zh", PROMPTS.get("rag_response_zh", PROMPTS.get("rag_response", "")))
    else:
        rag_prompt = PROMPTS.get("rag_response_zh", PROMPTS.get("rag_response", ""))
    sys_prompt = rag_prompt.format(context_data=describe)
    
    # è°ƒè¯•ï¼šæ‰“å°æ–‡æœ¬å—ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    if global_config.get("debug", False):
        print(f"\n[DEBUG] æ–‡æœ¬å—æ•°é‡: {len(text_chunks_detail)}")
        print(f"[DEBUG] æ–‡æœ¬å—æ€»é•¿åº¦: {len(text_units)} å­—ç¬¦")
        print(f"[DEBUG] ä¸Šä¸‹æ–‡æ€»é•¿åº¦: {len(describe)} å­—ç¬¦")
        if text_units:
            print(f"[DEBUG] æ–‡æœ¬å—é¢„è§ˆ:\n{text_units[:500]}...")
    
    response = use_llm_func(query, system_prompt=sys_prompt)
    print(f"âœ… LLM å›ç­”ç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(response)} å­—ç¬¦)")
    
    # 10. æ ¹æ®å‚æ•°å†³å®šè¿”å›æ ¼å¼
    if return_structured:
        return {
            "query": query,
            "answer": response,
            "retrieved_entities": [
                {
                    "entity_name": i[0],
                    "parent": i[1],
                    "description": i[2],
                    "source_ids": i[-1] if len(i) > 3 else ""  # source_id å¯èƒ½åŒ…å«å¤šä¸ª chunk hash
                }
                for i in entity_results
            ],
            "text_chunks": text_chunks_detail,  # å®ä½“åŸæœ¬çš„æ–‡æœ¬å—
            "reasoning_path": reasoning_path,
            "reasoning_path_information": reasoning_path_information_description,
            "aggregation_entities": list(aggregation) if aggregation else [],
            "context_summary": describe  # å®Œæ•´çš„ä¸Šä¸‹æ–‡æ‘˜è¦
        }
    else:
        return describe, response


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-p",
        "--path",
        type=str,
        default="/newdataf/SJ/LeanRAG/basicLaw_doc_output_7B_processed/",
        help="æ³•å¾‹çŸ¥è¯†å›¾è°±å·¥ä½œç›®å½•ï¼ˆåŒ…å«ç”Ÿæˆçš„å‘é‡/ç¤¾åŒº/å…³ç³»ç­‰ï¼‰",
    )
    parser.add_argument(
        "--chunks",
        type=str,
        default=None,
        help="chunks.json è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æä¾›ä¼šå½±å“æ–‡æœ¬è¯æ®æå–ï¼‰",
    )
    parser.add_argument("-q", "--query", type=str, required=False, help="æŸ¥è¯¢é—®é¢˜ï¼ˆä¸­æ–‡/è‹±æ–‡å‡å¯ï¼‰ã€‚è‹¥æä¾› --input-json å¯ä¸å¡«")
    parser.add_argument("--input-json", type=str, default=None, help="æ‰¹é‡æŸ¥è¯¢çš„è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„ï¼ˆæ•°ç»„ï¼Œæ¯æ¡åŒ…å« question å­—æ®µï¼‰")
    parser.add_argument("--output-json", type=str, default=None, help="æ‰¹é‡æŸ¥è¯¢çš„è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ˆå†™å…¥ prediction å­—æ®µï¼‰")
    parser.add_argument("-k", "--topk", type=int, default=10, help="æ£€ç´¢ Top-K å®ä½“/èŠ‚ç‚¹")
    parser.add_argument("-l", "--level", type=int, default=2, help="æ£€ç´¢å±‚çº§ï¼š0åŸå§‹èŠ‚ç‚¹/1èšåˆèŠ‚ç‚¹/2å…¨éƒ¨")
    parser.add_argument("-n", "--num", type=int, default=1, help="LLM å¹¶å‘å®ä¾‹æ•°é‡ï¼ˆç”¨äºç”Ÿæˆå›ç­”ï¼‰")
    parser.add_argument("--model", type=str, default=os.environ.get("VLLM_MODEL", "/newdatad/WHH/MyEmoHH/models/Qwen2-7B-Instruct"), help="vLLM æœ¬åœ°æ¨¡å‹åç§°æˆ–æƒé‡è·¯å¾„ï¼ˆé»˜è®¤ï¼š/newdatad/WHH/MyEmoHH/models/Qwen2-7B-Instructï¼‰")
    parser.add_argument("--tp", type=int, default=int(os.environ.get("VLLM_TP", "2")), help="vLLM å¼ é‡å¹¶è¡Œåº¦ tensor_parallel_sizeï¼ˆé»˜è®¤2ï¼Œä½¿ç”¨ä¸¤å¼ GPUå¡ï¼›å¯è®¾ç½®ä¸º4ä½¿ç”¨å››å¼ å¡ä»¥æ”¯æŒæ›´é•¿åºåˆ—ï¼‰")
    parser.add_argument("--max-new-tokens", type=int, default=int(os.environ.get("VLLM_MAX_NEW_TOKENS", "512")), help="ç”Ÿæˆçš„æœ€å¤§æ–°tokenæ•°")
    parser.add_argument("--temperature", type=float, default=float(os.environ.get("VLLM_TEMPERATURE", "0.3")), help="é‡‡æ ·æ¸©åº¦ï¼ˆDeepSeek-R1æ¨è0.3ï¼ŒQwenæ¨è0.2ï¼‰")
    parser.add_argument("--top-p", type=float, default=float(os.environ.get("VLLM_TOP_P", "0.9")), help="top_p é‡‡æ ·é˜ˆå€¼")
    parser.add_argument("--gpu-mem-util", type=float, default=float(os.environ.get("VLLM_GPU_MEM_UTIL", "0.8")), help="æ¯å¼  GPU ç›®æ ‡æ˜¾å­˜å ç”¨æ¯”ä¾‹ï¼Œé™ä½å¯ç¼“è§£ OOM")
    parser.add_argument("--max-model-len", type=int, default=int(os.environ.get("VLLM_MAX_MODEL_LEN", "16384")), help="æœ€å¤§æ¨¡å‹åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤16384ï¼Œå¦‚éœ€æ›´é•¿ä¸Šä¸‹æ–‡å¯è°ƒé«˜ï¼Œæ˜¾å­˜ä¸è¶³æ—¶å¯é™åˆ°8192ï¼‰")
    parser.add_argument("--no-truncate", action="store_true", help="ç¦ç”¨ä¸Šä¸‹æ–‡æˆªæ–­ï¼ˆä¿ç•™å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œéœ€è¦è¶³å¤Ÿçš„æ˜¾å­˜å’Œ max_model_lenï¼‰")
    parser.add_argument("--quantization", type=str, default=os.environ.get("VLLM_QUANT", None), help="é‡åŒ–æ–¹å¼ï¼Œå¦‚ awq/gptq/bitsandbytesï¼ˆæ ¹æ®æ¨¡å‹æä¾›æƒ…å†µï¼‰")
    parser.add_argument("--dtype", type=str, default=os.environ.get("VLLM_DTYPE", None), help="ç²¾åº¦ï¼šbfloat16/float16/float32ï¼ˆä¸æŒ‡å®šåˆ™ç”± vLLM è‡ªåŠ¨é€‰æ‹©ï¼‰")
    parser.add_argument("--structured", action="store_true", help="è¿”å›ç»“æ„åŒ–ç»“æœï¼ˆåŒ…å«æ–‡æœ¬å—ç­‰è¯¦ç»†ä¿¡æ¯ï¼‰")
    parser.add_argument("--text-units-k", type=int, default=None, help="æ–‡æœ¬å—æ•°é‡ï¼ˆé»˜è®¤ï¼šæ³•æ¡æŸ¥è¯¢10ä¸ªï¼Œå…¶ä»–5ä¸ªï¼‰")
    args = parser.parse_args()

    working_dir = os.path.normpath(args.path.rstrip("/"))
    chunks_file = args.chunks

    # æ‰“å°è·¯å¾„ä¿¡æ¯ç”¨äºè°ƒè¯•
    print(f"[DEBUG] Command line path argument: {args.path}")
    print(f"[DEBUG] Normalized working directory: {working_dir}")

    # æ¨¡å‹è·¯å¾„æ£€æµ‹å’Œé…ç½®
    # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹è·¯å¾„
    if os.environ.get('VLLM_MODEL_PATH'):
        model_path = os.environ.get('VLLM_MODEL_PATH')
        print(f"ğŸ“ ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹è·¯å¾„: {model_path}")
    elif os.path.exists(args.model):
        # å¦‚æœå‚æ•°æ˜¯å­˜åœ¨çš„è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
        model_path = args.model
        print(f"ğŸ“ ä½¿ç”¨æŒ‡å®šçš„æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_path}")
    elif args.model.startswith('/'):
        # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ä½†ä¸å­˜åœ¨ï¼Œä»ç„¶å°è¯•ä½¿ç”¨ï¼ˆå¯èƒ½è·¯å¾„é…ç½®é”™è¯¯ï¼‰
        model_path = args.model
        print(f"âš ï¸  è­¦å‘Š: æŒ‡å®šçš„æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print(f"   å°†å°è¯•ä½¿ç”¨è¯¥è·¯å¾„ï¼ˆå¦‚æœå¤±è´¥ï¼ŒvLLM å¯èƒ½ä¼šæŠ¥é”™ï¼‰")
    else:
        # å°è¯•ä»æœ¬åœ°ç¼“å­˜ä¸­æŸ¥æ‰¾æ¨¡å‹
        # æ¨¡å‹ç¼“å­˜ç›®å½•åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        model_cache_dirs = [
            "/root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct",   # ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹
            "/root/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct",  # å¤‡é€‰æ¨¡å‹
            "/root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-V2-Lite-Chat",  # å¤‡é€‰æ¨¡å‹
            "/root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B",  # å¤‡é€‰æ¨¡å‹
        ]
        
        model_path = None
        found_model = None
        
        # é¦–å…ˆæ£€æŸ¥ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹ï¼ˆQwen2-7B-Instructï¼‰
        target_cache_dir = "/root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct"
        if os.path.exists(target_cache_dir):
            snapshots_dir = os.path.join(target_cache_dir, "snapshots")
            if os.path.exists(snapshots_dir):
                try:
                    snapshots = [d for d in os.listdir(snapshots_dir) 
                                if os.path.isdir(os.path.join(snapshots_dir, d))]
                    if snapshots:
                        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                        snapshots.sort(key=lambda x: os.path.getmtime(os.path.join(snapshots_dir, x)), reverse=True)
                        model_path = os.path.join(snapshots_dir, snapshots[0])
                        found_model = "Qwen2-7B-Instruct"
                        print(f"âœ… æ‰¾åˆ°æœ¬åœ°ç¼“å­˜æ¨¡å‹: {found_model}")
                        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
                except Exception as e:
                    print(f"âš ï¸  æ£€æµ‹æ¨¡å‹è·¯å¾„æ—¶å‡ºé”™ ({target_cache_dir}): {e}")
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ¨¡å‹
        if model_path is None:
            for cache_dir in model_cache_dirs:
                if os.path.exists(cache_dir):
                    snapshots_dir = os.path.join(cache_dir, "snapshots")
                    if os.path.exists(snapshots_dir):
                        try:
                            snapshots = [d for d in os.listdir(snapshots_dir) 
                                        if os.path.isdir(os.path.join(snapshots_dir, d))]
                            if snapshots:
                                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                                snapshots.sort(key=lambda x: os.path.getmtime(os.path.join(snapshots_dir, x)), reverse=True)
                                model_path = os.path.join(snapshots_dir, snapshots[0])
                                found_model = os.path.basename(cache_dir)
                                print(f"âœ… æ‰¾åˆ°å¯ç”¨æ¨¡å‹: {found_model}")
                                print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
                                break
                        except Exception as e:
                            print(f"âš ï¸  æ£€æµ‹æ¨¡å‹è·¯å¾„æ—¶å‡ºé”™ ({cache_dir}): {e}")
                            continue
        
        # å¦‚æœæ‰€æœ‰è·¯å¾„éƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šçš„è·¯å¾„æˆ–æ¨¡å‹åç§°
        if model_path is None:
            model_path = args.model  # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šçš„æ¨¡å‹è·¯å¾„æˆ–åç§°
            if os.path.exists(model_path):
                print(f"âœ… ä½¿ç”¨æŒ‡å®šçš„æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_path}")
            elif model_path.startswith('/'):
                print(f"âš ï¸  è­¦å‘Š: æŒ‡å®šçš„æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
                print(f"   å°†å°è¯•ä½¿ç”¨è¯¥è·¯å¾„ï¼ˆå¦‚æœå¤±è´¥ï¼ŒvLLM å¯èƒ½ä¼šæŠ¥é”™ï¼‰")
            else:
                print(f"âš ï¸  æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨ HuggingFace æ¨¡å‹åç§°: {model_path}")
                print(f"ğŸ’¡ æç¤º: å¦‚æœæ¨¡å‹æœªä¸‹è½½ï¼ŒvLLM ä¼šè‡ªåŠ¨ä» HuggingFace ä¸‹è½½")

    # vLLM æœ¬åœ°å¼•æ“
    # é»˜è®¤ä½¿ç”¨ä¸¤å¼ GPUå¡ï¼ˆtensor_parallel_size=2ï¼‰ï¼Œå¯ä»¥æ”¯æŒæ›´é•¿çš„åºåˆ—é•¿åº¦
    max_model_len = args.max_model_len
    tensor_parallel_size = max(1, args.tp)
    
    # æ£€æŸ¥ max_model_len æ˜¯å¦è¶…è¿‡æ¨¡å‹é™åˆ¶
    # Qwen2-7B-Instruct çš„æœ€å¤§é•¿åº¦æ˜¯ 32768
    # å¦‚æœç”¨æˆ·è®¾ç½®çš„å€¼è¶…è¿‡ 32768ï¼Œéœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡å…è®¸è¦†ç›–
    if max_model_len > 32768:
        # è®¾ç½®ç¯å¢ƒå˜é‡å…è®¸è¶…è¿‡æ¨¡å‹é™åˆ¶ï¼ˆéœ€è¦è°¨æ…ä½¿ç”¨ï¼‰
        if not os.environ.get('VLLM_ALLOW_LONG_MAX_MODEL_LEN'):
            os.environ['VLLM_ALLOW_LONG_MAX_MODEL_LEN'] = '1'
            print(f"âš ï¸  è­¦å‘Š: max_model_len ({max_model_len}) è¶…è¿‡æ¨¡å‹é»˜è®¤æœ€å¤§é•¿åº¦ (32768)")
            print(f"   å·²è®¾ç½® VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 ä»¥å…è®¸è¦†ç›–")
            print(f"   æ³¨æ„: å¦‚æœæ¨¡å‹ä½¿ç”¨ RoPE ä½ç½®ç¼–ç ï¼Œè¶…è¿‡é™åˆ¶å¯èƒ½å¯¼è‡´ NaN")
            print(f"   å¦‚æœæ¨¡å‹ä½¿ç”¨ç»å¯¹ä½ç½®ç¼–ç ï¼Œè¶…è¿‡é™åˆ¶å¯èƒ½å¯¼è‡´ CUDA é”™è¯¯")
    
    print(f"ğŸ“ æœ€å¤§åºåˆ—é•¿åº¦: {max_model_len}")
    
    # è®¾ç½®ä½¿ç”¨ç¬¬1å¼ GPUå¡ï¼ˆç´¢å¼•ä¸º0ï¼‰
    if not os.environ.get('CUDA_VISIBLE_DEVICES'):
        os.environ['CUDA_VISIBLE_DEVICES'] = '1'
        print(f"âœ… è®¾ç½® CUDA_VISIBLE_DEVICES=1ï¼Œä½¿ç”¨ç¬¬2å¼ GPUå¡")
        # å¦‚æœåªä½¿ç”¨1å¼ å¡ï¼Œè‡ªåŠ¨è°ƒæ•´ tensor_parallel_size ä¸º1
        if tensor_parallel_size > 1:
            print(f"âš ï¸  æ£€æµ‹åˆ°åªä½¿ç”¨1å¼ GPUå¡ï¼Œè‡ªåŠ¨å°† tensor_parallel_size ä» {tensor_parallel_size} è°ƒæ•´ä¸º 1")
            tensor_parallel_size = 1
    else:
        cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES')
        print(f"â„¹ï¸  ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„ CUDA_VISIBLE_DEVICES={cuda_devices}")
        # æ£€æŸ¥å¯è§çš„GPUæ•°é‡ï¼Œå¦‚æœå°‘äº tensor_parallel_sizeï¼Œåˆ™è°ƒæ•´
        visible_gpu_count = len([x for x in cuda_devices.split(',') if x.strip()])
        if visible_gpu_count < tensor_parallel_size:
            print(f"âš ï¸  å¯è§GPUæ•°é‡ ({visible_gpu_count}) å°‘äº tensor_parallel_size ({tensor_parallel_size})ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º {visible_gpu_count}")
            tensor_parallel_size = visible_gpu_count
    
    # åœ¨ç¡®å®š tensor_parallel_size ä¹‹åï¼Œæ ¹æ®å®é™…ä½¿ç”¨çš„å¡æ•°è®¾ç½®æ˜¾å­˜åˆ©ç”¨ç‡
    # å•å¡æ—¶ä½¿ç”¨ 0.8ï¼ˆé»˜è®¤ï¼‰ï¼ŒåŒå¡ 0.75ï¼Œ4 å¡ 0.85ï¼›è‹¥å‘½ä»¤è¡Œæ›´ä½/æ›´é«˜ä¼šå– min/max èŒƒå›´
    if tensor_parallel_size >= 4:
        gpu_mem_util = min(0.85, max(0.1, args.gpu_mem_util))  # 4å¡æ—¶å¯ä»¥ä½¿ç”¨æ›´é«˜çš„æ˜¾å­˜åˆ©ç”¨ç‡
        print(f"âœ… ä½¿ç”¨ {tensor_parallel_size} å¼  GPU å¡ï¼Œæ˜¾å­˜åˆ©ç”¨ç‡: {gpu_mem_util}")
    elif tensor_parallel_size >= 2:
        gpu_mem_util = min(0.75, max(0.1, args.gpu_mem_util))  # åŒå¡æ—¶ä½¿ç”¨ä¸­ç­‰æ˜¾å­˜åˆ©ç”¨ç‡
        print(f"âœ… ä½¿ç”¨ {tensor_parallel_size} å¼  GPU å¡ï¼Œæ˜¾å­˜åˆ©ç”¨ç‡: {gpu_mem_util}")
    else:
        gpu_mem_util = min(0.8, max(0.1, args.gpu_mem_util))  # å•å¡æ—¶é»˜è®¤æé«˜åˆ° 0.8ï¼Œé¿å… KV cache ä¸ºè´Ÿ
        print(f"âœ… ä½¿ç”¨ {tensor_parallel_size} å¼  GPU å¡ï¼Œæ˜¾å­˜åˆ©ç”¨ç‡: {gpu_mem_util}")
    
    llm = LLM(
        model=model_path,  # ä½¿ç”¨æ£€æµ‹åˆ°çš„æ¨¡å‹è·¯å¾„
        tensor_parallel_size=tensor_parallel_size,
        gpu_memory_utilization=gpu_mem_util,
        max_model_len=max_model_len,  # é™åˆ¶æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé¿å…æ˜¾å­˜ä¸è¶³
        quantization=(args.quantization or None),
        dtype=(args.dtype or "auto"),
    )

    sampling_params = SamplingParams(
        temperature=max(0.0, args.temperature),
        top_p=min(1.0, max(0.0, args.top_p)),
        max_tokens=max(1, args.max_new_tokens),
        repetition_penalty=1.1,  # æ·»åŠ é‡å¤æƒ©ç½šï¼Œé˜²æ­¢é‡å¤ç”Ÿæˆ
        stop=["---", "## ç”¨æˆ·:", "## æ³•æ¡æŸ¥è¯¢"],  # æ·»åŠ åœæ­¢åºåˆ—ï¼Œé˜²æ­¢é‡å¤æ ¼å¼
    )

    def vllm_generate_text(user_prompt: str, system_prompt: str = ""):
        # ç®€å•æŒ‡ä»¤æ‹¼æ¥ï¼Œé€‚é…å¤§å¤šæ•° Instruct æ¨¡å‹
        composed = (system_prompt.strip() + "\n\n" + user_prompt.strip()).strip()
        outputs = llm.generate([composed], sampling_params=sampling_params)
        if not outputs:
            return ""
        response_text = outputs[0].outputs[0].text
        
        # åå¤„ç†ï¼šå»é™¤é‡å¤å†…å®¹
        import re
        
        # å¦‚æœå›ç­”ä¸­åŒ…å«é‡å¤çš„"## æ³•æ¡æŸ¥è¯¢"æ ‡è®°ï¼Œåªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„å†…å®¹
        if "## æ³•æ¡æŸ¥è¯¢" in response_text:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª"## æ³•æ¡æŸ¥è¯¢"ä¹‹å‰çš„æ‰€æœ‰å†…å®¹ï¼ˆå¦‚æœæ²¡æœ‰"## æ³•æ¡æŸ¥è¯¢"æ ‡è®°ï¼Œä¿ç•™å…¨éƒ¨ï¼‰
            parts = response_text.split("## æ³•æ¡æŸ¥è¯¢")
            if len(parts) > 1:
                # ä¿ç•™ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼ˆç¬¬ä¸€ä¸ª"## æ³•æ¡æŸ¥è¯¢"ä¹‹å‰çš„å†…å®¹ï¼‰
                response_text = parts[0].strip()
        
        # å¦‚æœå›ç­”ä¸­åŒ…å«é‡å¤çš„"### ç”¨æˆ·è¯¢é—®"æ ‡è®°ï¼Œåªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„å†…å®¹
        if "### ç”¨æˆ·è¯¢é—®" in response_text:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª"### ç”¨æˆ·è¯¢é—®"ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
            parts = response_text.split("### ç”¨æˆ·è¯¢é—®")
            if len(parts) > 1:
                # ä¿ç•™ç¬¬ä¸€ä¸ªéƒ¨åˆ†
                response_text = parts[0].strip()
        
        # å¦‚æœå›ç­”ä¸­åŒ…å«é‡å¤çš„"---"åˆ†éš”ç¬¦ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªå®Œæ•´å›ç­”
        if response_text.count("---") > 2:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª"---"åˆ°ç¬¬äºŒä¸ª"---"ä¹‹é—´çš„å†…å®¹
            parts = response_text.split("---")
            if len(parts) >= 3:
                # ä¿ç•™ç¬¬ä¸€ä¸ªå®Œæ•´å›ç­”ï¼ˆç¬¬ä¸€éƒ¨åˆ† + ç¬¬äºŒéƒ¨åˆ†ï¼‰
                response_text = (parts[0] + "---" + parts[1]).strip()
        
        # æ£€æµ‹å¹¶å»é™¤å®Œå…¨é‡å¤çš„æ®µè½ï¼ˆå¦‚æœæ•´ä¸ªå›ç­”é‡å¤äº†å¤šæ¬¡ï¼‰
        # é€šè¿‡æ£€æµ‹"## æ³•æ¡å†…å®¹"å‡ºç°çš„æ¬¡æ•°æ¥åˆ¤æ–­
        if response_text.count("## æ³•æ¡å†…å®¹") > 1:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª"## æ³•æ¡å†…å®¹"åˆ°ç¬¬äºŒä¸ª"## æ³•æ¡å†…å®¹"ä¹‹é—´çš„å†…å®¹
            parts = response_text.split("## æ³•æ¡å†…å®¹")
            if len(parts) >= 3:
                # ä¿ç•™ç¬¬ä¸€ä¸ªå®Œæ•´çš„æ³•æ¡å†…å®¹å—
                response_text = ("## æ³•æ¡å†…å®¹" + parts[1]).strip()
        
        return response_text

    global_config = {
        "working_dir": working_dir,
        "chunks_file": chunks_file,
        "embeddings_func": embedding,
        "use_llm_func": vllm_generate_text,
        "topk": max(1, args.topk),
        "level_mode": max(0, min(2, args.level)),
        "text_units_k": args.text_units_k,  # æ–‡æœ¬å—æ•°é‡
        "max_model_len": max_model_len,  # ä¼ é€’ç»™ query_law_graphï¼Œç”¨äºä¸Šä¸‹æ–‡æˆªæ–­
        "no_truncate": args.no_truncate,  # æ˜¯å¦ç¦ç”¨æˆªæ–­
    }

    # æ‰¹å¤„ç†æ¨¡å¼ï¼ˆä¼˜å…ˆï¼‰
    if args.input_json:
        input_path = args.input_json
        output_path = args.output_json or (os.path.splitext(input_path)[0] + "_pred.json")
        print(f"\n{'='*70}")
        print(f"ğŸ“‚ è¯»å–è¾“å…¥æ–‡ä»¶: {input_path}")
        print(f"{'='*70}")
        with open(input_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if not isinstance(data, list):
            raise ValueError("è¾“å…¥ JSON å¿…é¡»æ˜¯æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåŒ…å« question çš„å¯¹è±¡")
        print(f"âœ… æˆåŠŸè¯»å– {len(data)} ä¸ªé—®é¢˜")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"{'='*70}\n")
        results = []
        for idx, item in enumerate(tqdm(data, desc="å¤„ç†é—®é¢˜", unit="ä¸ª"), 1):
            question = item.get("question", "").strip()
            if not question:
                # ç©ºé—®é¢˜ç›´æ¥é€ä¼ 
                new_item = dict(item)
                new_item["prediction"] = ""
                results.append(new_item)
                continue
            print(f"\n[{idx}/{len(data)}] å¤„ç†é—®é¢˜: {question[:50]}{'...' if len(question) > 50 else ''}")
            try:
                result = query_law_graph(global_config, question, return_structured=args.structured)
                new_item = dict(item)
                if args.structured:
                    # ç»“æ„åŒ–è¿”å›ï¼šä¿å­˜å®Œæ•´ç»“æœ
                    new_item["prediction"] = result["answer"]
                    new_item["retrieved_entities"] = result["retrieved_entities"]
                    new_item["text_chunks"] = result["text_chunks"]
                    new_item["reasoning_path"] = result["reasoning_path"]
                else:
                    # ç®€å•è¿”å›ï¼šåªä¿å­˜ç­”æ¡ˆ
                    _, resp = result
                    new_item["prediction"] = resp
                results.append(new_item)
                print(f"âœ… é—®é¢˜ {idx} å¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"âŒ å¤„ç†é—®é¢˜ {idx} æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜ç©ºç»“æœï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                new_item = dict(item)
                new_item["prediction"] = f"[é”™è¯¯: {str(e)}]"
                results.append(new_item)
        print(f"\n{'='*70}")
        print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_path}")
        print(f"{'='*70}")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"âœ… æˆåŠŸä¿å­˜ {len(results)} æ¡ç»“æœåˆ° {output_path}")
        print(f"{'='*70}\n")
        return

    # å•é—®æ¨¡å¼
    if not args.query:
        raise SystemExit("å¿…é¡»æä¾› -q/--queryï¼Œæˆ–æä¾› --input-json è¿›è¡Œæ‰¹å¤„ç†")
    
    result = query_law_graph(global_config, args.query, return_structured=args.structured)
    
    if args.structured:
        # ç»“æ„åŒ–è¾“å‡º
        print("\n" + "=" * 70)
        print("[æŸ¥è¯¢é—®é¢˜]")
        print("=" * 70)
        print(result["query"])
        
        print("\n" + "=" * 70)
        print("[æ£€ç´¢åˆ°çš„å®ä½“]")
        print("=" * 70)
        for i, entity in enumerate(result["retrieved_entities"], 1):
            print(f"\n{i}. {entity['entity_name']}")
            print(f"   çˆ¶èŠ‚ç‚¹: {entity['parent']}")
            print(f"   æè¿°: {entity['description'][:100]}..." if len(entity['description']) > 100 else f"   æè¿°: {entity['description']}")
            print(f"   æ¥æºID: {entity['source_ids']}")
        
        print("\n" + "=" * 70)
        print("[å®ä½“åŸæœ¬çš„æ–‡æœ¬å—]")
        print("=" * 70)
        for i, chunk in enumerate(result["text_chunks"], 1):
            print(f"\n[æ–‡æœ¬å— {i}] (hash: {chunk['hash_code']}, è¢« {chunk['relevance_count']} ä¸ªå®ä½“å¼•ç”¨)")
            print("-" * 70)
            print(chunk['text'])
            print("-" * 70)
        
        print("\n" + "=" * 70)
        print("[æ¨ç†è·¯å¾„]")
        print("=" * 70)
        for i, path in enumerate(result["reasoning_path"], 1):
            print(f"è·¯å¾„ {i}: {' -> '.join(path)}")
        
        print("\n" + "=" * 70)
        print("[LLM ç”Ÿæˆçš„å›ç­”]")
        print("=" * 70)
        print(result["answer"])
        
        # å¯é€‰ï¼šä¿å­˜ç»“æ„åŒ–ç»“æœåˆ°æ–‡ä»¶
        output_file = f"query_result_{args.query[:20].replace(' ', '_')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\n[ç»“æ„åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_file}]")
    else:
        # ç®€å•è¾“å‡ºï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        ref, resp = result
    print("\n[Retrieved Context]\n" + ref)
    print("\n" + "#" * 50)
    print("\n[LLM Response]\n" + str(resp))


if __name__ == "__main__":
    main()


